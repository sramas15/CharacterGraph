Cutting off listeners with fewer than 5 interactions with the speaker.
Play: 0

=========================================================================================================
Speaker: FIRST SOLDIER
Number of Total Listeners: 87

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
154 features selected out of 308 total
F1 mean: 0.09 (+/- 0.06)

             precision    recall  f1-score   support

    BERTRAM       0.33      0.06      0.10        18
 FIRST LORD       0.00      0.00      0.00         9
   PAROLLES       0.41      0.97      0.58        35
SECOND LORD       1.00      0.08      0.15        25

avg / total       0.52      0.43      0.30        87

=========================================================================================================

=========================================================================================================
Speaker: BERTRAM
Number of Total Listeners: 184

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
378 features selected out of 757 total
F1 mean: 0.07 (+/- 0.04)

             precision    recall  f1-score   support

COUNTESS OF ROUSILLON       0.00      0.00      0.00         9
      DIANA       1.00      0.33      0.50        18
 FIRST LORD       0.60      0.15      0.24        20
FIRST SOLDIER       0.00      0.00      0.00        12
     HELENA       0.67      0.15      0.25        13
KING OF FRANCE       0.47      0.65      0.55        26
      LAFEU       0.54      0.29      0.38        24
   PAROLLES       0.37      0.98      0.54        41
SECOND LORD       0.54      0.33      0.41        21

avg / total       0.49      0.45      0.39       184

=========================================================================================================

=========================================================================================================
Speaker: HELENA
Number of Total Listeners: 137

Best model LogisticRegression(C=2.8000000000000003, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
539 features selected out of 1079 total
F1 mean: 0.17 (+/- 0.07)

             precision    recall  f1-score   support

    BERTRAM       0.78      0.58      0.67        12
      CLOWN       1.00      0.60      0.75         5
COUNTESS OF ROUSILLON       0.95      1.00      0.97        19
      DIANA       0.64      0.56      0.60        16
  GENTLEMAN       1.00      0.57      0.73         7
KING OF FRANCE       0.63      0.95      0.76        20
      LAFEU       1.00      0.33      0.50        12
   PAROLLES       0.88      1.00      0.94        23
      WIDOW       0.67      0.78      0.72        23

avg / total       0.80      0.77      0.76       137

=========================================================================================================

=========================================================================================================
Speaker: COUNTESS OF ROUSILLON
Number of Total Listeners: 126

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
406 features selected out of 813 total
F1 mean: 0.13 (+/- 0.08)

             precision    recall  f1-score   support

    BERTRAM       0.50      0.20      0.29        10
      CLOWN       0.49      0.97      0.65        37
FIRST GENTLEMAN       0.00      0.00      0.00         8
     HELENA       0.62      0.50      0.55        26
KING OF FRANCE       0.00      0.00      0.00         5
      LAFEU       0.50      0.61      0.55        18
SECOND GENTLEMAN       0.00      0.00      0.00         8
    STEWARD       1.00      0.36      0.53        14

avg / total       0.49      0.53      0.46       126

=========================================================================================================

=========================================================================================================
Speaker: KING OF FRANCE
Number of Total Listeners: 166

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
552 features selected out of 1104 total
F1 mean: 0.06 (+/- 0.08)

             precision    recall  f1-score   support

    BERTRAM       0.42      0.47      0.44        32
COUNTESS OF ROUSILLON       0.00      0.00      0.00        10
      DIANA       0.44      0.76      0.56        25
 FIRST LORD       0.50      0.08      0.14        12
     HELENA       0.71      0.62      0.67        24
      LAFEU       0.45      0.69      0.55        35
   PAROLLES       0.40      0.13      0.20        15
SECOND LORD       0.50      0.23      0.32        13

avg / total       0.46      0.48      0.43       166

=========================================================================================================

=========================================================================================================
Speaker: FIRST LORD
Number of Total Listeners: 67

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
180 features selected out of 361 total
F1 mean: 0.12 (+/- 0.02)

             precision    recall  f1-score   support

    BERTRAM       0.50      0.12      0.19        17
FIRST SOLDIER       0.00      0.00      0.00         5
KING OF FRANCE       0.00      0.00      0.00         5
   PAROLLES       0.00      0.00      0.00         8
SECOND LORD       0.49      0.97      0.65        32

avg / total       0.36      0.49      0.36        67

=========================================================================================================

=========================================================================================================
Speaker: CLOWN
Number of Total Listeners: 65

Best model LogisticRegression(C=0.40000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
316 features selected out of 633 total
F1 mean: 0.26 (+/- 0.25)

             precision    recall  f1-score   support

COUNTESS OF ROUSILLON       0.95      0.97      0.96        37
     HELENA       0.80      0.80      0.80         5
      LAFEU       0.92      0.80      0.86        15
   PAROLLES       0.78      0.88      0.82         8

avg / total       0.91      0.91      0.91        65

=========================================================================================================

=========================================================================================================
Speaker: PAROLLES
Number of Total Listeners: 219

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
514 features selected out of 1029 total
F1 mean: 0.09 (+/- 0.09)

             precision    recall  f1-score   support

    BERTRAM       0.49      0.73      0.58        51
      CLOWN       0.67      0.20      0.31        10
      DIANA       0.00      0.00      0.00         5
 FIRST LORD       0.00      0.00      0.00        12
FIRST SOLDIER       0.50      0.31      0.38        32
     HELENA       0.79      0.42      0.55        26
KING OF FRANCE       0.50      0.11      0.18         9
      LAFEU       0.44      0.95      0.60        41
SECOND LORD       0.67      0.30      0.42        33

avg / total       0.51      0.50      0.45       219

=========================================================================================================

=========================================================================================================
Speaker: SECOND LORD
Number of Total Listeners: 99

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
278 features selected out of 557 total
F1 mean: 0.24 (+/- 0.17)

             precision    recall  f1-score   support

    BERTRAM       0.56      0.42      0.48        24
 FIRST LORD       0.65      0.85      0.74        33
FIRST SOLDIER       1.00      0.26      0.42        19
   PAROLLES       0.55      0.78      0.64        23

avg / total       0.67      0.62      0.59        99

=========================================================================================================

=========================================================================================================
Speaker: WIDOW
Number of Total Listeners: 37

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
137 features selected out of 275 total
F1 mean: 0.18 (+/- 0.12)

             precision    recall  f1-score   support

      DIANA       0.50      0.27      0.35        11
     HELENA       0.55      0.94      0.70        17
    MARIANA       0.50      0.11      0.18         9

avg / total       0.52      0.54      0.47        37

=========================================================================================================

=========================================================================================================
Speaker: DIANA
Number of Total Listeners: 65

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
215 features selected out of 431 total
F1 mean: 0.14 (+/- 0.07)

             precision    recall  f1-score   support

    BERTRAM       0.80      0.44      0.57        18
     HELENA       0.75      0.67      0.71         9
KING OF FRANCE       0.50      0.96      0.66        23
      LAFEU       0.00      0.00      0.00         7
      WIDOW       0.67      0.25      0.36         8

avg / total       0.58      0.58      0.53        65

=========================================================================================================

=========================================================================================================
Speaker: LAFEU
Number of Total Listeners: 140

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
424 features selected out of 849 total
F1 mean: 0.10 (+/- 0.04)

             precision    recall  f1-score   support

    BERTRAM       0.50      0.59      0.54        29
      CLOWN       0.00      0.00      0.00        14
COUNTESS OF ROUSILLON       0.62      0.42      0.50        24
     HELENA       1.00      0.20      0.33        10
KING OF FRANCE       0.62      0.57      0.59        23
   PAROLLES       0.57      0.95      0.71        40

avg / total       0.55      0.57      0.52       140

=========================================================================================================

Play: 1

=========================================================================================================
Speaker: CHARMIAN
Number of Total Listeners: 98

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
174 features selected out of 348 total
F1 mean: 0.14 (+/- 0.07)

             precision    recall  f1-score   support

     ALEXAS       0.50      0.19      0.27        16
  CLEOPATRA       0.49      1.00      0.66        42
  ENOBARBUS       0.00      0.00      0.00         7
       IRAS       0.00      0.00      0.00        17
 SOOTHSAYER       0.50      0.19      0.27        16

avg / total       0.37      0.49      0.37        98

=========================================================================================================

=========================================================================================================
Speaker: CLEOPATRA
Number of Total Listeners: 289

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
732 features selected out of 1465 total
F1 mean: 0.05 (+/- 0.01)

             precision    recall  f1-score   support

     ALEXAS       0.50      0.08      0.13        13
     ANTONY       0.48      0.94      0.63        62
     CAESAR       0.67      0.50      0.57        12
   CHARMIAN       0.52      0.83      0.64        66
      CLOWN       0.00      0.00      0.00         8
  DOLABELLA       1.00      0.38      0.55        16
  ENOBARBUS       0.64      0.29      0.40        24
       EROS       0.00      0.00      0.00         6
       IRAS       0.75      0.20      0.32        15
    MARDIAN       1.00      0.10      0.18        10
  MESSENGER       0.75      0.51      0.61        35
 PROCULEIUS       1.00      0.33      0.50         9
   SELEUCUS       0.50      0.20      0.29         5
    THYREUS       0.00      0.00      0.00         8

avg / total       0.58      0.55      0.49       289

=========================================================================================================

=========================================================================================================
Speaker: POMPEY
Number of Total Listeners: 87

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
243 features selected out of 487 total
F1 mean: 0.06 (+/- 0.07)

             precision    recall  f1-score   support

        ALL       0.00      0.00      0.00         5
     ANTONY       0.38      0.58      0.46        24
     CAESAR       0.67      0.13      0.22        15
  ENOBARBUS       0.43      0.43      0.43        14
    LEPIDUS       0.00      0.00      0.00         7
      MENAS       0.55      0.82      0.65        22

avg / total       0.43      0.46      0.40        87

=========================================================================================================

=========================================================================================================
Speaker: MESSENGER
Number of Total Listeners: 53

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
99 features selected out of 198 total
F1 mean: 0.26 (+/- 0.03)

             precision    recall  f1-score   support

     ANTONY       0.00      0.00      0.00         8
   CHARMIAN       0.00      0.00      0.00        10
  CLEOPATRA       0.66      1.00      0.80        35

avg / total       0.44      0.66      0.53        53

=========================================================================================================

=========================================================================================================
Speaker: MAECENAS
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
83 features selected out of 166 total
F1 mean: 0.20 (+/- 0.08)

             precision    recall  f1-score   support

    AGRIPPA       0.52      1.00      0.68        13
     CAESAR       1.00      0.14      0.25         7
  ENOBARBUS       1.00      0.14      0.25         7

avg / total       0.77      0.56      0.46        27

=========================================================================================================

=========================================================================================================
Speaker: ENOBARBUS
Number of Total Listeners: 159

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
436 features selected out of 872 total
F1 mean: 0.05 (+/- 0.05)

             precision    recall  f1-score   support

    AGRIPPA       0.59      0.76      0.67        21
     ANTONY       0.40      0.91      0.55        35
     CAESAR       0.00      0.00      0.00         8
   CANIDIUS       0.00      0.00      0.00         5
   CHARMIAN       0.00      0.00      0.00         6
  CLEOPATRA       0.78      0.37      0.50        19
       EROS       1.00      0.17      0.29         6
    LEPIDUS       1.00      0.12      0.22         8
   MAECENAS       0.00      0.00      0.00        10
      MENAS       0.49      0.86      0.62        22
     POMPEY       1.00      0.08      0.14        13
     SCARUS       0.00      0.00      0.00         6

avg / total       0.50      0.48      0.39       159

=========================================================================================================

=========================================================================================================
Speaker: CAESAR
Number of Total Listeners: 148

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
515 features selected out of 1031 total
F1 mean: 0.05 (+/- 0.05)

             precision    recall  f1-score   support

    AGRIPPA       0.55      0.35      0.43        17
     ANTONY       0.36      0.95      0.52        39
  CLEOPATRA       0.75      0.30      0.43        10
  DOLABELLA       1.00      0.45      0.62        11
  ENOBARBUS       1.00      0.17      0.29        12
    LEPIDUS       0.86      0.25      0.39        24
   MAECENAS       0.60      0.64      0.62        14
    OCTAVIA       1.00      0.09      0.17        11
     POMPEY       0.00      0.00      0.00        10

avg / total       0.63      0.47      0.42       148

=========================================================================================================

=========================================================================================================
Speaker: ALEXAS
Number of Total Listeners: 21

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
84 features selected out of 169 total
F1 mean: 0.24 (+/- 0.30)

             precision    recall  f1-score   support

   CHARMIAN       0.54      0.88      0.67         8
  CLEOPATRA       0.88      0.88      0.88         8
 SOOTHSAYER       0.00      0.00      0.00         5

avg / total       0.54      0.67      0.59        21

=========================================================================================================

=========================================================================================================
Speaker: ANTONY
Number of Total Listeners: 314

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
875 features selected out of 1750 total
F1 mean: 0.06 (+/- 0.08)

             precision    recall  f1-score   support

    AGRIPPA       0.00      0.00      0.00         6
        ALL       1.00      0.25      0.40         8
     CAESAR       0.47      0.67      0.55        43
   CHARMIAN       0.00      0.00      0.00         5
  CLEOPATRA       0.44      0.93      0.60        81
   DIOMEDES       0.00      0.00      0.00         6
  ENOBARBUS       0.51      0.45      0.48        49
       EROS       0.84      0.52      0.64        31
    LEPIDUS       0.60      0.12      0.21        24
  MESSENGER       1.00      0.22      0.36         9
    OCTAVIA       1.00      0.38      0.55         8
     POMPEY       0.33      0.06      0.10        18
     SCARUS       1.00      0.44      0.62         9
    SERVANT       0.00      0.00      0.00         5
    SOLDIER       0.00      0.00      0.00         6
 SOOTHSAYER       1.00      0.33      0.50         6

avg / total       0.53      0.51      0.45       314

=========================================================================================================

=========================================================================================================
Speaker: EROS
Number of Total Listeners: 31

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
81 features selected out of 163 total
F1 mean: 0.33 (+/- 0.31)

             precision    recall  f1-score   support

     ANTONY       0.74      1.00      0.85        20
  CLEOPATRA       0.00      0.00      0.00         5
  ENOBARBUS       1.00      0.67      0.80         6

avg / total       0.67      0.77      0.70        31

=========================================================================================================

=========================================================================================================
Speaker: AGRIPPA
Number of Total Listeners: 39

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
119 features selected out of 238 total
F1 mean: 0.20 (+/- 0.06)

             precision    recall  f1-score   support

     CAESAR       0.75      0.27      0.40        11
  ENOBARBUS       0.61      1.00      0.76        17
   MAECENAS       0.57      0.36      0.44        11

avg / total       0.64      0.62      0.57        39

=========================================================================================================

=========================================================================================================
Speaker: LEPIDUS
Number of Total Listeners: 53

Best model LogisticRegression(C=0.70000000000000007, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
129 features selected out of 258 total
F1 mean: 0.08 (+/- 0.10)

             precision    recall  f1-score   support

     ANTONY       0.47      0.53      0.50        15
     CAESAR       0.48      0.87      0.62        15
  ENOBARBUS       1.00      0.44      0.62         9
   MAECENAS       1.00      0.43      0.60         7
     POMPEY       0.50      0.14      0.22         7

avg / total       0.64      0.55      0.53        53

=========================================================================================================

=========================================================================================================
Speaker: IRAS
Number of Total Listeners: 28

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
59 features selected out of 118 total
F1 mean: 0.22 (+/- 0.00)

             precision    recall  f1-score   support

   CHARMIAN       0.54      1.00      0.70        14
  CLEOPATRA       1.00      0.22      0.36         9
 SOOTHSAYER       0.00      0.00      0.00         5

avg / total       0.59      0.57      0.47        28

=========================================================================================================

=========================================================================================================
Speaker: MENAS
Number of Total Listeners: 48

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
120 features selected out of 241 total
F1 mean: 0.36 (+/- 0.26)

             precision    recall  f1-score   support

     ANTONY       0.00      0.00      0.00         7
  ENOBARBUS       0.73      0.96      0.83        23
     POMPEY       0.67      0.67      0.67        18

avg / total       0.60      0.71      0.65        48

=========================================================================================================

=========================================================================================================
Speaker: DOLABELLA
Number of Total Listeners: 29

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
100 features selected out of 200 total
F1 mean: 0.22 (+/- 0.11)

             precision    recall  f1-score   support

     CAESAR       0.57      0.50      0.53         8
  CLEOPATRA       0.73      1.00      0.84        16
FIRST GUARD       0.00      0.00      0.00         5

avg / total       0.56      0.69      0.61        29

=========================================================================================================

Play: 2

=========================================================================================================
Speaker: DUKE SENIOR
Number of Total Listeners: 43

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
221 features selected out of 442 total
F1 mean: 0.13 (+/- 0.18)

             precision    recall  f1-score   support

 FIRST LORD       1.00      0.67      0.80         9
     JAQUES       0.61      1.00      0.76        17
    ORLANDO       0.75      0.50      0.60        12
   ROSALIND       1.00      0.20      0.33         5

avg / total       0.77      0.70      0.67        43

=========================================================================================================

=========================================================================================================
Speaker: CELIA
Number of Total Listeners: 166

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
377 features selected out of 755 total
F1 mean: 0.13 (+/- 0.01)

             precision    recall  f1-score   support

  FREDERICK       0.00      0.00      0.00        11
    LE BEAU       0.00      0.00      0.00         9
     OLIVER       0.00      0.00      0.00        10
    ORLANDO       0.00      0.00      0.00        12
   ROSALIND       0.64      1.00      0.78       107
 TOUCHSTONE       0.00      0.00      0.00        17

avg / total       0.42      0.64      0.51       166

=========================================================================================================

=========================================================================================================
Speaker: FREDERICK
Number of Total Listeners: 39

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
102 features selected out of 205 total
F1 mean: 0.09 (+/- 0.12)

             precision    recall  f1-score   support

      CELIA       0.38      1.00      0.55        14
    LE BEAU       0.00      0.00      0.00         8
    ORLANDO       1.00      0.17      0.29         6
   ROSALIND       1.00      0.09      0.17        11

avg / total       0.57      0.41      0.29        39

=========================================================================================================

=========================================================================================================
Speaker: ROSALIND
Number of Total Listeners: 311

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
713 features selected out of 1427 total
F1 mean: 0.07 (+/- 0.04)

             precision    recall  f1-score   support

      CELIA       0.49      0.95      0.65       111
      CORIN       1.00      0.10      0.18        10
DUKE SENIOR       0.00      0.00      0.00         7
  FREDERICK       1.00      0.11      0.20         9
     JAQUES       1.00      0.12      0.22         8
    LE BEAU       0.00      0.00      0.00        10
     OLIVER       0.00      0.00      0.00        10
    ORLANDO       0.67      0.63      0.65        86
      PHEBE       0.67      0.14      0.24        14
    SILVIUS       0.67      0.22      0.33        18
 TOUCHSTONE       1.00      0.04      0.07        28

avg / total       0.60      0.55      0.46       311

=========================================================================================================

=========================================================================================================
Speaker: LE BEAU
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
73 features selected out of 146 total
F1 mean: 0.29 (+/- 0.16)

             precision    recall  f1-score   support

      CELIA       0.50      0.27      0.35        11
   ROSALIND       0.53      0.75      0.62        12

avg / total       0.52      0.52      0.49        23

=========================================================================================================

=========================================================================================================
Speaker: JAQUES
Number of Total Listeners: 69

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
348 features selected out of 697 total
F1 mean: 0.23 (+/- 0.28)

             precision    recall  f1-score   support

     AMIENS       1.00      0.78      0.88         9
DUKE SENIOR       0.79      0.61      0.69        18
    ORLANDO       0.57      0.94      0.71        18
   ROSALIND       1.00      0.22      0.36         9
 TOUCHSTONE       0.75      0.80      0.77        15

avg / total       0.78      0.71      0.69        69

=========================================================================================================

=========================================================================================================
Speaker: OLIVER
Number of Total Listeners: 56

Best model LogisticRegression(C=0.59999999999999998, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
246 features selected out of 493 total
F1 mean: 0.26 (+/- 0.29)

             precision    recall  f1-score   support

       ADAM       1.00      0.14      0.25         7
      CELIA       0.50      0.20      0.29        15
    CHARLES       1.00      1.00      1.00         5
    ORLANDO       0.65      1.00      0.79        13
   ROSALIND       0.50      0.75      0.60        16

avg / total       0.64      0.61      0.55        56

=========================================================================================================

=========================================================================================================
Speaker: CORIN
Number of Total Listeners: 28

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
140 features selected out of 280 total
F1 mean: 0.36 (+/- 0.07)

             precision    recall  f1-score   support

   ROSALIND       1.00      0.45      0.62        11
 TOUCHSTONE       0.74      1.00      0.85        17

avg / total       0.84      0.79      0.76        28

=========================================================================================================

=========================================================================================================
Speaker: SILVIUS
Number of Total Listeners: 36

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
129 features selected out of 258 total
F1 mean: 0.33 (+/- 0.38)

             precision    recall  f1-score   support

    ORLANDO       0.00      0.00      0.00         5
      PHEBE       0.67      0.80      0.73        15
   ROSALIND       0.67      0.75      0.71        16

avg / total       0.57      0.67      0.62        36

=========================================================================================================

=========================================================================================================
Speaker: TOUCHSTONE
Number of Total Listeners: 111

Best model LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=200, multi_class='ovr',
          penalty='l1', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=1)
384 features selected out of 768 total
F1 mean: 0.19 (+/- 0.10)

             precision    recall  f1-score   support

     AUDREY       0.61      0.61      0.61        18
      CELIA       0.55      0.33      0.41        18
      CORIN       0.93      0.87      0.90        15
DUKE SENIOR       0.00      0.00      0.00         5
     JAQUES       0.62      0.82      0.71        22
   ROSALIND       0.55      0.74      0.63        23
    WILLIAM       0.88      0.70      0.78        10

avg / total       0.63      0.65      0.63       111

=========================================================================================================

=========================================================================================================
Speaker: ORLANDO
Number of Total Listeners: 185

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
403 features selected out of 806 total
F1 mean: 0.09 (+/- 0.05)

             precision    recall  f1-score   support

       ADAM       0.70      0.50      0.58        14
      CELIA       0.50      0.05      0.09        20
DUKE SENIOR       0.80      0.33      0.47        12
  FREDERICK       0.00      0.00      0.00         6
     JAQUES       0.00      0.00      0.00        21
    LE BEAU       0.00      0.00      0.00         8
     OLIVER       1.00      0.25      0.40        12
      PHEBE       0.00      0.00      0.00         7
   ROSALIND       0.47      0.99      0.64        79
    SILVIUS       0.00      0.00      0.00         6

avg / total       0.43      0.50      0.38       185

=========================================================================================================

=========================================================================================================
Speaker: PHEBE
Number of Total Listeners: 43

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
152 features selected out of 305 total
F1 mean: 0.15 (+/- 0.09)

             precision    recall  f1-score   support

    ORLANDO       0.00      0.00      0.00         9
   ROSALIND       0.50      0.07      0.12        14
    SILVIUS       0.49      1.00      0.66        20

avg / total       0.39      0.49      0.35        43

=========================================================================================================

Play: 3

=========================================================================================================
Speaker: ANGELO
Number of Total Listeners: 52

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
151 features selected out of 302 total
F1 mean: 0.08 (+/- 0.05)

             precision    recall  f1-score   support

    ADRIANA       0.00      0.00      0.00         5
ANTIPHOLUS OF EPHESUS       0.44      0.94      0.60        18
ANTIPHOLUS OF SYRACUSE       1.00      0.25      0.40         8
    OFFICER       0.00      0.00      0.00         5
SECOND MERCHANT       0.64      0.44      0.52        16

avg / total       0.50      0.50      0.43        52

=========================================================================================================

=========================================================================================================
Speaker: DUKE
Number of Total Listeners: 35

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
186 features selected out of 372 total
F1 mean: 0.12 (+/- 0.15)

             precision    recall  f1-score   support

    ADRIANA       1.00      0.62      0.77         8
     AEGEON       0.78      0.64      0.70        11
ANTIPHOLUS OF EPHESUS       0.48      0.91      0.62        11
  COURTEZAN       0.00      0.00      0.00         5

avg / total       0.62      0.63      0.59        35

=========================================================================================================

=========================================================================================================
Speaker: DROMIO OF SYRACUSE
Number of Total Listeners: 137

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
361 features selected out of 723 total
F1 mean: 0.20 (+/- 0.16)

             precision    recall  f1-score   support

    ADRIANA       0.82      0.50      0.62        18
ANTIPHOLUS OF EPHESUS       0.57      0.50      0.53        16
ANTIPHOLUS OF SYRACUSE       0.70      1.00      0.83        71
  COURTEZAN       0.00      0.00      0.00         6
DROMIO OF EPHESUS       0.55      0.38      0.44        16
    LUCIANA       0.00      0.00      0.00        10

avg / total       0.60      0.69      0.62       137

=========================================================================================================

=========================================================================================================
Speaker: ANTIPHOLUS OF EPHESUS
Number of Total Listeners: 168

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
319 features selected out of 638 total
F1 mean: 0.03 (+/- 0.01)

             precision    recall  f1-score   support

    ADRIANA       0.33      0.10      0.15        21
     AEGEON       0.00      0.00      0.00         7
     ANGELO       0.44      0.52      0.48        21
ANTIPHOLUS OF SYRACUSE       0.00      0.00      0.00         6
  BALTHAZAR       0.00      0.00      0.00        10
  COURTEZAN       0.00      0.00      0.00         6
DROMIO OF EPHESUS       0.35      0.94      0.51        47
DROMIO OF SYRACUSE       0.67      0.14      0.24        14
       DUKE       0.43      0.25      0.32        12
    OFFICER       0.00      0.00      0.00         9
      PINCH       0.00      0.00      0.00         7
SECOND MERCHANT       0.00      0.00      0.00         8

avg / total       0.28      0.37      0.26       168

=========================================================================================================

=========================================================================================================
Speaker: AEGEON
Number of Total Listeners: 27

Best model LogisticRegression(C=1.6000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
272 features selected out of 544 total
F1 mean: 0.17 (+/- 0.26)

             precision    recall  f1-score   support

ANTIPHOLUS OF EPHESUS       0.40      0.25      0.31         8
DROMIO OF EPHESUS       0.56      0.56      0.56         9
       DUKE       0.69      0.90      0.78        10

avg / total       0.56      0.59      0.57        27

=========================================================================================================

=========================================================================================================
Speaker: DROMIO OF EPHESUS
Number of Total Listeners: 105

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
243 features selected out of 487 total
F1 mean: 0.10 (+/- 0.06)

             precision    recall  f1-score   support

    ADRIANA       0.50      0.40      0.44        20
ANTIPHOLUS OF EPHESUS       0.51      0.97      0.67        39
ANTIPHOLUS OF SYRACUSE       1.00      0.60      0.75        10
  BALTHAZAR       0.00      0.00      0.00         6
DROMIO OF SYRACUSE       0.67      0.31      0.42        13
    LUCIANA       0.50      0.08      0.14        12
    OFFICER       0.00      0.00      0.00         5

avg / total       0.52      0.54      0.47       105

=========================================================================================================

=========================================================================================================
Speaker: LUCIANA
Number of Total Listeners: 52

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
202 features selected out of 404 total
F1 mean: 0.29 (+/- 0.09)

             precision    recall  f1-score   support

    ADRIANA       0.77      0.97      0.86        34
ANTIPHOLUS OF SYRACUSE       0.71      0.42      0.53        12
DROMIO OF SYRACUSE       0.50      0.17      0.25         6

avg / total       0.72      0.75      0.71        52

=========================================================================================================

=========================================================================================================
Speaker: ANTIPHOLUS OF SYRACUSE
Number of Total Listeners: 121

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
307 features selected out of 615 total
F1 mean: 0.17 (+/- 0.20)

             precision    recall  f1-score   support

    ADRIANA       0.33      0.20      0.25        10
     ANGELO       0.67      0.25      0.36         8
ANTIPHOLUS OF EPHESUS       0.33      0.17      0.22         6
  COURTEZAN       0.00      0.00      0.00         5
DROMIO OF EPHESUS       1.00      0.60      0.75        10
DROMIO OF SYRACUSE       0.68      0.99      0.80        69
    LUCIANA       1.00      0.23      0.38        13

avg / total       0.67      0.68      0.62       121

=========================================================================================================

=========================================================================================================
Speaker: ADRIANA
Number of Total Listeners: 151

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
403 features selected out of 806 total
F1 mean: 0.04 (+/- 0.02)

             precision    recall  f1-score   support

     ABBESS       0.67      0.17      0.27        12
     ANGELO       0.00      0.00      0.00         5
ANTIPHOLUS OF EPHESUS       0.33      0.24      0.28        17
ANTIPHOLUS OF SYRACUSE       0.00      0.00      0.00        10
  COURTEZAN       0.00      0.00      0.00         5
DROMIO OF EPHESUS       0.33      0.05      0.09        19
DROMIO OF SYRACUSE       0.43      0.35      0.39        17
       DUKE       1.00      0.33      0.50         6
    LUCIANA       0.37      0.98      0.53        44
    OFFICER       0.00      0.00      0.00         8
      PINCH       0.00      0.00      0.00         8

avg / total       0.33      0.38      0.28       151

=========================================================================================================

Play: 4

=========================================================================================================
Speaker: FIRST CITIZEN
Number of Total Listeners: 52

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
174 features selected out of 348 total
F1 mean: 0.17 (+/- 0.11)

             precision    recall  f1-score   support

        ALL       0.00      0.00      0.00         8
   MENENIUS       0.56      0.88      0.68        16
SECOND CITIZEN       0.48      0.81      0.60        16
   SICINIUS       0.00      0.00      0.00         6
THIRD CITIZEN       0.00      0.00      0.00         6

avg / total       0.32      0.52      0.40        52

=========================================================================================================

=========================================================================================================
Speaker: LARTIUS
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
80 features selected out of 160 total
F1 mean: 0.41 (+/- 0.04)

             precision    recall  f1-score   support

   COMINIUS       1.00      0.14      0.25         7
    MARCIUS       0.73      1.00      0.84        16

avg / total       0.81      0.74      0.66        23

=========================================================================================================

=========================================================================================================
Speaker: AUFIDIUS
Number of Total Listeners: 46

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
271 features selected out of 543 total
F1 mean: 0.17 (+/- 0.09)

             precision    recall  f1-score   support

FIRST CONSPIRATOR       0.50      0.20      0.29         5
 FIRST LORD       0.75      0.43      0.55         7
    MARCIUS       0.79      1.00      0.88        22
SECOND CONSPIRATOR       0.00      0.00      0.00         5
THIRD CONSPIRATOR       0.33      0.57      0.42         7

avg / total       0.59      0.65      0.60        46

=========================================================================================================

=========================================================================================================
Speaker: BRUTUS
Number of Total Listeners: 187

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
342 features selected out of 685 total
F1 mean: 0.06 (+/- 0.00)

             precision    recall  f1-score   support

BOTH TRIBUNES       0.00      0.00      0.00         5
   COMINIUS       0.00      0.00      0.00        10
FIRST CITIZEN       0.00      0.00      0.00         7
FIRST SENATOR       0.00      0.00      0.00         8
    MARCIUS       0.50      0.04      0.07        25
   MENENIUS       0.60      0.07      0.13        42
  PLEBEIANS       0.00      0.00      0.00         7
   SICINIUS       0.43      1.00      0.60        78
   VOLUMNIA       0.00      0.00      0.00         5

avg / total       0.38      0.44      0.29       187

=========================================================================================================

=========================================================================================================
Speaker: SICINIUS
Number of Total Listeners: 241

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
384 features selected out of 769 total
F1 mean: 0.05 (+/- 0.01)

             precision    recall  f1-score   support

     AEDILE       0.00      0.00      0.00         7
     BRUTUS       0.44      0.90      0.59        82
   COMINIUS       0.00      0.00      0.00        13
FIRST CITIZEN       0.00      0.00      0.00         8
FIRST SENATOR       0.00      0.00      0.00        10
    MARCIUS       0.00      0.00      0.00        26
   MENENIUS       0.42      0.43      0.42        63
  MESSENGER       0.00      0.00      0.00        10
  PLEBEIANS       0.33      0.18      0.24        11
SECOND MESSENGER       0.00      0.00      0.00         5
   VOLUMNIA       0.00      0.00      0.00         6

avg / total       0.27      0.43      0.32       241

=========================================================================================================

=========================================================================================================
Speaker: SECOND CITIZEN
Number of Total Listeners: 28

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
63 features selected out of 126 total
F1 mean: 0.17 (+/- 0.11)

             precision    recall  f1-score   support

        ALL       0.00      0.00      0.00         5
FIRST CITIZEN       0.50      1.00      0.67        13
THIRD CITIZEN       1.00      0.20      0.33        10

avg / total       0.59      0.54      0.43        28

=========================================================================================================

=========================================================================================================
Speaker: COMINIUS
Number of Total Listeners: 113

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
401 features selected out of 803 total
F1 mean: 0.05 (+/- 0.03)

             precision    recall  f1-score   support

     BRUTUS       0.00      0.00      0.00         9
FIRST SENATOR       0.00      0.00      0.00         6
    LARTIUS       0.00      0.00      0.00         8
    MARCIUS       0.45      1.00      0.62        38
   MENENIUS       0.64      0.52      0.57        31
   SICINIUS       0.75      0.21      0.33        14
   VOLUMNIA       0.00      0.00      0.00         7

avg / total       0.42      0.50      0.41       113

=========================================================================================================

=========================================================================================================
Speaker: PLEBEIANS
Number of Total Listeners: 31

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
31 features selected out of 63 total
F1 mean: 0.13 (+/- 0.31)

             precision    recall  f1-score   support

     BRUTUS       0.00      0.00      0.00         7
    MARCIUS       0.50      0.20      0.29         5
   MENENIUS       0.44      0.50      0.47         8
   SICINIUS       0.40      0.73      0.52        11

avg / total       0.34      0.42      0.35        31

=========================================================================================================

=========================================================================================================
Speaker: FIRST SENATOR
Number of Total Listeners: 43

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
101 features selected out of 203 total
F1 mean: 0.05 (+/- 0.06)

             precision    recall  f1-score   support

     BRUTUS       0.00      0.00      0.00         5
   COMINIUS       0.00      0.00      0.00         7
    MARCIUS       0.43      0.55      0.48        11
   MENENIUS       0.41      0.92      0.57        13
   SICINIUS       0.00      0.00      0.00         7

avg / total       0.23      0.42      0.30        43

=========================================================================================================

=========================================================================================================
Speaker: VALERIA
Number of Total Listeners: 26

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
109 features selected out of 219 total
F1 mean: 0.35 (+/- 0.04)

             precision    recall  f1-score   support

   VIRGILIA       0.54      1.00      0.70        14
   VOLUMNIA       0.00      0.00      0.00        12

avg / total       0.29      0.54      0.38        26

=========================================================================================================

=========================================================================================================
Speaker: MENENIUS
Number of Total Listeners: 337

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
679 features selected out of 1359 total
F1 mean: 0.06 (+/- 0.02)

             precision    recall  f1-score   support

BOTH TRIBUNES       0.33      0.08      0.13        12
     BRUTUS       0.41      0.41      0.41        59
   COMINIUS       0.56      0.34      0.42        41
FIRST CITIZEN       1.00      0.69      0.81        16
FIRST SENATOR       0.00      0.00      0.00        19
FIRST WATCH       0.50      0.50      0.50        12
    MARCIUS       0.34      0.72      0.46        54
 PATRICIANS       0.00      0.00      0.00         5
  PLEBEIANS       0.00      0.00      0.00        12
SECOND WATCH       0.00      0.00      0.00         8
   SICINIUS       0.42      0.65      0.51        65
   VIRGILIA       0.00      0.00      0.00         8
   VOLUMNIA       0.64      0.35      0.45        26

avg / total       0.40      0.43      0.39       337

=========================================================================================================

=========================================================================================================
Speaker: MARCIUS
Number of Total Listeners: 350

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
949 features selected out of 1899 total
F1 mean: 0.02 (+/- 0.02)

             precision    recall  f1-score   support

        ALL       0.00      0.00      0.00         6
   AUFIDIUS       0.92      0.48      0.63        25
     BRUTUS       0.50      0.07      0.12        28
   COMINIUS       0.43      0.69      0.53        58
FIRST CITIZEN       0.50      0.14      0.22         7
FIRST SENATOR       0.00      0.00      0.00        17
FOURTH CITIZEN       0.67      0.67      0.67         6
    LARTIUS       0.71      0.33      0.45        30
   MENENIUS       0.30      0.81      0.44        69
  MESSENGER       0.00      0.00      0.00         7
SECOND CITIZEN       0.00      0.00      0.00         9
   SICINIUS       0.40      0.06      0.11        33
THIRD CITIZEN       0.00      0.00      0.00         8
THIRD SERVANT       1.00      0.11      0.20         9
   VIRGILIA       0.67      0.22      0.33         9
   VOLUMNIA       0.50      0.38      0.43        29

avg / total       0.44      0.40      0.34       350

=========================================================================================================

=========================================================================================================
Speaker: THIRD SERVANT
Number of Total Listeners: 35

Best model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=200, multi_class='multinomial',
          penalty='l1', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=1)
111 features selected out of 223 total
F1 mean: 0.37 (+/- 0.42)

             precision    recall  f1-score   support

FIRST SERVANT       0.56      0.42      0.48        12
    MARCIUS       0.62      0.91      0.74        11
SECOND SERVANT       0.50      0.42      0.45        12

avg / total       0.56      0.57      0.55        35

=========================================================================================================

=========================================================================================================
Speaker: VIRGILIA
Number of Total Listeners: 47

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
72 features selected out of 144 total
F1 mean: 0.16 (+/- 0.02)

             precision    recall  f1-score   support

    MARCIUS       0.00      0.00      0.00         6
   MENENIUS       0.00      0.00      0.00         5
    VALERIA       0.00      0.00      0.00        13
   VOLUMNIA       0.49      1.00      0.66        23

avg / total       0.24      0.49      0.32        47

=========================================================================================================

=========================================================================================================
Speaker: VOLUMNIA
Number of Total Listeners: 106

Best model LogisticRegression(C=0.30000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
457 features selected out of 914 total
F1 mean: 0.08 (+/- 0.07)

             precision    recall  f1-score   support

     BRUTUS       0.50      0.20      0.29         5
   COMINIUS       0.00      0.00      0.00         8
    MARCIUS       0.58      0.84      0.69        25
   MENENIUS       0.57      0.52      0.54        25
   SICINIUS       0.50      0.11      0.18         9
    VALERIA       0.00      0.00      0.00         8
   VIRGILIA       0.49      0.81      0.61        26

avg / total       0.46      0.54      0.47       106

=========================================================================================================

=========================================================================================================
Speaker: FIRST SERVANT
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
67 features selected out of 135 total
F1 mean: 0.36 (+/- 0.07)

             precision    recall  f1-score   support

SECOND SERVANT       0.60      1.00      0.75        15
THIRD SERVANT       0.00      0.00      0.00        10

avg / total       0.36      0.60      0.45        25

=========================================================================================================

=========================================================================================================
Speaker: SECOND SERVANT
Number of Total Listeners: 23

Best model LogisticRegression(C=0.70000000000000007, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
75 features selected out of 151 total
F1 mean: 0.31 (+/- 0.43)

             precision    recall  f1-score   support

FIRST SERVANT       0.65      1.00      0.79        11
    MARCIUS       1.00      0.80      0.89         5
THIRD SERVANT       0.50      0.14      0.22         7

avg / total       0.68      0.70      0.64        23

=========================================================================================================

Play: 5

=========================================================================================================
Speaker: PHILARIO
Number of Total Listeners: 20

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
80 features selected out of 160 total
F1 mean: 0.28 (+/- 0.13)

             precision    recall  f1-score   support

    IACHIMO       1.00      0.30      0.46        10
  POSTHUMUS       0.59      1.00      0.74        10

avg / total       0.79      0.65      0.60        20

=========================================================================================================

=========================================================================================================
Speaker: PISANIO
Number of Total Listeners: 62

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
276 features selected out of 552 total
F1 mean: 0.40 (+/- 0.20)

             precision    recall  f1-score   support

     CLOTEN       1.00      0.79      0.88        14
  CYMBELINE       1.00      0.22      0.36         9
     IMOGEN       0.80      1.00      0.89        39

avg / total       0.87      0.84      0.81        62

=========================================================================================================

=========================================================================================================
Speaker: IACHIMO
Number of Total Listeners: 104

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
636 features selected out of 1272 total
F1 mean: 0.20 (+/- 0.07)

             precision    recall  f1-score   support

  CYMBELINE       1.00      0.43      0.60         7
  FRENCHMAN       0.00      0.00      0.00         7
     IMOGEN       0.93      0.89      0.91        28
   PHILARIO       0.50      0.14      0.22        21
  POSTHUMUS       0.59      0.98      0.73        41

avg / total       0.65      0.68      0.62       104

=========================================================================================================

=========================================================================================================
Speaker: GUIDERIUS
Number of Total Listeners: 103

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
263 features selected out of 526 total
F1 mean: 0.16 (+/- 0.05)

             precision    recall  f1-score   support

  ARVIRAGUS       0.49      0.68      0.57        41
   BELARIUS       0.54      0.53      0.53        38
     CLOTEN       1.00      0.78      0.88         9
  CYMBELINE       0.50      0.20      0.29         5
     IMOGEN       0.00      0.00      0.00        10

avg / total       0.51      0.54      0.51       103

=========================================================================================================

=========================================================================================================
Speaker: CYMBELINE
Number of Total Listeners: 160

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
395 features selected out of 790 total
F1 mean: 0.04 (+/- 0.06)

             precision    recall  f1-score   support

  ARVIRAGUS       0.00      0.00      0.00         7
   BELARIUS       0.53      0.42      0.47        24
     CLOTEN       0.55      0.46      0.50        13
  CORNELIUS       0.60      0.25      0.35        12
  GUIDERIUS       0.00      0.00      0.00         8
    IACHIMO       1.00      0.12      0.22         8
     IMOGEN       0.33      0.97      0.49        34
     LUCIUS       0.50      0.31      0.38        16
    PISANIO       0.75      0.35      0.48        17
  POSTHUMUS       0.33      0.10      0.15        10
      QUEEN       0.67      0.18      0.29        11

avg / total       0.48      0.42      0.37       160

=========================================================================================================

=========================================================================================================
Speaker: IMOGEN
Number of Total Listeners: 169

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
683 features selected out of 1366 total
F1 mean: 0.08 (+/- 0.05)

             precision    recall  f1-score   support

  ARVIRAGUS       0.33      0.08      0.13        12
   BELARIUS       0.38      0.33      0.35        18
     CLOTEN       0.89      0.67      0.76        12
  CYMBELINE       0.58      0.61      0.60        23
  GUIDERIUS       0.36      0.31      0.33        13
    IACHIMO       0.64      1.00      0.78        28
       LADY       1.00      0.20      0.33         5
     LUCIUS       1.00      0.33      0.50         9
    PISANIO       0.66      0.88      0.76        42
  POSTHUMUS       1.00      0.29      0.44         7

avg / total       0.63      0.62      0.58       169

=========================================================================================================

=========================================================================================================
Speaker: LUCIUS
Number of Total Listeners: 43

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
198 features selected out of 397 total
F1 mean: 0.10 (+/- 0.10)

             precision    recall  f1-score   support

    CAPTAIN       0.00      0.00      0.00         6
     CLOTEN       0.00      0.00      0.00         7
  CYMBELINE       0.41      0.92      0.57        13
     IMOGEN       0.50      0.55      0.52        11
 SOOTHSAYER       0.50      0.17      0.25         6

avg / total       0.32      0.44      0.34        43

=========================================================================================================

=========================================================================================================
Speaker: POSTHUMUS
Number of Total Listeners: 112

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
469 features selected out of 939 total
F1 mean: 0.10 (+/- 0.12)

             precision    recall  f1-score   support

  CYMBELINE       0.50      0.20      0.29        10
  FRENCHMAN       0.00      0.00      0.00         5
     GAOLER       1.00      0.29      0.44         7
    IACHIMO       0.49      0.96      0.65        46
     IMOGEN       0.71      0.45      0.56        11
       LORD       1.00      0.80      0.89         5
   PHILARIO       0.80      0.14      0.24        28

avg / total       0.62      0.54      0.47       112

=========================================================================================================

=========================================================================================================
Speaker: BELARIUS
Number of Total Listeners: 116

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
493 features selected out of 986 total
F1 mean: 0.15 (+/- 0.06)

             precision    recall  f1-score   support

  ARVIRAGUS       0.44      0.49      0.47        41
  CYMBELINE       1.00      0.43      0.60        14
  GUIDERIUS       0.48      0.66      0.55        44
     IMOGEN       0.50      0.12      0.19        17

avg / total       0.53      0.49      0.47       116

=========================================================================================================

=========================================================================================================
Speaker: FIRST LORD
Number of Total Listeners: 21

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
64 features selected out of 128 total
F1 mean: 0.32 (+/- 0.15)

             precision    recall  f1-score   support

     CLOTEN       0.57      1.00      0.73        12
SECOND LORD       0.00      0.00      0.00         9

avg / total       0.33      0.57      0.42        21

=========================================================================================================

=========================================================================================================
Speaker: SECOND LORD
Number of Total Listeners: 35

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
103 features selected out of 207 total
F1 mean: 0.33 (+/- 0.05)

             precision    recall  f1-score   support

     CLOTEN       0.60      1.00      0.75        21
 FIRST LORD       0.00      0.00      0.00        14

avg / total       0.36      0.60      0.45        35

=========================================================================================================

=========================================================================================================
Speaker: CLOTEN
Number of Total Listeners: 104

Best model LogisticRegression(C=0.30000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
325 features selected out of 651 total
F1 mean: 0.21 (+/- 0.11)

             precision    recall  f1-score   support

  CYMBELINE       0.64      0.58      0.61        12
 FIRST LORD       0.60      0.30      0.40        20
  GUIDERIUS       1.00      0.90      0.95        10
     IMOGEN       0.73      1.00      0.85        11
       LADY       1.00      0.50      0.67         6
    PISANIO       1.00      0.85      0.92        13
      QUEEN       1.00      0.25      0.40         8
SECOND LORD       0.47      0.83      0.60        24

avg / total       0.73      0.66      0.65       104

=========================================================================================================

=========================================================================================================
Speaker: QUEEN
Number of Total Listeners: 42

Best model LogisticRegression(C=0.30000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
277 features selected out of 555 total
F1 mean: 0.12 (+/- 0.16)

             precision    recall  f1-score   support

     CLOTEN       0.67      0.67      0.67         9
  CORNELIUS       0.88      1.00      0.93         7
  CYMBELINE       0.55      0.60      0.57        10
     IMOGEN       0.71      0.62      0.67         8
    PISANIO       0.57      0.50      0.53         8

avg / total       0.66      0.67      0.66        42

=========================================================================================================

=========================================================================================================
Speaker: ARVIRAGUS
Number of Total Listeners: 88

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
250 features selected out of 500 total
F1 mean: 0.15 (+/- 0.10)

             precision    recall  f1-score   support

   BELARIUS       0.52      0.38      0.44        34
  CYMBELINE       0.00      0.00      0.00         5
  GUIDERIUS       0.49      0.82      0.61        38
     IMOGEN       0.00      0.00      0.00        11

avg / total       0.41      0.50      0.44        88

=========================================================================================================

Play: 6

=========================================================================================================
Speaker: GUILDENSTERN
Number of Total Listeners: 40

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
93 features selected out of 186 total
F1 mean: 0.36 (+/- 0.04)

             precision    recall  f1-score   support

     HAMLET       0.65      1.00      0.79        24
ROSENCRANTZ       1.00      0.19      0.32        16

avg / total       0.79      0.68      0.60        40

=========================================================================================================

=========================================================================================================
Speaker: OSRIC
Number of Total Listeners: 42

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
98 features selected out of 197 total
F1 mean: 0.24 (+/- 0.03)

             precision    recall  f1-score   support

     HAMLET       0.60      1.00      0.75        25
    HORATIO       0.00      0.00      0.00        12
    LAERTES       0.00      0.00      0.00         5

avg / total       0.35      0.60      0.44        42

=========================================================================================================

=========================================================================================================
Speaker: MARCELLUS
Number of Total Listeners: 60

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
125 features selected out of 251 total
F1 mean: 0.20 (+/- 0.05)

             precision    recall  f1-score   support

   BERNARDO       0.00      0.00      0.00        14
     HAMLET       0.00      0.00      0.00        15
    HORATIO       0.52      1.00      0.68        31

avg / total       0.27      0.52      0.35        60

=========================================================================================================

=========================================================================================================
Speaker: ROSENCRANTZ
Number of Total Listeners: 80

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
181 features selected out of 363 total
F1 mean: 0.14 (+/- 0.02)

             precision    recall  f1-score   support

GUILDENSTERN       0.33      0.22      0.26        23
     HAMLET       0.54      1.00      0.70        35
       KING       0.00      0.00      0.00         9
   POLONIUS       0.00      0.00      0.00         7
      QUEEN       0.00      0.00      0.00         6

avg / total       0.33      0.50      0.38        80

=========================================================================================================

=========================================================================================================
Speaker: BERNARDO
Number of Total Listeners: 32

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
71 features selected out of 143 total
F1 mean: 0.27 (+/- 0.09)

             precision    recall  f1-score   support

  FRANCISCO       1.00      0.17      0.29         6
    HORATIO       0.42      1.00      0.59        13
  MARCELLUS       0.00      0.00      0.00        13

avg / total       0.36      0.44      0.29        32

=========================================================================================================

=========================================================================================================
Speaker: LAERTES
Number of Total Listeners: 98

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
310 features selected out of 620 total
F1 mean: 0.12 (+/- 0.05)

             precision    recall  f1-score   support

     HAMLET       0.57      0.19      0.29        21
       KING       0.49      0.98      0.65        44
    OPHELIA       1.00      0.23      0.38        13
      OSRIC       0.00      0.00      0.00         6
      QUEEN       0.00      0.00      0.00        14

avg / total       0.47      0.51      0.40        98

=========================================================================================================

=========================================================================================================
Speaker: CLOWN
Number of Total Listeners: 39

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
165 features selected out of 331 total
F1 mean: 0.41 (+/- 0.21)

             precision    recall  f1-score   support

     HAMLET       0.68      1.00      0.81        21
    HORATIO       0.00      0.00      0.00         5
SECOND CLOWN       1.00      0.62      0.76        13

avg / total       0.70      0.74      0.69        39

=========================================================================================================

=========================================================================================================
Speaker: HAMLET
Number of Total Listeners: 586

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
1427 features selected out of 2855 total
F1 mean: 0.04 (+/- 0.02)

             precision    recall  f1-score   support

        ALL       0.00      0.00      0.00         5
       BOTH       0.00      0.00      0.00        13
    CAPTAIN       0.00      0.00      0.00         6
      CLOWN       1.00      0.15      0.26        27
FIRST PLAYER       0.50      0.11      0.18         9
      GHOST       0.67      0.10      0.17        20
GUILDENSTERN       0.46      0.24      0.31        46
    HORATIO       0.33      0.99      0.50       118
       KING       0.49      0.41      0.45        58
    LAERTES       0.33      0.12      0.18        24
  MARCELLUS       0.00      0.00      0.00        25
    OPHELIA       0.83      0.28      0.42        36
      OSRIC       1.00      0.10      0.19        29
   POLONIUS       0.57      0.40      0.47        53
   PROLOGUE       0.00      0.00      0.00         6
      QUEEN       0.62      0.40      0.49        57
ROSENCRANTZ       0.52      0.50      0.51        54

avg / total       0.50      0.42      0.37       586

=========================================================================================================

=========================================================================================================
Speaker: KING
Number of Total Listeners: 166

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
681 features selected out of 1363 total
F1 mean: 0.15 (+/- 0.08)

             precision    recall  f1-score   support

     HAMLET       0.57      0.72      0.63        36
    HORATIO       0.00      0.00      0.00         5
    LAERTES       0.68      0.62      0.65        40
    OPHELIA       0.00      0.00      0.00        10
   POLONIUS       0.67      0.17      0.28        23
      QUEEN       0.47      0.83      0.60        41
ROSENCRANTZ       0.80      0.36      0.50        11

avg / total       0.55      0.56      0.51       166

=========================================================================================================

=========================================================================================================
Speaker: POLONIUS
Number of Total Listeners: 128

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
483 features selected out of 967 total
F1 mean: 0.16 (+/- 0.09)

             precision    recall  f1-score   support

FIRST PLAYER       0.00      0.00      0.00         6
     HAMLET       0.51      1.00      0.67        43
       KING       0.54      0.32      0.40        22
    OPHELIA       0.82      0.56      0.67        16
      QUEEN       0.50      0.29      0.36        21
   REYNALDO       1.00      0.47      0.64        15
ROSENCRANTZ       0.00      0.00      0.00         5

avg / total       0.56      0.56      0.51       128

=========================================================================================================

=========================================================================================================
Speaker: HORATIO
Number of Total Listeners: 159

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
365 features selected out of 730 total
F1 mean: 0.12 (+/- 0.02)

             precision    recall  f1-score   support

   BERNARDO       0.50      0.06      0.11        16
       BOTH       0.00      0.00      0.00         8
      CLOWN       0.00      0.00      0.00         6
     HAMLET       0.60      0.99      0.74        85
  MARCELLUS       0.50      0.21      0.30        38
      OSRIC       0.00      0.00      0.00         6

avg / total       0.49      0.58      0.48       159

=========================================================================================================

=========================================================================================================
Speaker: OPHELIA
Number of Total Listeners: 72

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
223 features selected out of 447 total
F1 mean: 0.19 (+/- 0.13)

             precision    recall  f1-score   support

     HAMLET       0.64      1.00      0.78        27
       KING       0.60      0.55      0.57        11
    LAERTES       0.83      0.56      0.67         9
   POLONIUS       0.82      0.56      0.67        16
      QUEEN       1.00      0.33      0.50         9

avg / total       0.74      0.69      0.67        72

=========================================================================================================

=========================================================================================================
Speaker: QUEEN
Number of Total Listeners: 120

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
271 features selected out of 543 total
F1 mean: 0.08 (+/- 0.06)

             precision    recall  f1-score   support

GUILDENSTERN       0.00      0.00      0.00         5
     HAMLET       0.47      0.93      0.62        41
    HORATIO       1.00      0.20      0.33         5
       KING       0.50      0.56      0.53        34
    LAERTES       0.00      0.00      0.00         9
    OPHELIA       0.00      0.00      0.00         6
   POLONIUS       0.00      0.00      0.00        15
ROSENCRANTZ       0.00      0.00      0.00         5

avg / total       0.34      0.48      0.38       120

=========================================================================================================

Play: 7

=========================================================================================================
Speaker: HOSTESS
Number of Total Listeners: 46

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
96 features selected out of 193 total
F1 mean: 0.26 (+/- 0.11)

             precision    recall  f1-score   support

   BARDOLPH       0.00      0.00      0.00         6
   FALSTAFF       0.52      0.96      0.68        24
     PRINCE       0.50      0.06      0.11        16

avg / total       0.45      0.52      0.39        46

=========================================================================================================

=========================================================================================================
Speaker: DOUGLAS
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
78 features selected out of 157 total
F1 mean: 0.22 (+/- 0.00)

             precision    recall  f1-score   support

    HOTSPUR       0.56      1.00      0.72        14
     VERNON       0.00      0.00      0.00         5
  WORCESTER       0.50      0.12      0.20         8

avg / total       0.44      0.56      0.43        27

=========================================================================================================

=========================================================================================================
Speaker: FALSTAFF
Number of Total Listeners: 248

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
665 features selected out of 1331 total
F1 mean: 0.08 (+/- 0.01)

             precision    recall  f1-score   support

   BARDOLPH       1.00      0.17      0.30        23
   GADSHILL       0.00      0.00      0.00        10
    HOSTESS       0.50      0.07      0.12        29
       KING       0.00      0.00      0.00         6
       PETO       0.00      0.00      0.00         5
      POINS       0.00      0.00      0.00        38
     PRINCE       0.54      1.00      0.70       130
WESTMORELAND       0.00      0.00      0.00         7

avg / total       0.44      0.55      0.41       248

=========================================================================================================

=========================================================================================================
Speaker: VERNON
Number of Total Listeners: 28

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
90 features selected out of 180 total
F1 mean: 0.10 (+/- 0.17)

             precision    recall  f1-score   support

    DOUGLAS       0.00      0.00      0.00         9
    HOTSPUR       0.41      1.00      0.58        11
  WORCESTER       1.00      0.12      0.22         8

avg / total       0.45      0.43      0.29        28

=========================================================================================================

=========================================================================================================
Speaker: HOTSPUR
Number of Total Listeners: 174

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
728 features selected out of 1457 total
F1 mean: 0.09 (+/- 0.08)

             precision    recall  f1-score   support

      BLUNT       0.80      0.57      0.67         7
    DOUGLAS       0.64      0.33      0.44        27
  GLENDOWER       0.55      0.72      0.62        25
       LADY       1.00      0.50      0.67        10
  MESSENGER       0.00      0.00      0.00         5
   MORTIMER       0.50      0.19      0.28        21
NORTHUMBERLAND       0.56      0.25      0.34        20
     PRINCE       1.00      0.20      0.33         5
     VERNON       1.00      0.09      0.17        11
  WORCESTER       0.41      0.93      0.57        43

avg / total       0.58      0.50      0.46       174

=========================================================================================================

=========================================================================================================
Speaker: POINS
Number of Total Listeners: 61

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
207 features selected out of 415 total
F1 mean: 0.23 (+/- 0.05)

             precision    recall  f1-score   support

   FALSTAFF       0.00      0.00      0.00        20
    FRANCIS       0.00      0.00      0.00         6
     PRINCE       0.57      1.00      0.73        35

avg / total       0.33      0.57      0.42        61

=========================================================================================================

=========================================================================================================
Speaker: KING
Number of Total Listeners: 31

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
472 features selected out of 944 total
F1 mean: 0.24 (+/- 0.04)

             precision    recall  f1-score   support

     PRINCE       0.80      0.94      0.86        17
WESTMORELAND       1.00      0.83      0.91         6
  WORCESTER       0.83      0.62      0.71         8

avg / total       0.85      0.84      0.83        31

=========================================================================================================

=========================================================================================================
Speaker: GLENDOWER
Number of Total Listeners: 36

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
168 features selected out of 337 total
F1 mean: 0.35 (+/- 0.17)

             precision    recall  f1-score   support

    HOTSPUR       0.62      0.90      0.73        20
   MORTIMER       0.71      0.31      0.43        16

avg / total       0.66      0.64      0.60        36

=========================================================================================================

=========================================================================================================
Speaker: NORTHUMBERLAND
Number of Total Listeners: 22

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
70 features selected out of 140 total
F1 mean: 0.33 (+/- 0.00)

             precision    recall  f1-score   support

    HOTSPUR       0.50      1.00      0.67        11
  WORCESTER       0.00      0.00      0.00        11

avg / total       0.25      0.50      0.33        22

=========================================================================================================

=========================================================================================================
Speaker: WORCESTER
Number of Total Listeners: 57

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
261 features selected out of 522 total
F1 mean: 0.12 (+/- 0.06)

             precision    recall  f1-score   support

    DOUGLAS       0.00      0.00      0.00         9
    HOTSPUR       0.53      1.00      0.69        29
NORTHUMBERLAND       1.00      0.08      0.14        13
     VERNON       1.00      0.17      0.29         6

avg / total       0.60      0.54      0.41        57

=========================================================================================================

=========================================================================================================
Speaker: FRANCIS
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
28 features selected out of 57 total
F1 mean: 0.34 (+/- 0.07)

             precision    recall  f1-score   support

      POINS       0.00      0.00      0.00        11
     PRINCE       0.56      1.00      0.72        14

avg / total       0.31      0.56      0.40        25

=========================================================================================================

=========================================================================================================
Speaker: PRINCE
Number of Total Listeners: 252

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
635 features selected out of 1270 total
F1 mean: 0.08 (+/- 0.04)

             precision    recall  f1-score   support

   BARDOLPH       0.50      0.13      0.21        15
   FALSTAFF       0.52      0.99      0.68       116
    FRANCIS       0.00      0.00      0.00        10
   GADSHILL       0.00      0.00      0.00         5
    HOSTESS       0.00      0.00      0.00        15
       KING       0.67      0.50      0.57        16
  LANCASTER       0.33      0.09      0.14        11
       PETO       1.00      0.11      0.20         9
      POINS       0.67      0.17      0.27        48
WESTMORELAND       0.00      0.00      0.00         7

avg / total       0.49      0.54      0.43       252

=========================================================================================================

=========================================================================================================
Speaker: BARDOLPH
Number of Total Listeners: 22

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
56 features selected out of 113 total
F1 mean: 0.33 (+/- 0.06)

             precision    recall  f1-score   support

   FALSTAFF       0.67      1.00      0.80        12
     PRINCE       1.00      0.40      0.57        10

avg / total       0.82      0.73      0.70        22

=========================================================================================================

=========================================================================================================
Speaker: MORTIMER
Number of Total Listeners: 21

Best model LogisticRegression(C=0.70000000000000007, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
138 features selected out of 277 total
F1 mean: 0.35 (+/- 0.40)

             precision    recall  f1-score   support

  GLENDOWER       0.60      0.82      0.69        11
    HOTSPUR       0.67      0.40      0.50        10

avg / total       0.63      0.62      0.60        21

=========================================================================================================

=========================================================================================================
Speaker: GADSHILL
Number of Total Listeners: 24

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
107 features selected out of 215 total
F1 mean: 0.35 (+/- 0.04)

             precision    recall  f1-score   support

CHAMBERLAIN       0.86      1.00      0.92         6
   FALSTAFF       0.50      1.00      0.67         6
     PRINCE       0.00      0.00      0.00         6
SECOND CARRIER       1.00      0.83      0.91         6

avg / total       0.59      0.71      0.62        24

=========================================================================================================

Play: 8

=========================================================================================================
Speaker: SHALLOW
Number of Total Listeners: 124

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
277 features selected out of 554 total
F1 mean: 0.08 (+/- 0.06)

             precision    recall  f1-score   support

   BARDOLPH       1.00      0.12      0.21        17
       DAVY       1.00      0.33      0.50        12
   FALSTAFF       0.49      0.96      0.64        51
     FEEBLE       0.00      0.00      0.00         5
     MOULDY       0.00      0.00      0.00         6
     PISTOL       0.00      0.00      0.00         7
    SILENCE       0.71      0.46      0.56        26

avg / total       0.58      0.54      0.46       124

=========================================================================================================

=========================================================================================================
Speaker: HOSTESS
Number of Total Listeners: 94

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
284 features selected out of 568 total
F1 mean: 0.03 (+/- 0.05)

             precision    recall  f1-score   support

   BARDOLPH       0.67      0.22      0.33         9
CHIEF JUSTICE       0.00      0.00      0.00         6
       DOLL       0.75      0.39      0.51        23
   FALSTAFF       0.37      0.90      0.52        31
       FANG       0.67      0.22      0.33         9
     PISTOL       0.00      0.00      0.00         6
     PRINCE       0.00      0.00      0.00         5
      SNARE       0.00      0.00      0.00         5

avg / total       0.43      0.44      0.36        94

=========================================================================================================

=========================================================================================================
Speaker: DOLL
Number of Total Listeners: 48

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
169 features selected out of 339 total
F1 mean: 0.26 (+/- 0.06)

             precision    recall  f1-score   support

   FALSTAFF       0.70      0.88      0.78        24
FIRST BEADLE       0.00      0.00      0.00         5
    HOSTESS       0.56      0.53      0.54        19

avg / total       0.57      0.65      0.60        48

=========================================================================================================

=========================================================================================================
Speaker: PRINCE JOHN
Number of Total Listeners: 43

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
165 features selected out of 331 total
F1 mean: 0.06 (+/- 0.04)

             precision    recall  f1-score   support

 ARCHBISHOP       0.67      0.25      0.36         8
CHIEF JUSTICE       1.00      1.00      1.00         7
   COLVILLE       0.00      0.00      0.00         5
   FALSTAFF       0.44      0.89      0.59         9
   HASTINGS       0.00      0.00      0.00         5
WESTMORELAND       0.40      0.67      0.50         9

avg / total       0.46      0.53      0.46        43

=========================================================================================================

=========================================================================================================
Speaker: WARWICK
Number of Total Listeners: 39

Best model LogisticRegression(C=1.4000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
165 features selected out of 331 total
F1 mean: 0.12 (+/- 0.06)

             precision    recall  f1-score   support

CHIEF JUSTICE       1.00      0.83      0.91         6
   CLARENCE       0.50      0.30      0.37        10
       KING       0.61      1.00      0.76        17
PRINCE HUMPHREY       0.00      0.00      0.00         6

avg / total       0.55      0.64      0.57        39

=========================================================================================================

=========================================================================================================
Speaker: DAVY
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
83 features selected out of 167 total
F1 mean: 0.22 (+/- 0.09)

             precision    recall  f1-score   support

   FALSTAFF       0.50      0.20      0.29         5
    SHALLOW       0.62      1.00      0.76        13
    SILENCE       0.00      0.00      0.00         5

avg / total       0.46      0.61      0.49        23

=========================================================================================================

=========================================================================================================
Speaker: FALSTAFF
Number of Total Listeners: 353

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
711 features selected out of 1422 total
F1 mean: 0.04 (+/- 0.02)

             precision    recall  f1-score   support

   BARDOLPH       0.42      0.23      0.29        22
   BULLCALF       0.00      0.00      0.00         8
CHIEF JUSTICE       0.54      0.77      0.63        47
   COLVILLE       0.75      0.75      0.75         8
       DAVY       0.00      0.00      0.00         5
       DOLL       0.48      0.44      0.46        32
     FEEBLE       0.00      0.00      0.00         8
      GOWER       0.00      0.00      0.00         8
    HOSTESS       0.48      0.64      0.55        39
     MOULDY       0.00      0.00      0.00         6
       PAGE       1.00      0.29      0.44        14
     PISTOL       0.50      0.05      0.08        22
      POINS       0.00      0.00      0.00         9
     PRINCE       0.50      0.13      0.21        23
PRINCE JOHN       0.71      0.56      0.63         9
    SERVANT       0.00      0.00      0.00         6
    SHALLOW       0.36      0.92      0.52        65
    SILENCE       0.00      0.00      0.00        17
       WART       0.00      0.00      0.00         5

avg / total       0.40      0.45      0.37       353

=========================================================================================================

=========================================================================================================
Speaker: POINS
Number of Total Listeners: 41

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
139 features selected out of 278 total
F1 mean: 0.27 (+/- 0.04)

             precision    recall  f1-score   support

   BARDOLPH       0.00      0.00      0.00         7
   FALSTAFF       0.00      0.00      0.00         6
     PRINCE       0.68      1.00      0.81        28

avg / total       0.47      0.68      0.55        41

=========================================================================================================

=========================================================================================================
Speaker: CHIEF JUSTICE
Number of Total Listeners: 82

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
203 features selected out of 407 total
F1 mean: 0.10 (+/- 0.01)

             precision    recall  f1-score   support

   FALSTAFF       0.55      1.00      0.71        42
      GOWER       0.00      0.00      0.00         8
    HOSTESS       0.50      0.12      0.20         8
       PAGE       0.00      0.00      0.00         5
PRINCE JOHN       0.00      0.00      0.00         5
    SERVANT       0.00      0.00      0.00         6
    WARWICK       0.75      0.38      0.50         8

avg / total       0.41      0.56      0.43        82

=========================================================================================================

=========================================================================================================
Speaker: MOWBRAY
Number of Total Listeners: 33

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
134 features selected out of 268 total
F1 mean: 0.17 (+/- 0.05)

             precision    recall  f1-score   support

 ARCHBISHOP       0.52      0.93      0.67        15
   HASTINGS       0.50      0.22      0.31         9
WESTMORELAND       1.00      0.22      0.36         9

avg / total       0.64      0.55      0.49        33

=========================================================================================================

=========================================================================================================
Speaker: WESTMORELAND
Number of Total Listeners: 46

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
203 features selected out of 406 total
F1 mean: 0.08 (+/- 0.14)

             precision    recall  f1-score   support

 ARCHBISHOP       0.38      0.75      0.50        16
   HASTINGS       0.00      0.00      0.00         9
    MOWBRAY       0.43      0.38      0.40        16
PRINCE JOHN       0.00      0.00      0.00         5

avg / total       0.28      0.39      0.31        46

=========================================================================================================

=========================================================================================================
Speaker: HASTINGS
Number of Total Listeners: 34

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
130 features selected out of 261 total
F1 mean: 0.11 (+/- 0.12)

             precision    recall  f1-score   support

 ARCHBISHOP       0.48      1.00      0.65        16
LORD BARDOLPH       0.00      0.00      0.00         7
    MOWBRAY       1.00      0.09      0.17        11

avg / total       0.55      0.50      0.36        34

=========================================================================================================

=========================================================================================================
Speaker: CLARENCE
Number of Total Listeners: 24

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
65 features selected out of 130 total
F1 mean: 0.15 (+/- 0.15)

             precision    recall  f1-score   support

       KING       0.50      0.50      0.50         8
PRINCE HUMPHREY       0.00      0.00      0.00         6
    WARWICK       0.56      0.90      0.69        10

avg / total       0.40      0.54      0.46        24

=========================================================================================================

=========================================================================================================
Speaker: KING
Number of Total Listeners: 56

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
440 features selected out of 880 total
F1 mean: 0.14 (+/- 0.03)

             precision    recall  f1-score   support

   CLARENCE       0.60      0.23      0.33        13
     PRINCE       0.67      0.33      0.44         6
PRINCE HUMPHREY       0.50      0.09      0.15        11
    WARWICK       0.54      0.96      0.69        26

avg / total       0.56      0.55      0.48        56

=========================================================================================================

=========================================================================================================
Speaker: PRINCE
Number of Total Listeners: 110

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
459 features selected out of 918 total
F1 mean: 0.07 (+/- 0.06)

             precision    recall  f1-score   support

   BARDOLPH       0.33      0.06      0.11        16
CHIEF JUSTICE       0.67      0.67      0.67         6
   CLARENCE       0.00      0.00      0.00         5
   FALSTAFF       0.50      0.31      0.38        16
    HOSTESS       0.00      0.00      0.00         8
       KING       0.60      0.60      0.60         5
       PAGE       0.00      0.00      0.00        11
      POINS       0.40      0.97      0.56        35
    WARWICK       0.00      0.00      0.00         8

avg / total       0.31      0.43      0.31       110

=========================================================================================================

=========================================================================================================
Speaker: SILENCE
Number of Total Listeners: 37

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
54 features selected out of 109 total
F1 mean: 0.23 (+/- 0.04)

             precision    recall  f1-score   support

       DAVY       0.00      0.00      0.00         5
   FALSTAFF       0.50      0.17      0.25        12
    SHALLOW       0.58      0.95      0.72        20

avg / total       0.47      0.57      0.47        37

=========================================================================================================

=========================================================================================================
Speaker: PISTOL
Number of Total Listeners: 71

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
163 features selected out of 326 total
F1 mean: 0.08 (+/- 0.02)

             precision    recall  f1-score   support

   BARDOLPH       0.00      0.00      0.00         8
       DOLL       0.00      0.00      0.00         7
   FALSTAFF       0.41      0.93      0.57        28
    HOSTESS       0.43      0.25      0.32        12
    SHALLOW       0.00      0.00      0.00        10
    SILENCE       0.00      0.00      0.00         6

avg / total       0.23      0.41      0.28        71

=========================================================================================================

=========================================================================================================
Speaker: BARDOLPH
Number of Total Listeners: 44

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
90 features selected out of 181 total
F1 mean: 0.12 (+/- 0.08)

             precision    recall  f1-score   support

   FALSTAFF       0.44      0.92      0.59        12
    HOSTESS       1.00      0.17      0.29         6
      POINS       0.00      0.00      0.00         6
     PRINCE       0.60      0.67      0.63         9
    SHALLOW       0.88      0.64      0.74        11

avg / total       0.60      0.57      0.51        44

=========================================================================================================

=========================================================================================================
Speaker: ARCHBISHOP
Number of Total Listeners: 53

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
271 features selected out of 542 total
F1 mean: 0.07 (+/- 0.06)

             precision    recall  f1-score   support

   HASTINGS       0.60      0.43      0.50        14
    MOWBRAY       0.43      0.72      0.54        18
PRINCE JOHN       1.00      0.14      0.25         7
WESTMORELAND       0.42      0.36      0.38        14

avg / total       0.55      0.47      0.45        53

=========================================================================================================

=========================================================================================================
Speaker: LORD BARDOLPH
Number of Total Listeners: 22

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
181 features selected out of 363 total
F1 mean: 0.28 (+/- 0.28)

             precision    recall  f1-score   support

 ARCHBISHOP       0.00      0.00      0.00         6
   HASTINGS       0.50      1.00      0.67         6
NORTHUMBERLAND       1.00      1.00      1.00        10

avg / total       0.59      0.73      0.64        22

=========================================================================================================

=========================================================================================================
Speaker: PAGE
Number of Total Listeners: 21

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
69 features selected out of 139 total
F1 mean: 0.33 (+/- 0.38)

             precision    recall  f1-score   support

   BARDOLPH       0.00      0.00      0.00         5
   FALSTAFF       1.00      1.00      1.00         9
     PRINCE       0.58      1.00      0.74         7

avg / total       0.62      0.76      0.67        21

=========================================================================================================

Cutting off listeners with fewer than 5 interactions with the speaker.
Play: 9

=========================================================================================================
Speaker: HOSTESS
Number of Total Listeners: 27

Best model LogisticRegression(C=1.6000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
110 features selected out of 221 total
F1 mean: 0.06 (+/- 0.10)

             precision    recall  f1-score   support

   BARDOLPH       0.43      0.38      0.40         8
        BOY       0.00      0.00      0.00         5
        NYM       0.40      0.57      0.47         7
     PISTOL       0.40      0.57      0.47         7

avg / total       0.33      0.41      0.36        27

=========================================================================================================

=========================================================================================================
Speaker: BOY
Number of Total Listeners: 29

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
145 features selected out of 291 total
F1 mean: 0.06 (+/- 0.11)

             precision    recall  f1-score   support

   BARDOLPH       0.75      0.60      0.67         5
FRENCH SOLDIER       0.00      0.00      0.00         7
        NYM       0.50      0.20      0.29         5
     PISTOL       0.48      0.92      0.63        12

avg / total       0.41      0.52      0.42        29

=========================================================================================================

=========================================================================================================
Speaker: NYM
Number of Total Listeners: 40

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
92 features selected out of 185 total
F1 mean: 0.10 (+/- 0.22)

             precision    recall  f1-score   support

   BARDOLPH       0.59      0.62      0.61        16
    HOSTESS       0.00      0.00      0.00         8
     PISTOL       0.48      0.69      0.56        16

avg / total       0.43      0.53      0.47        40

=========================================================================================================

=========================================================================================================
Speaker: FLUELLEN
Number of Total Listeners: 98

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
359 features selected out of 719 total
F1 mean: 0.15 (+/- 0.15)

             precision    recall  f1-score   support

      GOWER       0.61      0.87      0.72        38
       JAMY       0.00      0.00      0.00         6
 KING HENRY       0.64      0.73      0.68        22
     PISTOL       0.75      0.67      0.71        18
   WILLIAMS       1.00      0.21      0.35        14

avg / total       0.66      0.65      0.61        98

=========================================================================================================

=========================================================================================================
Speaker: DAUPHIN
Number of Total Listeners: 54

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
241 features selected out of 483 total
F1 mean: 0.11 (+/- 0.03)

             precision    recall  f1-score   support

  CONSTABLE       0.52      0.96      0.68        25
FRENCH KING       0.75      0.38      0.50         8
    ORLEANS       0.75      0.20      0.32        15
   RAMBURES       0.00      0.00      0.00         6

avg / total       0.56      0.56      0.47        54

=========================================================================================================

=========================================================================================================
Speaker: PISTOL
Number of Total Listeners: 105

Best model LogisticRegression(C=0.30000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
264 features selected out of 529 total
F1 mean: 0.15 (+/- 0.15)

             precision    recall  f1-score   support

   BARDOLPH       0.40      0.25      0.31        16
        BOY       0.50      0.29      0.36        14
   FLUELLEN       0.63      0.94      0.76        18
FRENCH SOLDIER       0.64      0.69      0.67        13
      GOWER       0.00      0.00      0.00         7
    HOSTESS       0.00      0.00      0.00         6
 KING HENRY       1.00      0.91      0.95        11
        NYM       0.44      0.80      0.57        20

avg / total       0.50      0.57      0.52       105

=========================================================================================================

=========================================================================================================
Speaker: FRENCH KING
Number of Total Listeners: 31

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
197 features selected out of 394 total
F1 mean: 0.06 (+/- 0.10)

             precision    recall  f1-score   support

  CONSTABLE       0.50      0.33      0.40         6
    DAUPHIN       0.53      0.82      0.64        11
     EXETER       0.60      0.43      0.50         7
 KING HENRY       1.00      0.71      0.83         7

avg / total       0.65      0.61      0.61        31

=========================================================================================================

=========================================================================================================
Speaker: CONSTABLE
Number of Total Listeners: 65

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
232 features selected out of 465 total
F1 mean: 0.18 (+/- 0.10)

             precision    recall  f1-score   support

    DAUPHIN       0.63      0.74      0.68        23
  MESSENGER       0.00      0.00      0.00         5
    ORLEANS       0.61      0.96      0.74        24
   RAMBURES       0.00      0.00      0.00        13

avg / total       0.45      0.62      0.51        65

=========================================================================================================

=========================================================================================================
Speaker: ALICE
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
54 features selected out of 108 total
F1 mean: 0.44 (+/- 0.00)

             precision    recall  f1-score   support

  KATHERINE       0.80      1.00      0.89        20
 KING HENRY       0.00      0.00      0.00         5

avg / total       0.64      0.80      0.71        25

=========================================================================================================

=========================================================================================================
Speaker: EXETER
Number of Total Listeners: 31

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
259 features selected out of 518 total
F1 mean: 0.17 (+/- 0.11)

             precision    recall  f1-score   support

    DAUPHIN       0.00      0.00      0.00         5
FRENCH KING       0.50      0.83      0.62         6
 KING HENRY       0.75      1.00      0.86        12
WESTMORELAND       1.00      0.62      0.77         8

avg / total       0.65      0.71      0.65        31

=========================================================================================================

=========================================================================================================
Speaker: KING HENRY
Number of Total Listeners: 251

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
1032 features selected out of 2064 total
F1 mean: 0.05 (+/- 0.05)

             precision    recall  f1-score   support

      ALICE       0.50      0.18      0.27        11
      BATES       0.50      0.08      0.14        12
   BURGUNDY       0.62      0.38      0.48        13
  CAMBRIDGE       0.31      0.44      0.36         9
 CANTERBURY       0.50      0.17      0.25         6
  ERPINGHAM       0.40      0.33      0.36         6
     EXETER       0.46      0.57      0.51        28
   FLUELLEN       0.32      0.88      0.47        33
FRENCH KING       0.40      0.46      0.43        13
 GLOUCESTER       0.64      0.64      0.64        11
       GREY       0.50      0.12      0.20         8
  KATHERINE       0.68      0.79      0.73        19
    MONTJOY       0.67      0.18      0.29        11
     PISTOL       1.00      0.40      0.57        10
QUEEN ISABEL       0.50      0.30      0.37        10
     SCROOP       0.00      0.00      0.00         8
WESTMORELAND       0.67      0.31      0.42        13
   WILLIAMS       0.57      0.43      0.49        30

avg / total       0.51      0.46      0.43       251

=========================================================================================================

=========================================================================================================
Speaker: BARDOLPH
Number of Total Listeners: 31

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
66 features selected out of 132 total
F1 mean: 0.17 (+/- 0.10)

             precision    recall  f1-score   support

    HOSTESS       0.00      0.00      0.00         5
        NYM       0.48      0.93      0.64        15
     PISTOL       0.50      0.09      0.15        11

avg / total       0.41      0.48      0.36        31

=========================================================================================================

=========================================================================================================
Speaker: ORLEANS
Number of Total Listeners: 43

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
102 features selected out of 204 total
F1 mean: 0.25 (+/- 0.03)

             precision    recall  f1-score   support

  CONSTABLE       0.58      1.00      0.74        25
    DAUPHIN       0.00      0.00      0.00        12
   RAMBURES       0.00      0.00      0.00         6

avg / total       0.34      0.58      0.43        43

=========================================================================================================

=========================================================================================================
Speaker: KATHERINE
Number of Total Listeners: 38

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
112 features selected out of 224 total
F1 mean: 0.67 (+/- 0.42)

             precision    recall  f1-score   support

      ALICE       0.88      0.91      0.89        23
 KING HENRY       0.86      0.80      0.83        15

avg / total       0.87      0.87      0.87        38

=========================================================================================================

=========================================================================================================
Speaker: WILLIAMS
Number of Total Listeners: 44

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
147 features selected out of 294 total
F1 mean: 0.18 (+/- 0.07)

             precision    recall  f1-score   support

      BATES       0.00      0.00      0.00         9
   FLUELLEN       0.67      0.17      0.27        12
 KING HENRY       0.54      0.96      0.69        23

avg / total       0.46      0.55      0.43        44

=========================================================================================================

Play: 10

=========================================================================================================
Speaker: BEDFORD
Number of Total Listeners: 33

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
174 features selected out of 349 total
F1 mean: 0.25 (+/- 0.16)

             precision    recall  f1-score   support

   BURGUNDY       0.50      0.55      0.52        11
     EXETER       0.67      0.67      0.67         6
 GLOUCESTER       0.60      0.60      0.60         5
     TALBOT       0.50      0.45      0.48        11

avg / total       0.55      0.55      0.54        33

=========================================================================================================

=========================================================================================================
Speaker: BASTARD
Number of Total Listeners: 30

Best model LogisticRegression(C=0.90000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
92 features selected out of 185 total
F1 mean: 0.08 (+/- 0.16)

             precision    recall  f1-score   support

    ALENCON       0.38      0.56      0.45         9
    CHARLES       0.41      0.70      0.52        10
    PUCELLE       0.00      0.00      0.00         5
   REIGNIER       0.00      0.00      0.00         6

avg / total       0.25      0.40      0.31        30

=========================================================================================================

=========================================================================================================
Speaker: ALENCON
Number of Total Listeners: 40

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
132 features selected out of 264 total
F1 mean: 0.09 (+/- 0.13)

             precision    recall  f1-score   support

    BASTARD       0.00      0.00      0.00         7
    CHARLES       0.50      0.71      0.59        14
    PUCELLE       0.00      0.00      0.00         7
   REIGNIER       0.40      0.67      0.50        12

avg / total       0.30      0.45      0.36        40

=========================================================================================================

=========================================================================================================
Speaker: TALBOT
Number of Total Listeners: 65

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
432 features selected out of 864 total
F1 mean: 0.24 (+/- 0.07)

             precision    recall  f1-score   support

    BEDFORD       0.40      0.17      0.24        12
   BURGUNDY       0.54      0.93      0.68        15
   COUNTESS       0.75      1.00      0.86         9
       JOHN       1.00      1.00      1.00        13
       KING       1.00      1.00      1.00         5
  MESSENGER       1.00      0.17      0.29         6
    PUCELLE       1.00      0.60      0.75         5

avg / total       0.75      0.72      0.68        65

=========================================================================================================

=========================================================================================================
Speaker: VERNON
Number of Total Listeners: 26

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
76 features selected out of 153 total
F1 mean: 0.17 (+/- 0.24)

             precision    recall  f1-score   support

     BASSET       0.47      0.88      0.61         8
       KING       0.00      0.00      0.00         5
PLANTAGENET       0.57      0.57      0.57         7
   SOMERSET       0.25      0.17      0.20         6

avg / total       0.36      0.46      0.39        26

=========================================================================================================

=========================================================================================================
Speaker: WARWICK
Number of Total Listeners: 53

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
168 features selected out of 337 total
F1 mean: 0.05 (+/- 0.07)

             precision    recall  f1-score   support

 GLOUCESTER       0.00      0.00      0.00         6
       KING       0.33      0.44      0.38         9
PLANTAGENET       0.45      1.00      0.62        17
    PUCELLE       0.00      0.00      0.00         6
   SOMERSET       0.00      0.00      0.00         7
 WINCHESTER       0.33      0.12      0.18         8

avg / total       0.25      0.42      0.29        53

=========================================================================================================

=========================================================================================================
Speaker: KING
Number of Total Listeners: 57

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
310 features selected out of 621 total
F1 mean: 0.07 (+/- 0.05)

             precision    recall  f1-score   support

     EXETER       0.25      0.14      0.18         7
 GLOUCESTER       0.49      0.96      0.65        23
PLANTAGENET       0.67      0.33      0.44         6
     TALBOT       0.00      0.00      0.00         7
    WARWICK       0.50      0.12      0.20         8
 WINCHESTER       0.33      0.17      0.22         6

avg / total       0.40      0.47      0.38        57

=========================================================================================================

=========================================================================================================
Speaker: PLANTAGENET
Number of Total Listeners: 83

Best model LogisticRegression(C=0.40000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
290 features selected out of 581 total
F1 mean: 0.08 (+/- 0.10)

             precision    recall  f1-score   support

       KING       1.00      0.12      0.22         8
   MORTIMER       1.00      1.00      1.00         7
    PUCELLE       0.67      0.57      0.62        14
   SOMERSET       0.48      0.78      0.60        18
    SUFFOLK       1.00      0.29      0.44         7
     VERNON       0.67      0.25      0.36         8
    WARWICK       0.52      0.71      0.60        21

avg / total       0.68      0.59      0.56        83

=========================================================================================================

=========================================================================================================
Speaker: CHARLES
Number of Total Listeners: 85

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
259 features selected out of 518 total
F1 mean: 0.10 (+/- 0.10)

             precision    recall  f1-score   support

    ALENCON       0.46      0.50      0.48        22
    BASTARD       0.67      0.14      0.24        14
   BURGUNDY       0.00      0.00      0.00         8
    PUCELLE       0.45      0.84      0.58        25
   REIGNIER       0.45      0.31      0.37        16

avg / total       0.45      0.46      0.40        85

=========================================================================================================

=========================================================================================================
Speaker: BURGUNDY
Number of Total Listeners: 33

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
114 features selected out of 228 total
F1 mean: 0.12 (+/- 0.14)

             precision    recall  f1-score   support

    BEDFORD       0.50      0.25      0.33         8
    CHARLES       0.50      0.14      0.22         7
    PUCELLE       0.56      0.62      0.59         8
     TALBOT       0.44      0.80      0.57        10

avg / total       0.50      0.48      0.44        33

=========================================================================================================

=========================================================================================================
Speaker: SOMERSET
Number of Total Listeners: 45

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
130 features selected out of 261 total
F1 mean: 0.11 (+/- 0.03)

             precision    recall  f1-score   support

       LUCY       1.00      0.40      0.57         5
PLANTAGENET       0.45      1.00      0.62        19
    SUFFOLK       0.00      0.00      0.00         6
     VERNON       0.00      0.00      0.00         6
    WARWICK       1.00      0.11      0.20         9

avg / total       0.50      0.49      0.37        45

=========================================================================================================

=========================================================================================================
Speaker: GLOUCESTER
Number of Total Listeners: 75

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
315 features selected out of 630 total
F1 mean: 0.10 (+/- 0.13)

             precision    recall  f1-score   support

    BEDFORD       0.00      0.00      0.00         5
     EXETER       0.60      0.25      0.35        12
       KING       0.53      0.84      0.65        19
      MAYOR       0.00      0.00      0.00         6
     TALBOT       0.00      0.00      0.00         5
    WARWICK       0.00      0.00      0.00         6
 WINCHESTER       0.55      1.00      0.71        22

avg / total       0.39      0.55      0.43        75

=========================================================================================================

=========================================================================================================
Speaker: REIGNIER
Number of Total Listeners: 39

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
132 features selected out of 264 total
F1 mean: 0.30 (+/- 0.32)

             precision    recall  f1-score   support

    ALENCON       0.50      0.67      0.57        12
    CHARLES       0.38      0.42      0.40        12
    PUCELLE       0.50      0.14      0.22         7
    SUFFOLK       1.00      1.00      1.00         8

avg / total       0.57      0.56      0.54        39

=========================================================================================================

=========================================================================================================
Speaker: PUCELLE
Number of Total Listeners: 83

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
400 features selected out of 801 total
F1 mean: 0.04 (+/- 0.06)

             precision    recall  f1-score   support

    ALENCON       0.40      0.20      0.27        10
    BASTARD       0.00      0.00      0.00         6
   BURGUNDY       0.00      0.00      0.00         9
    CHARLES       0.44      0.96      0.60        27
PLANTAGENET       0.62      0.80      0.70        10
   REIGNIER       0.50      0.12      0.20         8
     TALBOT       1.00      0.29      0.44         7
    WARWICK       0.50      0.17      0.25         6

avg / total       0.43      0.48      0.39        83

=========================================================================================================

=========================================================================================================
Speaker: WINCHESTER
Number of Total Listeners: 35

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
183 features selected out of 366 total
F1 mean: 0.18 (+/- 0.06)

             precision    recall  f1-score   support

 GLOUCESTER       0.67      0.95      0.78        19
       KING       0.67      0.40      0.50         5
PLANTAGENET       0.60      0.60      0.60         5
    WARWICK       0.00      0.00      0.00         6

avg / total       0.54      0.66      0.58        35

=========================================================================================================

=========================================================================================================
Speaker: SUFFOLK
Number of Total Listeners: 47

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
299 features selected out of 598 total
F1 mean: 0.21 (+/- 0.24)

             precision    recall  f1-score   support

 GLOUCESTER       1.00      1.00      1.00         5
   MARGARET       0.69      0.96      0.80        25
PLANTAGENET       0.75      0.33      0.46         9
   REIGNIER       1.00      0.38      0.55         8

avg / total       0.78      0.74      0.71        47

=========================================================================================================

Play: 11

=========================================================================================================
Speaker: CLIFFORD
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
81 features selected out of 163 total
F1 mean: 0.21 (+/- 0.08)

             precision    recall  f1-score   support

 KING HENRY       1.00      0.14      0.25         7
    RICHARD       1.00      0.20      0.33         5
       YORK       0.57      1.00      0.72        13

avg / total       0.77      0.60      0.51        25

=========================================================================================================

=========================================================================================================
Speaker: CADE
Number of Total Listeners: 103

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
320 features selected out of 641 total
F1 mean: 0.08 (+/- 0.06)

             precision    recall  f1-score   support

        ALL       1.00      0.17      0.29        12
       DICK       0.42      1.00      0.59        40
       JOHN       0.00      0.00      0.00         6
        SAY       1.00      0.11      0.20         9
      SMITH       0.50      0.06      0.11        17
   STAFFORD       0.50      0.10      0.17        10
WILLIAM STAFFORD       0.00      0.00      0.00         9

avg / total       0.50      0.44      0.31       103

=========================================================================================================

=========================================================================================================
Speaker: WARWICK
Number of Total Listeners: 62

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
245 features selected out of 490 total
F1 mean: 0.05 (+/- 0.09)

             precision    recall  f1-score   support

   CARDINAL       0.00      0.00      0.00         5
 KING HENRY       0.46      0.60      0.52        10
      QUEEN       0.40      0.20      0.27        10
  SALISBURY       0.43      0.50      0.46        12
    SUFFOLK       0.44      0.36      0.40        11
       YORK       0.57      0.86      0.69        14

avg / total       0.43      0.48      0.44        62

=========================================================================================================

=========================================================================================================
Speaker: YORK
Number of Total Listeners: 123

Best model LogisticRegression(C=0.40000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
555 features selected out of 1110 total
F1 mean: 0.09 (+/- 0.08)

             precision    recall  f1-score   support

 BUCKINGHAM       1.00      0.78      0.88         9
   CARDINAL       0.31      0.31      0.31        13
   CLIFFORD       0.50      0.90      0.64        10
 GLOUCESTER       0.00      0.00      0.00         5
 KING HENRY       0.44      0.50      0.47        14
      QUEEN       0.36      0.36      0.36        14
    RICHARD       1.00      0.20      0.33         5
  SALISBURY       0.57      0.33      0.42        12
   SOMERSET       0.50      0.22      0.31         9
    SUFFOLK       0.37      0.67      0.48        15
    WARWICK       0.50      0.47      0.48        17

avg / total       0.48      0.46      0.44       123

=========================================================================================================

=========================================================================================================
Speaker: SOMERSET
Number of Total Listeners: 22

Best model LogisticRegression(C=0.40000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
53 features selected out of 107 total
F1 mean: 0.08 (+/- 0.13)

             precision    recall  f1-score   support

   CARDINAL       0.33      0.20      0.25         5
 KING HENRY       0.75      0.60      0.67         5
      QUEEN       0.40      0.57      0.47         7
       YORK       0.40      0.40      0.40         5

avg / total       0.46      0.45      0.45        22

=========================================================================================================

=========================================================================================================
Speaker: CARDINAL
Number of Total Listeners: 74

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
219 features selected out of 438 total
F1 mean: 0.07 (+/- 0.11)

             precision    recall  f1-score   support

 BUCKINGHAM       0.50      0.25      0.33         8
 GLOUCESTER       0.36      0.72      0.48        18
 KING HENRY       0.47      0.44      0.45        16
      QUEEN       0.00      0.00      0.00         6
   SOMERSET       0.00      0.00      0.00         7
    SUFFOLK       0.35      0.50      0.41        12
       YORK       0.50      0.14      0.22         7

avg / total       0.35      0.39      0.34        74

=========================================================================================================

=========================================================================================================
Speaker: BUCKINGHAM
Number of Total Listeners: 31

Best model LogisticRegression(C=0.70000000000000007, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
139 features selected out of 279 total
F1 mean: 0.24 (+/- 0.29)

             precision    recall  f1-score   support

   CARDINAL       0.75      0.50      0.60         6
 KING HENRY       0.56      0.90      0.69        10
      QUEEN       0.50      0.29      0.36         7
       YORK       1.00      0.88      0.93         8

avg / total       0.70      0.68      0.66        31

=========================================================================================================

=========================================================================================================
Speaker: QUEEN
Number of Total Listeners: 121

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
482 features selected out of 965 total
F1 mean: 0.06 (+/- 0.05)

             precision    recall  f1-score   support

 BUCKINGHAM       0.00      0.00      0.00         7
   CARDINAL       0.00      0.00      0.00        11
 GLOUCESTER       0.00      0.00      0.00         8
 KING HENRY       0.44      0.69      0.53        35
   SOMERSET       0.50      0.14      0.22         7
    SUFFOLK       0.48      0.82      0.61        38
    WARWICK       0.00      0.00      0.00         5
       YORK       0.00      0.00      0.00        10

avg / total       0.31      0.46      0.36       121

=========================================================================================================

=========================================================================================================
Speaker: SUFFOLK
Number of Total Listeners: 128

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
491 features selected out of 983 total
F1 mean: 0.05 (+/- 0.01)

             precision    recall  f1-score   support

   CARDINAL       0.33      0.06      0.11        16
 GLOUCESTER       0.67      0.15      0.25        13
 KING HENRY       0.47      0.40      0.43        20
 LIEUTENANT       0.50      0.42      0.45        12
      QUEEN       0.44      0.97      0.61        34
    WARWICK       0.00      0.00      0.00         8
   WHITMORE       0.50      0.30      0.37        10
       YORK       0.43      0.40      0.41        15

avg / total       0.44      0.45      0.39       128

=========================================================================================================

=========================================================================================================
Speaker: DICK
Number of Total Listeners: 33

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
84 features selected out of 169 total
F1 mean: 0.41 (+/- 0.02)

             precision    recall  f1-score   support

       CADE       0.70      1.00      0.82        23
      SMITH       0.00      0.00      0.00        10

avg / total       0.49      0.70      0.57        33

=========================================================================================================

=========================================================================================================
Speaker: SALISBURY
Number of Total Listeners: 24

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
197 features selected out of 395 total
F1 mean: 0.17 (+/- 0.14)

             precision    recall  f1-score   support

 KING HENRY       1.00      0.60      0.75         5
    WARWICK       0.50      0.90      0.64        10
       YORK       0.67      0.22      0.33         9

avg / total       0.67      0.58      0.55        24

=========================================================================================================

=========================================================================================================
Speaker: SIMPCOX
Number of Total Listeners: 30

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
54 features selected out of 108 total
F1 mean: 0.23 (+/- 0.05)

             precision    recall  f1-score   support

 GLOUCESTER       0.57      1.00      0.73        16
 KING HENRY       0.50      0.17      0.25         6
       WIFE       0.00      0.00      0.00         8

avg / total       0.40      0.57      0.44        30

=========================================================================================================

=========================================================================================================
Speaker: KING HENRY
Number of Total Listeners: 187

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
472 features selected out of 945 total
F1 mean: 0.02 (+/- 0.04)

             precision    recall  f1-score   support

 BUCKINGHAM       0.56      0.45      0.50        20
   CARDINAL       0.40      0.11      0.17        19
   CLIFFORD       0.00      0.00      0.00         6
 GLOUCESTER       0.41      0.46      0.44        26
      QUEEN       0.32      0.93      0.48        42
  SALISBURY       0.33      0.12      0.18         8
        SAY       0.00      0.00      0.00         6
    SIMPCOX       0.00      0.00      0.00         6
   SOMERSET       0.00      0.00      0.00         7
    SUFFOLK       0.44      0.19      0.27        21
    WARWICK       0.50      0.11      0.18         9
       YORK       1.00      0.12      0.21        17

avg / total       0.41      0.37      0.30       187

=========================================================================================================

=========================================================================================================
Speaker: GLOUCESTER
Number of Total Listeners: 150

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
467 features selected out of 934 total
F1 mean: 0.10 (+/- 0.06)

             precision    recall  f1-score   support

   CARDINAL       0.35      0.29      0.31        28
    DUCHESS       1.00      0.77      0.87        13
 KING HENRY       0.38      0.60      0.46        30
      MAYOR       0.00      0.00      0.00         5
      QUEEN       0.40      0.15      0.22        13
    SIMPCOX       0.47      0.94      0.63        18
    SUFFOLK       0.35      0.38      0.36        24
       WIFE       0.00      0.00      0.00        10
       YORK       0.50      0.11      0.18         9

avg / total       0.40      0.43      0.39       150

=========================================================================================================

Play: 12

=========================================================================================================
Speaker: RICHARD
Number of Total Listeners: 232

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
578 features selected out of 1156 total
F1 mean: 0.04 (+/- 0.01)

             precision    recall  f1-score   support

   CLARENCE       1.00      0.03      0.05        36
   CLIFFORD       0.00      0.00      0.00         8
     EDWARD       0.39      1.00      0.56        87
   HASTINGS       0.00      0.00      0.00        10
 KING HENRY       1.00      0.15      0.27        13
  LADY GREY       0.00      0.00      0.00        10
   MONTAGUE       0.00      0.00      0.00         7
PRINCE OF WALES       0.00      0.00      0.00         6
QUEEN MARGARET       0.33      0.08      0.12        13
    WARWICK       0.33      0.03      0.06        32
       YORK       1.00      0.10      0.18        10

avg / total       0.47      0.40      0.26       232

=========================================================================================================

=========================================================================================================
Speaker: WARWICK
Number of Total Listeners: 213

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
563 features selected out of 1127 total
F1 mean: 0.05 (+/- 0.03)

             precision    recall  f1-score   support

       BONA       0.00      0.00      0.00         5
   CLARENCE       0.60      0.20      0.30        15
   CLIFFORD       0.00      0.00      0.00        10
     EDWARD       0.44      0.58      0.50        36
     EXETER       0.00      0.00      0.00         7
 KING HENRY       0.33      0.71      0.45        24
      LEWIS       0.43      0.23      0.30        13
NORTHUMBERLAND       0.00      0.00      0.00         6
     OXFORD       0.50      0.25      0.33        12
PRINCE OF WALES       0.00      0.00      0.00         7
QUEEN MARGARET       0.41      0.56      0.47        16
    RICHARD       0.39      0.58      0.47        36
   SOMERSET       0.50      0.12      0.20         8
       YORK       0.39      0.39      0.39        18

avg / total       0.36      0.40      0.35       213

=========================================================================================================

=========================================================================================================
Speaker: PRINCE OF WALES
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
112 features selected out of 224 total
F1 mean: 0.20 (+/- 0.20)

             precision    recall  f1-score   support

 KING HENRY       0.00      0.00      0.00         5
QUEEN MARGARET       0.62      1.00      0.77        15
    RICHARD       1.00      0.20      0.33         5

avg / total       0.57      0.64      0.53        25

=========================================================================================================

=========================================================================================================
Speaker: NORTHUMBERLAND
Number of Total Listeners: 36

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
81 features selected out of 163 total
F1 mean: 0.08 (+/- 0.04)

             precision    recall  f1-score   support

   CLIFFORD       0.38      1.00      0.55        13
 KING HENRY       0.00      0.00      0.00         5
QUEEN MARGARET       0.00      0.00      0.00         6
    WARWICK       0.00      0.00      0.00         6
       YORK       0.50      0.17      0.25         6

avg / total       0.22      0.39      0.24        36

=========================================================================================================

=========================================================================================================
Speaker: EXETER
Number of Total Listeners: 26

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
51 features selected out of 102 total
F1 mean: 0.21 (+/- 0.04)

             precision    recall  f1-score   support

 KING HENRY       0.50      1.00      0.67        12
    WARWICK       0.00      0.00      0.00         7
       YORK       0.50      0.14      0.22         7

avg / total       0.37      0.50      0.37        26

=========================================================================================================

=========================================================================================================
Speaker: CLARENCE
Number of Total Listeners: 85

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
223 features selected out of 446 total
F1 mean: 0.11 (+/- 0.07)

             precision    recall  f1-score   support

     EDWARD       0.42      0.71      0.53        31
 KING HENRY       0.00      0.00      0.00         6
QUEEN MARGARET       0.00      0.00      0.00         6
    RICHARD       0.50      0.32      0.39        28
    WARWICK       0.47      0.50      0.48        14

avg / total       0.40      0.45      0.40        85

=========================================================================================================

=========================================================================================================
Speaker: LADY GREY
Number of Total Listeners: 44

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
85 features selected out of 170 total
F1 mean: 0.24 (+/- 0.03)

             precision    recall  f1-score   support

   CLARENCE       0.00      0.00      0.00         7
     EDWARD       0.55      1.00      0.71        24
    RICHARD       0.00      0.00      0.00        13

avg / total       0.30      0.55      0.39        44

=========================================================================================================

=========================================================================================================
Speaker: YORK
Number of Total Listeners: 87

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
275 features selected out of 551 total
F1 mean: 0.06 (+/- 0.08)

             precision    recall  f1-score   support

   CLIFFORD       0.57      0.50      0.53         8
     EDWARD       0.00      0.00      0.00         8
     EXETER       0.00      0.00      0.00         6
 KING HENRY       0.33      0.50      0.40        14
   MONTAGUE       0.33      0.14      0.20         7
    NORFOLK       0.00      0.00      0.00         5
QUEEN MARGARET       0.67      0.40      0.50         5
    RICHARD       0.45      0.88      0.60        17
    WARWICK       0.35      0.41      0.38        17

avg / total       0.33      0.41      0.35        87

=========================================================================================================

=========================================================================================================
Speaker: CLIFFORD
Number of Total Listeners: 75

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
221 features selected out of 443 total
F1 mean: 0.06 (+/- 0.09)

             precision    recall  f1-score   support

 KING HENRY       0.38      0.50      0.43        10
NORTHUMBERLAND       0.32      1.00      0.48        15
QUEEN MARGARET       0.50      0.18      0.27        11
    RICHARD       1.00      0.33      0.50         6
    RUTLAND       1.00      0.78      0.88         9
    WARWICK       0.50      0.12      0.20         8
WESTMORELAND       0.00      0.00      0.00         7
       YORK       0.00      0.00      0.00         9

avg / total       0.44      0.43      0.36        75

=========================================================================================================

=========================================================================================================
Speaker: QUEEN MARGARET
Number of Total Listeners: 117

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
457 features selected out of 914 total
F1 mean: 0.05 (+/- 0.02)

             precision    recall  f1-score   support

   CLARENCE       0.00      0.00      0.00         7
   CLIFFORD       0.36      0.67      0.47        12
     EDWARD       0.45      0.42      0.43        12
 KING HENRY       0.33      0.09      0.14        11
      LEWIS       0.48      0.59      0.53        17
NORTHUMBERLAND       0.00      0.00      0.00         6
     OXFORD       0.50      0.40      0.44         5
PRINCE OF WALES       0.44      0.78      0.56        18
    RICHARD       0.00      0.00      0.00         7
    WARWICK       0.38      0.56      0.45        16
       YORK       0.00      0.00      0.00         6

avg / total       0.32      0.42      0.35       117

=========================================================================================================

=========================================================================================================
Speaker: EDWARD
Number of Total Listeners: 277

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
595 features selected out of 1190 total
F1 mean: 0.04 (+/- 0.01)

             precision    recall  f1-score   support

   CLARENCE       0.44      0.16      0.23        45
   HASTINGS       0.00      0.00      0.00        15
   HUNTSMAN       0.00      0.00      0.00         5
  LADY GREY       0.75      0.25      0.38        24
  MESSENGER       0.00      0.00      0.00         9
   MONTAGUE       0.00      0.00      0.00         8
 MONTGOMERY       1.00      0.17      0.29         6
     OXFORD       0.00      0.00      0.00         5
PRINCE OF WALES       0.00      0.00      0.00         6
QUEEN MARGARET       0.50      0.14      0.22        14
    RICHARD       0.41      0.96      0.58       102
    WARWICK       0.64      0.21      0.32        33
       YORK       0.00      0.00      0.00         5

avg / total       0.41      0.44      0.34       277

=========================================================================================================

=========================================================================================================
Speaker: LEWIS
Number of Total Listeners: 44

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
142 features selected out of 284 total
F1 mean: 0.13 (+/- 0.08)

             precision    recall  f1-score   support

       BONA       0.00      0.00      0.00         5
PRINCE OF WALES       0.00      0.00      0.00         5
QUEEN MARGARET       0.49      1.00      0.66        19
    WARWICK       0.40      0.13      0.20        15

avg / total       0.35      0.48      0.35        44

=========================================================================================================

=========================================================================================================
Speaker: KING HENRY
Number of Total Listeners: 159

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
479 features selected out of 959 total
F1 mean: 0.03 (+/- 0.04)

             precision    recall  f1-score   support

   CLARENCE       0.00      0.00      0.00         8
   CLIFFORD       0.40      0.31      0.35        13
     EXETER       0.33      0.33      0.33        15
     FATHER       0.50      0.20      0.29         5
FIRST KEEPER       0.62      0.56      0.59         9
   MONTAGUE       0.00      0.00      0.00         5
NORTHUMBERLAND       0.00      0.00      0.00         7
PRINCE OF WALES       0.00      0.00      0.00         8
QUEEN MARGARET       0.38      0.38      0.38        13
    RICHARD       0.80      0.62      0.70        13
SECOND KEEPER       0.57      0.40      0.47        10
        SON       0.50      0.60      0.55         5
    WARWICK       0.31      0.90      0.46        29
       YORK       0.40      0.11      0.17        19

avg / total       0.37      0.40      0.34       159

=========================================================================================================

Play: 13

=========================================================================================================
Speaker: BUCKINGHAM
Number of Total Listeners: 30

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
234 features selected out of 468 total
F1 mean: 0.22 (+/- 0.09)

             precision    recall  f1-score   support

ABERGAVENNY       0.00      0.00      0.00         7
    BRANDON       0.75      0.60      0.67         5
    NORFOLK       0.69      1.00      0.82        18

avg / total       0.54      0.70      0.60        30

=========================================================================================================

=========================================================================================================
Speaker: CHAMBERLAIN
Number of Total Listeners: 56

Best model LogisticRegression(C=0.59999999999999998, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
220 features selected out of 441 total
F1 mean: 0.20 (+/- 0.22)

             precision    recall  f1-score   support

       ANNE       1.00      0.67      0.80         6
     LOVELL       0.00      0.00      0.00        10
    NORFOLK       0.67      0.25      0.36         8
     SANDYS       0.54      1.00      0.70        15
    SUFFOLK       0.60      0.90      0.72        10
     WOLSEY       1.00      0.86      0.92         7

avg / total       0.58      0.64      0.57        56

=========================================================================================================

=========================================================================================================
Speaker: KING
Number of Total Listeners: 143

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
566 features selected out of 1133 total
F1 mean: 0.03 (+/- 0.03)

             precision    recall  f1-score   support

   CAMPEIUS       0.50      0.22      0.31         9
    CRANMER       0.72      0.76      0.74        17
   GARDINER       0.40      0.29      0.33         7
    LINCOLN       0.50      0.20      0.29         5
     LOVELL       0.50      0.27      0.35        11
    NORFOLK       0.50      0.18      0.27        11
QUEEN KATHARINE       0.40      0.78      0.53        18
    SUFFOLK       0.50      0.33      0.40        12
     SURREY       0.00      0.00      0.00         7
   SURVEYOR       0.00      0.00      0.00        12
     WOLSEY       0.46      0.82      0.59        34

avg / total       0.43      0.48      0.42       143

=========================================================================================================

=========================================================================================================
Speaker: QUEEN KATHARINE
Number of Total Listeners: 81

Best model LogisticRegression(C=1.1000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
507 features selected out of 1014 total
F1 mean: 0.15 (+/- 0.06)

             precision    recall  f1-score   support

   CAMPEIUS       0.50      0.28      0.36        18
   CAPUCIUS       0.62      0.83      0.71         6
   GRIFFITH       0.92      1.00      0.96        12
       KING       0.60      0.64      0.62        14
   PATIENCE       0.50      0.20      0.29         5
     WOLSEY       0.55      0.69      0.61        26

avg / total       0.60      0.62      0.60        81

=========================================================================================================

=========================================================================================================
Speaker: CROMWELL
Number of Total Listeners: 32

Best model LogisticRegression(C=2.3000000000000003, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
100 features selected out of 201 total
F1 mean: 0.22 (+/- 0.30)

             precision    recall  f1-score   support

 CHANCELLOR       0.00      0.00      0.00         5
   GARDINER       0.53      1.00      0.69         9
    NORFOLK       0.50      0.17      0.25         6
     WOLSEY       0.85      0.92      0.88        12

avg / total       0.56      0.66      0.57        32

=========================================================================================================

=========================================================================================================
Speaker: SURREY
Number of Total Listeners: 51

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
156 features selected out of 313 total
F1 mean: 0.16 (+/- 0.10)

             precision    recall  f1-score   support

CHAMBERLAIN       0.00      0.00      0.00        10
    NORFOLK       0.33      0.07      0.12        14
    SUFFOLK       0.39      0.94      0.55        17
     WOLSEY       0.71      0.50      0.59        10

avg / total       0.36      0.43      0.33        51

=========================================================================================================

=========================================================================================================
Speaker: NORFOLK
Number of Total Listeners: 92

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
365 features selected out of 731 total
F1 mean: 0.05 (+/- 0.04)

             precision    recall  f1-score   support

ABERGAVENNY       0.00      0.00      0.00         6
 BUCKINGHAM       0.71      0.88      0.79        17
CHAMBERLAIN       0.50      0.08      0.13        13
       KING       1.00      0.12      0.22         8
    SUFFOLK       0.39      0.96      0.55        25
     SURREY       0.00      0.00      0.00        11
     WOLSEY       0.50      0.25      0.33        12

avg / total       0.46      0.48      0.38        92

=========================================================================================================

=========================================================================================================
Speaker: GARDINER
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
188 features selected out of 376 total
F1 mean: 0.28 (+/- 0.20)

             precision    recall  f1-score   support

 CHANCELLOR       1.00      0.20      0.33         5
    CRANMER       0.75      0.50      0.60         6
   CROMWELL       0.50      0.89      0.64         9
     LOVELL       1.00      0.86      0.92         7

avg / total       0.78      0.67      0.65        27

=========================================================================================================

=========================================================================================================
Speaker: SECOND GENTLEMAN
Number of Total Listeners: 41

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
175 features selected out of 351 total
F1 mean: 0.45 (+/- 0.03)

             precision    recall  f1-score   support

FIRST GENTLEMAN       0.83      1.00      0.91        34
THIRD GENTLEMAN       0.00      0.00      0.00         7

avg / total       0.69      0.83      0.75        41

=========================================================================================================

=========================================================================================================
Speaker: WOLSEY
Number of Total Listeners: 141

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
566 features selected out of 1133 total
F1 mean: 0.08 (+/- 0.05)

             precision    recall  f1-score   support

   CAMPEIUS       0.00      0.00      0.00        17
CHAMBERLAIN       0.83      0.38      0.53        13
   CROMWELL       1.00      0.46      0.63        13
       KING       0.41      0.91      0.56        34
    NORFOLK       0.52      0.59      0.55        22
QUEEN KATHARINE       0.52      0.60      0.56        20
    SUFFOLK       0.00      0.00      0.00         9
     SURREY       0.60      0.23      0.33        13

avg / total       0.48      0.50      0.44       141

=========================================================================================================

=========================================================================================================
Speaker: SANDYS
Number of Total Listeners: 31

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
109 features selected out of 218 total
F1 mean: 0.19 (+/- 0.13)

             precision    recall  f1-score   support

       ANNE       1.00      0.25      0.40         8
CHAMBERLAIN       0.52      1.00      0.68        15
     LOVELL       0.00      0.00      0.00         8

avg / total       0.51      0.55      0.43        31

=========================================================================================================

=========================================================================================================
Speaker: CAMPEIUS
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
119 features selected out of 238 total
F1 mean: 0.15 (+/- 0.11)

             precision    recall  f1-score   support

       KING       1.00      0.17      0.29         6
QUEEN KATHARINE       0.00      0.00      0.00         8
     WOLSEY       0.50      1.00      0.67        13

avg / total       0.46      0.52      0.38        27

=========================================================================================================

=========================================================================================================
Speaker: LOVELL
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
139 features selected out of 278 total
F1 mean: 0.24 (+/- 0.49)

             precision    recall  f1-score   support

CHAMBERLAIN       0.00      0.00      0.00         8
   GARDINER       1.00      1.00      1.00         6
       KING       1.00      1.00      1.00         5
     SANDYS       0.50      1.00      0.67         8

avg / total       0.56      0.70      0.60        27

=========================================================================================================

=========================================================================================================
Speaker: SUFFOLK
Number of Total Listeners: 64

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
171 features selected out of 342 total
F1 mean: 0.08 (+/- 0.05)

             precision    recall  f1-score   support

CHAMBERLAIN       0.00      0.00      0.00        14
       KING       1.00      0.33      0.50         6
    NORFOLK       0.40      0.92      0.56        24
     SURREY       0.43      0.20      0.27        15
     WOLSEY       0.00      0.00      0.00         5

avg / total       0.34      0.42      0.32        64

=========================================================================================================

Play: 14

=========================================================================================================
Speaker: BASTARD
Number of Total Listeners: 174

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
712 features selected out of 1424 total
F1 mean: 0.05 (+/- 0.04)

             precision    recall  f1-score   support

    AUSTRIA       0.33      0.23      0.27        13
     BLANCH       0.00      0.00      0.00         5
    CITIZEN       0.00      0.00      0.00         6
  CONSTANCE       0.00      0.00      0.00         5
     ELINOR       0.50      0.07      0.12        14
     HUBERT       0.68      0.76      0.72        17
  KING JOHN       0.37      1.00      0.54        44
KING PHILIP       0.00      0.00      0.00        14
LADY FAULCONBRIDGE       1.00      0.50      0.67         6
      LEWIS       0.50      0.38      0.43         8
   PANDULPH       0.00      0.00      0.00         6
   PEMBROKE       0.00      0.00      0.00         8
PRINCE HENRY       0.00      0.00      0.00         7
     ROBERT       0.00      0.00      0.00         6
  SALISBURY       0.53      0.60      0.56        15

avg / total       0.33      0.44      0.33       174

=========================================================================================================

=========================================================================================================
Speaker: PANDULPH
Number of Total Listeners: 32

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
287 features selected out of 574 total
F1 mean: 0.19 (+/- 0.26)

             precision    recall  f1-score   support

  CONSTANCE       0.50      0.67      0.57         6
  KING JOHN       0.75      0.60      0.67         5
KING PHILIP       0.62      0.56      0.59         9
      LEWIS       0.92      0.92      0.92        12

avg / total       0.73      0.72      0.72        32

=========================================================================================================

=========================================================================================================
Speaker: HUBERT
Number of Total Listeners: 70

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
244 features selected out of 489 total
F1 mean: 0.18 (+/- 0.18)

             precision    recall  f1-score   support

     ARTHUR       0.82      1.00      0.90        18
    BASTARD       0.63      1.00      0.77        17
      BIGOT       0.00      0.00      0.00         5
EXECUTIONER       1.00      0.33      0.50         6
  KING JOHN       1.00      1.00      1.00        12
   PEMBROKE       0.00      0.00      0.00         5
  SALISBURY       0.43      0.43      0.43         7

avg / total       0.66      0.74      0.68        70

=========================================================================================================

=========================================================================================================
Speaker: CITIZEN
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
125 features selected out of 250 total
F1 mean: 0.15 (+/- 0.17)

             precision    recall  f1-score   support

    BASTARD       0.50      0.17      0.25         6
  KING JOHN       0.36      0.44      0.40         9
KING PHILIP       0.40      0.50      0.44         8

avg / total       0.41      0.39      0.38        23

=========================================================================================================

=========================================================================================================
Speaker: PEMBROKE
Number of Total Listeners: 33

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
173 features selected out of 346 total
F1 mean: 0.16 (+/- 0.19)

             precision    recall  f1-score   support

    BASTARD       0.00      0.00      0.00         6
  KING JOHN       0.50      0.12      0.20         8
  SALISBURY       0.58      0.95      0.72        19

avg / total       0.46      0.58      0.46        33

=========================================================================================================

=========================================================================================================
Speaker: LEWIS
Number of Total Listeners: 34

Best model LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=200, multi_class='multinomial',
          penalty='l1', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=1)
182 features selected out of 365 total
F1 mean: 0.11 (+/- 0.08)

             precision    recall  f1-score   support

    BASTARD       0.67      0.33      0.44         6
     BLANCH       0.38      0.60      0.46         5
  KING JOHN       0.50      0.20      0.29         5
KING PHILIP       0.33      0.20      0.25         5
   PANDULPH       0.72      1.00      0.84        13

avg / total       0.57      0.59      0.55        34

=========================================================================================================

=========================================================================================================
Speaker: KING PHILIP
Number of Total Listeners: 108

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
342 features selected out of 684 total
F1 mean: 0.03 (+/- 0.03)

             precision    recall  f1-score   support

    AUSTRIA       0.40      0.15      0.22        13
    BASTARD       0.50      0.07      0.12        14
    CITIZEN       0.00      0.00      0.00        12
  CONSTANCE       0.56      0.36      0.43        14
     ELINOR       0.00      0.00      0.00         6
  KING JOHN       0.34      0.96      0.51        25
      LEWIS       0.50      0.10      0.17        10
   PANDULPH       0.40      0.57      0.47        14

avg / total       0.36      0.38      0.29       108

=========================================================================================================

=========================================================================================================
Speaker: KING JOHN
Number of Total Listeners: 175

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
585 features selected out of 1171 total
F1 mean: 0.05 (+/- 0.07)

             precision    recall  f1-score   support

    AUSTRIA       0.00      0.00      0.00         8
    BASTARD       0.34      0.90      0.49        40
     BLANCH       0.00      0.00      0.00         7
  CHATILLON       0.00      0.00      0.00         5
    CITIZEN       1.00      0.08      0.15        12
  CONSTANCE       0.00      0.00      0.00         6
     ELINOR       0.62      0.32      0.42        25
     HUBERT       0.80      0.47      0.59        17
KING PHILIP       0.39      0.50      0.44        24
      LEWIS       0.00      0.00      0.00         6
  MESSENGER       1.00      0.25      0.40         8
   PEMBROKE       0.50      0.14      0.22         7
  SALISBURY       0.60      0.60      0.60        10

avg / total       0.46      0.42      0.36       175

=========================================================================================================

=========================================================================================================
Speaker: ELINOR
Number of Total Listeners: 40

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
129 features selected out of 259 total
F1 mean: 0.16 (+/- 0.12)

             precision    recall  f1-score   support

    BASTARD       0.75      0.30      0.43        10
  CONSTANCE       0.80      0.44      0.57         9
  KING JOHN       0.48      1.00      0.65        15
KING PHILIP       0.00      0.00      0.00         6

avg / total       0.55      0.55      0.48        40

=========================================================================================================

=========================================================================================================
Speaker: CONSTANCE
Number of Total Listeners: 88

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
411 features selected out of 823 total
F1 mean: 0.03 (+/- 0.04)

             precision    recall  f1-score   support

     ARTHUR       0.40      0.33      0.36         6
    AUSTRIA       0.50      0.10      0.17        10
     BLANCH       0.33      0.17      0.22         6
     ELINOR       0.40      0.18      0.25        11
  KING JOHN       0.33      0.58      0.42        12
KING PHILIP       0.44      0.74      0.55        19
      LEWIS       0.50      0.17      0.25         6
   PANDULPH       0.45      0.42      0.43        12
  SALISBURY       0.43      0.50      0.46         6

avg / total       0.42      0.41      0.37        88

=========================================================================================================

=========================================================================================================
Speaker: AUSTRIA
Number of Total Listeners: 36

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
87 features selected out of 175 total
F1 mean: 0.20 (+/- 0.19)

             precision    recall  f1-score   support

    BASTARD       0.38      0.90      0.53        10
  CONSTANCE       0.60      0.33      0.43         9
  KING JOHN       0.00      0.00      0.00         7
KING PHILIP       0.57      0.40      0.47        10

avg / total       0.41      0.44      0.38        36

=========================================================================================================

=========================================================================================================
Speaker: SALISBURY
Number of Total Listeners: 66

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
226 features selected out of 453 total
F1 mean: 0.07 (+/- 0.05)

             precision    recall  f1-score   support

    BASTARD       0.43      0.43      0.43        14
      BIGOT       0.00      0.00      0.00        11
     HUBERT       0.00      0.00      0.00         5
  KING JOHN       0.00      0.00      0.00         7
   PEMBROKE       0.44      0.96      0.61        24
PRINCE HENRY       0.00      0.00      0.00         5

avg / total       0.25      0.44      0.31        66

=========================================================================================================

Play: 15

=========================================================================================================
Speaker: FIRST CITIZEN
Number of Total Listeners: 53

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
45 features selected out of 90 total
F1 mean: 0.10 (+/- 0.03)

             precision    recall  f1-score   support

        ALL       0.00      0.00      0.00         7
     ANTONY       0.25      0.14      0.18         7
FOURTH CITIZEN       0.00      0.00      0.00         7
SECOND CITIZEN       0.33      0.75      0.46        16
THIRD CITIZEN       0.38      0.31      0.34        16

avg / total       0.25      0.34      0.27        53

=========================================================================================================

=========================================================================================================
Speaker: FOURTH CITIZEN
Number of Total Listeners: 40

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
49 features selected out of 99 total
F1 mean: 0.10 (+/- 0.04)

             precision    recall  f1-score   support

     ANTONY       1.00      0.14      0.25         7
FIRST CITIZEN       0.00      0.00      0.00         9
SECOND CITIZEN       0.50      0.10      0.17        10
THIRD CITIZEN       0.38      1.00      0.55        14

avg / total       0.43      0.40      0.28        40

=========================================================================================================

=========================================================================================================
Speaker: MESSALA
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
80 features selected out of 160 total
F1 mean: 0.32 (+/- 0.32)

             precision    recall  f1-score   support

     BRUTUS       0.52      1.00      0.69        11
    CASSIUS       0.67      0.22      0.33         9
   TITINIUS       1.00      0.43      0.60         7

avg / total       0.69      0.59      0.55        27

=========================================================================================================

=========================================================================================================
Speaker: BRUTUS
Number of Total Listeners: 275

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
675 features selected out of 1351 total
F1 mean: 0.05 (+/- 0.02)

             precision    recall  f1-score   support

     ANTONY       0.00      0.00      0.00        13
     CAESAR       0.00      0.00      0.00         8
      CASCA       0.00      0.00      0.00        13
    CASSIUS       0.49      1.00      0.66       119
       CATO       1.00      0.20      0.33         5
      CINNA       0.00      0.00      0.00         7
     CLITUS       1.00      0.60      0.75        10
     DECIUS       0.00      0.00      0.00         8
   LIGARIUS       0.00      0.00      0.00         6
   LUCILIUS       1.00      0.25      0.40         8
     LUCIUS       0.68      0.52      0.59        29
    MESSALA       0.00      0.00      0.00        13
   METELLUS       0.00      0.00      0.00         8
   OCTAVIUS       0.00      0.00      0.00         6
     PORTIA       0.00      0.00      0.00         7
   TITINIUS       0.00      0.00      0.00         7
      VARRO       0.00      0.00      0.00         8

avg / total       0.37      0.52      0.39       275

=========================================================================================================

=========================================================================================================
Speaker: LUCIUS
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
61 features selected out of 123 total
F1 mean: 0.44 (+/- 0.00)

             precision    recall  f1-score   support

     BRUTUS       0.83      1.00      0.91        20
     PORTIA       1.00      0.20      0.33         5

avg / total       0.87      0.84      0.79        25

=========================================================================================================

=========================================================================================================
Speaker: CAESAR
Number of Total Listeners: 88

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
242 features selected out of 485 total
F1 mean: 0.05 (+/- 0.09)

             precision    recall  f1-score   support

     ANTONY       0.33      0.10      0.15        10
     BRUTUS       0.38      0.87      0.53        15
  CALPURNIA       0.67      0.67      0.67        12
      CASCA       0.32      0.58      0.41        12
    CASSIUS       0.40      0.36      0.38        11
      CINNA       0.00      0.00      0.00         7
     DECIUS       0.71      0.42      0.53        12
 SOOTHSAYER       0.00      0.00      0.00         9

avg / total       0.38      0.43      0.37        88

=========================================================================================================

=========================================================================================================
Speaker: ANTONY
Number of Total Listeners: 103

Best model LogisticRegression(C=0.59999999999999998, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
450 features selected out of 901 total
F1 mean: 0.05 (+/- 0.09)

             precision    recall  f1-score   support

        ALL       0.37      0.47      0.41        15
     BRUTUS       0.50      0.47      0.48        15
     CAESAR       0.71      1.00      0.83         5
    CASSIUS       0.45      0.42      0.43        12
FIRST CITIZEN       0.32      0.64      0.42        11
FOURTH CITIZEN       0.50      0.17      0.25         6
   OCTAVIUS       0.73      0.85      0.79        13
SECOND CITIZEN       0.29      0.18      0.22        11
    SERVANT       0.75      0.50      0.60         6
THIRD CITIZEN       0.50      0.11      0.18         9

avg / total       0.49      0.48      0.45       103

=========================================================================================================

=========================================================================================================
Speaker: CASSIUS
Number of Total Listeners: 204

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
599 features selected out of 1198 total
F1 mean: 0.07 (+/- 0.01)

             precision    recall  f1-score   support

     ANTONY       0.00      0.00      0.00        11
     BRUTUS       0.59      1.00      0.75       113
     CAESAR       0.00      0.00      0.00         8
      CASCA       0.75      0.32      0.45        28
      CINNA       0.00      0.00      0.00        10
     DECIUS       0.00      0.00      0.00        11
   LUCILIUS       0.00      0.00      0.00         5
    MESSALA       0.00      0.00      0.00         8
   PINDARUS       1.00      0.40      0.57         5
  TREBONIUS       0.00      0.00      0.00         5

avg / total       0.46      0.61      0.49       204

=========================================================================================================

=========================================================================================================
Speaker: CASCA
Number of Total Listeners: 70

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
245 features selected out of 490 total
F1 mean: 0.08 (+/- 0.03)

             precision    recall  f1-score   support

     BRUTUS       0.50      0.10      0.17        20
     CAESAR       0.00      0.00      0.00         5
    CASSIUS       0.48      1.00      0.65        30
     CICERO       1.00      0.60      0.75         5
      CINNA       0.00      0.00      0.00         5
     DECIUS       0.00      0.00      0.00         5

avg / total       0.42      0.50      0.38        70

=========================================================================================================

=========================================================================================================
Speaker: OCTAVIUS
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
99 features selected out of 199 total
F1 mean: 0.19 (+/- 0.10)

             precision    recall  f1-score   support

     ANTONY       0.56      1.00      0.71        15
     BRUTUS       0.00      0.00      0.00         7
    CASSIUS       0.00      0.00      0.00         5

avg / total       0.31      0.56      0.40        27

=========================================================================================================

=========================================================================================================
Speaker: THIRD CITIZEN
Number of Total Listeners: 45

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
51 features selected out of 102 total
F1 mean: 0.12 (+/- 0.06)

             precision    recall  f1-score   support

     ANTONY       0.00      0.00      0.00         7
FIRST CITIZEN       0.33      0.71      0.45        14
FOURTH CITIZEN       0.40      0.31      0.35        13
SECOND CITIZEN       0.40      0.18      0.25        11

avg / total       0.32      0.36      0.30        45

=========================================================================================================

=========================================================================================================
Speaker: ALL
Number of Total Listeners: 30

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
22 features selected out of 45 total
F1 mean: 0.12 (+/- 0.05)

             precision    recall  f1-score   support

     ANTONY       0.39      0.90      0.55        10
FIRST CITIZEN       0.40      0.29      0.33         7
SECOND CITIZEN       0.50      0.12      0.20         8
THIRD CITIZEN       0.00      0.00      0.00         5

avg / total       0.36      0.40      0.31        30

=========================================================================================================

=========================================================================================================
Speaker: CINNA
Number of Total Listeners: 38

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
66 features selected out of 133 total
F1 mean: 0.05 (+/- 0.06)

             precision    recall  f1-score   support

     BRUTUS       0.75      0.50      0.60         6
      CASCA       0.33      0.20      0.25         5
    CASSIUS       0.58      0.88      0.70         8
FIRST CITIZEN       0.38      0.86      0.52         7
FOURTH CITIZEN       0.00      0.00      0.00         6
THIRD CITIZEN       0.33      0.17      0.22         6

avg / total       0.41      0.47      0.41        38

=========================================================================================================

=========================================================================================================
Speaker: SECOND CITIZEN
Number of Total Listeners: 57

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
49 features selected out of 98 total
F1 mean: 0.09 (+/- 0.03)

             precision    recall  f1-score   support

        ALL       0.00      0.00      0.00         8
     ANTONY       0.33      0.10      0.15        10
FIRST CITIZEN       0.33      0.93      0.48        15
FOURTH CITIZEN       0.25      0.17      0.20        12
THIRD CITIZEN       0.33      0.08      0.13        12

avg / total       0.27      0.32      0.22        57

=========================================================================================================

Play: 16

=========================================================================================================
Speaker: CORNWALL
Number of Total Listeners: 115

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
108 features selected out of 217 total
F1 mean: 0.09 (+/- 0.05)

             precision    recall  f1-score   support

     EDMUND       1.00      0.10      0.18        10
 GLOUCESTER       0.46      0.38      0.42        29
    GONERIL       0.00      0.00      0.00        10
       KENT       0.67      0.12      0.21        16
       LEAR       0.00      0.00      0.00         6
     OSWALD       0.00      0.00      0.00        10
      REGAN       0.34      0.88      0.50        34

avg / total       0.40      0.38      0.30       115

=========================================================================================================

=========================================================================================================
Speaker: ALBANY
Number of Total Listeners: 128

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
111 features selected out of 222 total
F1 mean: 0.05 (+/- 0.03)

             precision    recall  f1-score   support

    CAPTAIN       0.00      0.00      0.00         6
      EDGAR       0.33      0.18      0.24        22
     EDMUND       0.38      0.62      0.47        29
  GENTLEMAN       0.00      0.00      0.00         9
    GONERIL       0.39      0.90      0.55        29
       KENT       0.00      0.00      0.00         9
       LEAR       0.33      0.08      0.12        13
      REGAN       0.00      0.00      0.00        11

avg / total       0.27      0.38      0.28       128

=========================================================================================================

=========================================================================================================
Speaker: CORDELIA
Number of Total Listeners: 48

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
56 features selected out of 113 total
F1 mean: 0.16 (+/- 0.10)

             precision    recall  f1-score   support

     DOCTOR       0.64      0.50      0.56        14
       KENT       0.00      0.00      0.00        10
       LEAR       0.51      1.00      0.68        19
      REGAN       0.00      0.00      0.00         5

avg / total       0.39      0.54      0.43        48

=========================================================================================================

=========================================================================================================
Speaker: EDMUND
Number of Total Listeners: 101

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
136 features selected out of 272 total
F1 mean: 0.13 (+/- 0.05)

             precision    recall  f1-score   support

     ALBANY       0.47      0.41      0.44        17
   CORNWALL       0.00      0.00      0.00         7
      EDGAR       0.56      0.67      0.61        21
 GLOUCESTER       0.50      1.00      0.67        30
    GONERIL       0.00      0.00      0.00         7
       KENT       0.00      0.00      0.00         6
      REGAN       1.00      0.08      0.14        13

avg / total       0.47      0.51      0.42       101

=========================================================================================================

=========================================================================================================
Speaker: FOOL
Number of Total Listeners: 100

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
160 features selected out of 321 total
F1 mean: 0.18 (+/- 0.01)

             precision    recall  f1-score   support

      EDGAR       0.00      0.00      0.00        11
    GONERIL       0.00      0.00      0.00         6
       KENT       0.50      0.04      0.07        28
       LEAR       0.56      1.00      0.72        55

avg / total       0.45      0.56      0.41       100

=========================================================================================================

=========================================================================================================
Speaker: GENTLEMAN
Number of Total Listeners: 50

Best model LogisticRegression(C=1.3000000000000003, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
87 features selected out of 174 total
F1 mean: 0.34 (+/- 0.05)

             precision    recall  f1-score   support

     ALBANY       0.88      0.78      0.82         9
      EDGAR       0.67      0.67      0.67         9
       KENT       0.86      0.95      0.90        20
       LEAR       0.73      0.67      0.70        12

avg / total       0.80      0.80      0.80        50

=========================================================================================================

=========================================================================================================
Speaker: EDGAR
Number of Total Listeners: 163

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
192 features selected out of 384 total
F1 mean: 0.08 (+/- 0.04)

             precision    recall  f1-score   support

     ALBANY       0.00      0.00      0.00        19
     EDMUND       0.50      0.05      0.09        20
       FOOL       0.00      0.00      0.00        13
  GENTLEMAN       0.00      0.00      0.00         9
 GLOUCESTER       0.34      1.00      0.51        47
       KENT       0.00      0.00      0.00        21
       LEAR       0.39      0.31      0.35        29
     OSWALD       0.00      0.00      0.00         5

avg / total       0.23      0.35      0.22       163

=========================================================================================================

=========================================================================================================
Speaker: GONERIL
Number of Total Listeners: 95

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
113 features selected out of 227 total
F1 mean: 0.04 (+/- 0.02)

             precision    recall  f1-score   support

     ALBANY       0.45      0.87      0.60        23
   CORNWALL       0.00      0.00      0.00         6
     EDMUND       0.00      0.00      0.00        11
       FOOL       0.00      0.00      0.00         7
       LEAR       0.50      0.15      0.24        13
     OSWALD       0.00      0.00      0.00        11
      REGAN       0.43      0.83      0.56        24

avg / total       0.29      0.44      0.32        95

=========================================================================================================

=========================================================================================================
Speaker: OSWALD
Number of Total Listeners: 38

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
68 features selected out of 136 total
F1 mean: 0.32 (+/- 0.04)

             precision    recall  f1-score   support

      EDGAR       1.00      0.40      0.57         5
    GONERIL       1.00      0.43      0.60         7
       KENT       0.73      1.00      0.84        16
      REGAN       0.73      0.80      0.76        10

avg / total       0.81      0.76      0.74        38

=========================================================================================================

=========================================================================================================
Speaker: REGAN
Number of Total Listeners: 134

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
149 features selected out of 298 total
F1 mean: 0.07 (+/- 0.04)

             precision    recall  f1-score   support

     ALBANY       0.00      0.00      0.00         8
   CORNWALL       0.34      1.00      0.51        34
     EDMUND       1.00      0.07      0.12        15
 GLOUCESTER       0.50      0.04      0.08        23
    GONERIL       0.44      0.54      0.48        26
       LEAR       0.00      0.00      0.00        17
     OSWALD       0.00      0.00      0.00        11

avg / total       0.37      0.37      0.25       134

=========================================================================================================

=========================================================================================================
Speaker: LEAR
Number of Total Listeners: 360

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
281 features selected out of 562 total
F1 mean: 0.03 (+/- 0.01)

             precision    recall  f1-score   support

     ALBANY       0.50      0.07      0.12        14
   BURGUNDY       0.00      0.00      0.00         6
    CAPTAIN       0.00      0.00      0.00         5
   CORDELIA       0.43      0.12      0.19        24
   CORNWALL       0.00      0.00      0.00        13
     DOCTOR       0.00      0.00      0.00         7
      EDGAR       0.50      0.05      0.09        41
       FOOL       0.43      0.40      0.42        65
     FRANCE       0.00      0.00      0.00         5
  GENTLEMAN       0.00      0.00      0.00        10
 GLOUCESTER       0.50      0.09      0.15        33
    GONERIL       0.00      0.00      0.00        24
       KENT       0.26      0.89      0.41        83
     KNIGHT       0.00      0.00      0.00         8
      REGAN       0.00      0.00      0.00        22

avg / total       0.29      0.30      0.21       360

=========================================================================================================

=========================================================================================================
Speaker: KENT
Number of Total Listeners: 212

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
212 features selected out of 424 total
F1 mean: 0.06 (+/- 0.03)

             precision    recall  f1-score   support

     ALBANY       0.00      0.00      0.00        11
   CORDELIA       0.00      0.00      0.00         5
   CORNWALL       0.00      0.00      0.00        16
      EDGAR       0.00      0.00      0.00        18
     EDMUND       0.00      0.00      0.00         5
       FOOL       0.00      0.00      0.00        25
  GENTLEMAN       0.67      0.08      0.14        26
 GLOUCESTER       0.33      0.04      0.08        23
       LEAR       0.32      0.98      0.48        64
     OSWALD       0.50      0.21      0.30        19

avg / total       0.26      0.33      0.20       212

=========================================================================================================

=========================================================================================================
Speaker: GLOUCESTER
Number of Total Listeners: 185

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
199 features selected out of 398 total
F1 mean: 0.11 (+/- 0.06)

             precision    recall  f1-score   support

   CORNWALL       0.60      0.13      0.21        23
      EDGAR       0.36      1.00      0.52        53
     EDMUND       0.68      0.44      0.54        34
       KENT       0.50      0.05      0.08        22
       LEAR       0.67      0.09      0.15        23
    OLD MAN       0.00      0.00      0.00         9
      REGAN       0.50      0.10      0.16        21

avg / total       0.50      0.41      0.32       185

=========================================================================================================

Play: 17

=========================================================================================================
Speaker: KING
Number of Total Listeners: 227

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
447 features selected out of 894 total
F1 mean: 0.05 (+/- 0.03)

             precision    recall  f1-score   support

     ARMADO       0.00      0.00      0.00         6
    BEROWNE       0.38      0.93      0.53        70
      BOYET       0.00      0.00      0.00        12
    COSTARD       0.69      0.34      0.46        32
     DUMAIN       0.00      0.00      0.00        17
 JAQUENETTA       0.00      0.00      0.00         6
 LONGAVILLE       0.50      0.05      0.09        21
PRINCESS OF FRANCE       0.56      0.44      0.49        41
   ROSALINE       0.75      0.14      0.23        22

avg / total       0.43      0.43      0.35       227

=========================================================================================================

=========================================================================================================
Speaker: MOTH
Number of Total Listeners: 102

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
233 features selected out of 466 total
F1 mean: 0.20 (+/- 0.01)

             precision    recall  f1-score   support

     ARMADO       0.68      1.00      0.81        69
      BOYET       0.00      0.00      0.00         6
    COSTARD       0.00      0.00      0.00        16
 HOLOFERNES       0.00      0.00      0.00        11

avg / total       0.46      0.68      0.55       102

=========================================================================================================

=========================================================================================================
Speaker: MARIA
Number of Total Listeners: 33

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
105 features selected out of 210 total
F1 mean: 0.12 (+/- 0.12)

             precision    recall  f1-score   support

      BOYET       0.53      1.00      0.69         9
  KATHARINE       0.57      0.50      0.53         8
PRINCESS OF FRANCE       0.50      0.44      0.47         9
   ROSALINE       1.00      0.14      0.25         7

avg / total       0.63      0.55      0.50        33

=========================================================================================================

=========================================================================================================
Speaker: PRINCESS OF FRANCE
Number of Total Listeners: 225

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
471 features selected out of 942 total
F1 mean: 0.04 (+/- 0.02)

             precision    recall  f1-score   support

     ARMADO       0.00      0.00      0.00         5
    BEROWNE       0.45      0.17      0.25        29
      BOYET       0.37      0.85      0.52        48
    COSTARD       0.00      0.00      0.00        12
     DUMAIN       0.00      0.00      0.00         7
 FIRST LORD       0.00      0.00      0.00         6
   FORESTER       0.00      0.00      0.00         7
  KATHARINE       0.00      0.00      0.00        21
       KING       0.49      0.83      0.62        35
      MARIA       0.50      0.06      0.11        17
   ROSALINE       0.37      0.42      0.40        38

avg / total       0.32      0.41      0.31       225

=========================================================================================================

=========================================================================================================
Speaker: NATHANIEL
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
143 features selected out of 287 total
F1 mean: 0.41 (+/- 0.04)

             precision    recall  f1-score   support

       DULL       0.00      0.00      0.00         6
 HOLOFERNES       0.74      1.00      0.85        17

avg / total       0.55      0.74      0.63        23

=========================================================================================================

=========================================================================================================
Speaker: BOYET
Number of Total Listeners: 162

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
398 features selected out of 796 total
F1 mean: 0.07 (+/- 0.03)

             precision    recall  f1-score   support

    BEROWNE       0.37      0.69      0.48        29
    COSTARD       0.00      0.00      0.00        12
     DUMAIN       0.00      0.00      0.00         9
  KATHARINE       0.00      0.00      0.00        10
       KING       0.00      0.00      0.00        11
 LONGAVILLE       0.00      0.00      0.00        10
      MARIA       0.44      0.21      0.29        19
PRINCESS OF FRANCE       0.48      0.93      0.63        41
   ROSALINE       0.40      0.38      0.39        21

avg / total       0.29      0.43      0.33       162

=========================================================================================================

=========================================================================================================
Speaker: KATHARINE
Number of Total Listeners: 68

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
133 features selected out of 266 total
F1 mean: 0.09 (+/- 0.08)

             precision    recall  f1-score   support

    BEROWNE       1.00      0.57      0.73         7
      BOYET       1.00      0.12      0.22         8
     DUMAIN       0.00      0.00      0.00         5
 LONGAVILLE       0.60      0.38      0.46         8
      MARIA       0.00      0.00      0.00         9
PRINCESS OF FRANCE       0.37      0.73      0.49        15
   ROSALINE       0.39      0.69      0.50        16

avg / total       0.46      0.44      0.38        68

=========================================================================================================

=========================================================================================================
Speaker: ROSALINE
Number of Total Listeners: 129

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
303 features selected out of 606 total
F1 mean: 0.06 (+/- 0.07)

             precision    recall  f1-score   support

    BEROWNE       0.47      0.71      0.56        28
      BOYET       0.44      0.37      0.40        19
     DUMAIN       0.00      0.00      0.00         5
  KATHARINE       1.00      0.11      0.20        18
       KING       0.54      0.58      0.56        24
      MARIA       0.00      0.00      0.00         8
PRINCESS OF FRANCE       0.45      0.70      0.55        27

avg / total       0.50      0.48      0.43       129

=========================================================================================================

=========================================================================================================
Speaker: ARMADO
Number of Total Listeners: 159

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
362 features selected out of 725 total
F1 mean: 0.06 (+/- 0.00)

             precision    recall  f1-score   support

    BEROWNE       0.33      0.11      0.17         9
      BOYET       0.00      0.00      0.00         5
    COSTARD       0.00      0.00      0.00        20
       DULL       0.00      0.00      0.00         6
     DUMAIN       0.50      0.19      0.27        16
 HOLOFERNES       0.75      0.25      0.38        12
 JAQUENETTA       0.00      0.00      0.00         6
       KING       0.33      0.17      0.22         6
       MOTH       0.51      1.00      0.68        72
PRINCESS OF FRANCE       0.33      0.14      0.20         7

avg / total       0.39      0.51      0.39       159

=========================================================================================================

=========================================================================================================
Speaker: BEROWNE
Number of Total Listeners: 319

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
738 features selected out of 1477 total
F1 mean: 0.04 (+/- 0.04)

             precision    recall  f1-score   support

     ARMADO       0.00      0.00      0.00        10
      BOYET       0.50      0.47      0.48        32
    COSTARD       0.63      0.51      0.57        37
     DUMAIN       0.46      0.35      0.40        46
 HOLOFERNES       0.00      0.00      0.00         8
  KATHARINE       0.00      0.00      0.00         8
       KING       0.36      0.97      0.52        76
 LONGAVILLE       0.67      0.09      0.16        43
PRINCESS OF FRANCE       0.57      0.12      0.21        32
   ROSALINE       0.50      0.07      0.13        27

avg / total       0.46      0.42      0.35       319

=========================================================================================================

=========================================================================================================
Speaker: DUMAIN
Number of Total Listeners: 135

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
138 features selected out of 276 total
F1 mean: 0.07 (+/- 0.03)

             precision    recall  f1-score   support

     ARMADO       0.00      0.00      0.00        12
    BEROWNE       0.32      0.98      0.48        42
      BOYET       0.00      0.00      0.00        14
 HOLOFERNES       0.00      0.00      0.00         6
  KATHARINE       0.00      0.00      0.00         5
       KING       0.33      0.06      0.11        16
 LONGAVILLE       0.50      0.04      0.07        28
PRINCESS OF FRANCE       0.00      0.00      0.00         7
   ROSALINE       0.00      0.00      0.00         5

avg / total       0.24      0.32      0.17       135

=========================================================================================================

=========================================================================================================
Speaker: COSTARD
Number of Total Listeners: 142

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
301 features selected out of 602 total
F1 mean: 0.06 (+/- 0.07)

             precision    recall  f1-score   support

     ARMADO       0.50      0.63      0.56        19
    BEROWNE       0.51      0.69      0.59        35
      BOYET       0.44      0.47      0.46        17
     DUMAIN       0.00      0.00      0.00         5
 JAQUENETTA       0.00      0.00      0.00         5
       KING       0.53      0.83      0.65        29
      MARIA       0.50      0.20      0.29         5
       MOTH       0.60      0.20      0.30        15
PRINCESS OF FRANCE       1.00      0.08      0.15        12

avg / total       0.52      0.51      0.46       142

=========================================================================================================

=========================================================================================================
Speaker: HOLOFERNES
Number of Total Listeners: 114

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
328 features selected out of 656 total
F1 mean: 0.05 (+/- 0.01)

             precision    recall  f1-score   support

     ARMADO       0.34      0.76      0.47        17
    BEROWNE       0.40      0.67      0.50         9
      BOYET       0.00      0.00      0.00         7
    COSTARD       0.50      0.10      0.17        10
       DULL       0.67      0.13      0.22        15
     DUMAIN       0.00      0.00      0.00         8
 JAQUENETTA       0.00      0.00      0.00         6
       MOTH       0.50      0.08      0.14        12
  NATHANIEL       0.48      0.87      0.62        30

avg / total       0.39      0.43      0.33       114

=========================================================================================================

=========================================================================================================
Speaker: LONGAVILLE
Number of Total Listeners: 78

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
138 features selected out of 276 total
F1 mean: 0.08 (+/- 0.02)

             precision    recall  f1-score   support

    BEROWNE       0.40      0.96      0.56        26
      BOYET       1.00      0.12      0.22         8
     DUMAIN       0.54      0.39      0.45        18
  KATHARINE       1.00      0.12      0.22         8
       KING       0.00      0.00      0.00        18

avg / total       0.46      0.44      0.34        78

=========================================================================================================

Play: 18

=========================================================================================================
Speaker: THIRD WITCH
Number of Total Listeners: 21

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
49 features selected out of 99 total
F1 mean: 0.35 (+/- 0.40)

             precision    recall  f1-score   support

FIRST WITCH       0.50      0.40      0.44        10
SECOND WITCH       0.54      0.64      0.58        11

avg / total       0.52      0.52      0.52        21

=========================================================================================================

=========================================================================================================
Speaker: LENNOX
Number of Total Listeners: 20

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
67 features selected out of 135 total
F1 mean: 0.38 (+/- 0.11)

             precision    recall  f1-score   support

    MACBETH       0.70      1.00      0.82        14
    MACDUFF       0.00      0.00      0.00         6

avg / total       0.49      0.70      0.58        20

=========================================================================================================

=========================================================================================================
Speaker: MACDUFF
Number of Total Listeners: 76

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
280 features selected out of 560 total
F1 mean: 0.15 (+/- 0.11)

             precision    recall  f1-score   support

     LENNOX       1.00      0.09      0.17        11
    MACBETH       0.62      0.36      0.45        14
    MALCOLM       0.53      0.97      0.68        31
       ROSS       0.90      0.45      0.60        20

avg / total       0.71      0.59      0.54        76

=========================================================================================================

=========================================================================================================
Speaker: DOCTOR
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
82 features selected out of 165 total
F1 mean: 0.40 (+/- 0.05)

             precision    recall  f1-score   support

GENTLEWOMAN       0.65      1.00      0.79        15
LADY MACBETH       0.00      0.00      0.00         8

avg / total       0.43      0.65      0.51        23

=========================================================================================================

=========================================================================================================
Speaker: MALCOLM
Number of Total Listeners: 44

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
317 features selected out of 634 total
F1 mean: 0.22 (+/- 0.06)

             precision    recall  f1-score   support

    MACDUFF       0.70      1.00      0.82        28
       ROSS       0.67      0.22      0.33         9
     SIWARD       1.00      0.14      0.25         7

avg / total       0.74      0.70      0.63        44

=========================================================================================================

=========================================================================================================
Speaker: LADY MACBETH
Number of Total Listeners: 67

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
297 features selected out of 594 total
F1 mean: 0.15 (+/- 0.03)

             precision    recall  f1-score   support

     DOCTOR       0.50      0.33      0.40         6
GENTLEWOMAN       0.50      0.17      0.25         6
     LENNOX       0.00      0.00      0.00         5
    MACBETH       0.74      1.00      0.85        45
       ROSS       0.00      0.00      0.00         5

avg / total       0.59      0.72      0.63        67

=========================================================================================================

=========================================================================================================
Speaker: FIRST WITCH
Number of Total Listeners: 49

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
119 features selected out of 239 total
F1 mean: 0.13 (+/- 0.09)

             precision    recall  f1-score   support

        ALL       0.00      0.00      0.00         6
    MACBETH       0.50      0.22      0.31         9
SECOND WITCH       0.38      0.76      0.51        17
THIRD WITCH       0.45      0.29      0.36        17

avg / total       0.38      0.41      0.36        49

=========================================================================================================

=========================================================================================================
Speaker: ALL
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
44 features selected out of 89 total
F1 mean: 0.18 (+/- 0.20)

             precision    recall  f1-score   support

FIRST WITCH       0.40      0.25      0.31         8
    MACBETH       0.60      0.86      0.71         7
SECOND WITCH       0.00      0.00      0.00         5
THIRD WITCH       0.40      0.80      0.53         5

avg / total       0.38      0.48      0.40        25

=========================================================================================================

=========================================================================================================
Speaker: MACBETH
Number of Total Listeners: 204

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
760 features selected out of 1521 total
F1 mean: 0.06 (+/- 0.04)

             precision    recall  f1-score   support

        ALL       0.67      0.15      0.25        13
     BANQUO       0.66      0.68      0.67        31
     DOCTOR       1.00      0.20      0.33         5
FIRST MURTHERER       0.56      0.42      0.48        12
FIRST WITCH       0.50      0.31      0.38        13
LADY MACBETH       0.45      0.98      0.62        56
     LENNOX       0.60      0.38      0.46        24
      LORDS       0.00      0.00      0.00        10
    MACDUFF       0.86      0.33      0.48        18
       ROSS       0.00      0.00      0.00        10
SECOND MURTHERER       0.50      0.20      0.29         5
     SEYTON       0.67      0.57      0.62         7

avg / total       0.54      0.53      0.47       204

=========================================================================================================

=========================================================================================================
Speaker: FIRST MURTHERER
Number of Total Listeners: 28

Best model LogisticRegression(C=0.59999999999999998, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
55 features selected out of 111 total
F1 mean: 0.18 (+/- 0.37)

             precision    recall  f1-score   support

    MACBETH       0.69      0.90      0.78        10
SECOND MURTHERER       0.50      0.64      0.56        11
THIRD MURTHERER       1.00      0.14      0.25         7

avg / total       0.69      0.61      0.56        28

=========================================================================================================

=========================================================================================================
Speaker: SECOND WITCH
Number of Total Listeners: 24

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
39 features selected out of 79 total
F1 mean: 0.36 (+/- 0.12)

             precision    recall  f1-score   support

FIRST WITCH       0.53      0.75      0.62        12
THIRD WITCH       0.57      0.33      0.42        12

avg / total       0.55      0.54      0.52        24

=========================================================================================================

=========================================================================================================
Speaker: ROSS
Number of Total Listeners: 46

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
201 features selected out of 402 total
F1 mean: 0.11 (+/- 0.12)

             precision    recall  f1-score   support

    MACBETH       1.00      0.60      0.75         5
    MACDUFF       0.52      0.85      0.64        20
    MALCOLM       0.62      0.33      0.43        15
    OLD MAN       1.00      0.33      0.50         6

avg / total       0.67      0.59      0.57        46

=========================================================================================================

Play: 19

=========================================================================================================
Speaker: PORTIA
Number of Total Listeners: 225

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
672 features selected out of 1344 total
F1 mean: 0.07 (+/- 0.07)

             precision    recall  f1-score   support

    ANTONIO       0.32      0.26      0.29        27
    ARRAGON       0.00      0.00      0.00         6
   BASSANIO       0.48      0.67      0.56        43
DUKE OF VENICE       0.00      0.00      0.00        10
   GRATIANO       0.33      0.07      0.11        29
    JESSICA       0.00      0.00      0.00         8
    LORENZO       0.75      0.38      0.50        16
    NERISSA       0.54      0.79      0.64        48
PRINCE OF MOROCCO       1.00      0.43      0.60         7
    SHYLOCK       0.44      0.77      0.56        31

avg / total       0.43      0.48      0.42       225

=========================================================================================================

=========================================================================================================
Speaker: SOLANIO
Number of Total Listeners: 22

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
146 features selected out of 292 total
F1 mean: 0.42 (+/- 0.05)

             precision    recall  f1-score   support

    ANTONIO       0.00      0.00      0.00         5
    SALERIO       0.77      1.00      0.87        17

avg / total       0.60      0.77      0.67        22

=========================================================================================================

=========================================================================================================
Speaker: ANTONIO
Number of Total Listeners: 86

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
311 features selected out of 622 total
F1 mean: 0.05 (+/- 0.09)

             precision    recall  f1-score   support

   BASSANIO       0.44      0.89      0.59        27
DUKE OF VENICE       0.40      0.40      0.40         5
   GRATIANO       1.00      0.10      0.18        10
     PORTIA       0.00      0.00      0.00        13
    SALERIO       0.00      0.00      0.00         5
    SHYLOCK       0.68      0.68      0.68        19
    SOLANIO       0.67      0.57      0.62         7

avg / total       0.48      0.51      0.43        86

=========================================================================================================

=========================================================================================================
Speaker: LAUNCELOT
Number of Total Listeners: 53

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
225 features selected out of 450 total
F1 mean: 0.16 (+/- 0.18)

             precision    recall  f1-score   support

   BASSANIO       1.00      0.14      0.25         7
      GOBBO       0.70      0.95      0.81        20
    JESSICA       0.83      0.56      0.67         9
    LORENZO       0.71      1.00      0.83        12
    SHYLOCK       1.00      0.40      0.57         5

avg / total       0.79      0.74      0.69        53

=========================================================================================================

=========================================================================================================
Speaker: NERISSA
Number of Total Listeners: 50

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
170 features selected out of 341 total
F1 mean: 0.24 (+/- 0.02)

             precision    recall  f1-score   support

   BASSANIO       0.00      0.00      0.00         8
   GRATIANO       0.00      0.00      0.00        11
     PORTIA       0.62      1.00      0.77        31

avg / total       0.38      0.62      0.47        50

=========================================================================================================

=========================================================================================================
Speaker: JESSICA
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
150 features selected out of 300 total
F1 mean: 0.39 (+/- 0.07)

             precision    recall  f1-score   support

  LAUNCELOT       1.00      0.25      0.40         8
    LORENZO       0.74      1.00      0.85        17

avg / total       0.82      0.76      0.71        25

=========================================================================================================

=========================================================================================================
Speaker: GRATIANO
Number of Total Listeners: 97

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
306 features selected out of 612 total
F1 mean: 0.04 (+/- 0.05)

             precision    recall  f1-score   support

    ANTONIO       0.50      0.09      0.15        11
   BASSANIO       0.37      0.67      0.48        24
    LORENZO       0.00      0.00      0.00         9
    NERISSA       0.50      0.07      0.12        15
     PORTIA       0.40      0.78      0.53        23
    SALERIO       1.00      0.40      0.57         5
    SHYLOCK       0.67      0.20      0.31        10

avg / total       0.44      0.41      0.34        97

=========================================================================================================

=========================================================================================================
Speaker: GOBBO
Number of Total Listeners: 24

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
80 features selected out of 160 total
F1 mean: 0.44 (+/- 0.01)

             precision    recall  f1-score   support

   BASSANIO       0.00      0.00      0.00         5
  LAUNCELOT       0.79      1.00      0.88        19

avg / total       0.63      0.79      0.70        24

=========================================================================================================

=========================================================================================================
Speaker: SHYLOCK
Number of Total Listeners: 136

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
489 features selected out of 979 total
F1 mean: 0.05 (+/- 0.06)

             precision    recall  f1-score   support

    ANTONIO       0.62      0.60      0.61        25
   BASSANIO       0.49      0.68      0.57        28
DUKE OF VENICE       0.00      0.00      0.00        10
   GRATIANO       0.50      0.08      0.14        12
    JESSICA       0.67      0.40      0.50         5
  LAUNCELOT       0.67      0.40      0.50         5
     PORTIA       0.44      0.92      0.59        24
    SALERIO       0.67      0.22      0.33         9
    SOLANIO       0.00      0.00      0.00         9
      TUBAL       0.75      1.00      0.86         9

avg / total       0.48      0.53      0.46       136

=========================================================================================================

=========================================================================================================
Speaker: DUKE OF VENICE
Number of Total Listeners: 46

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
120 features selected out of 241 total
F1 mean: 0.03 (+/- 0.05)

             precision    recall  f1-score   support

    ANTONIO       0.50      0.38      0.43         8
   BASSANIO       0.00      0.00      0.00         7
   GRATIANO       0.00      0.00      0.00         6
     PORTIA       0.50      0.38      0.43         8
    SALERIO       0.25      0.17      0.20         6
    SHYLOCK       0.33      0.91      0.49        11

avg / total       0.29      0.37      0.29        46

=========================================================================================================

=========================================================================================================
Speaker: LORENZO
Number of Total Listeners: 75

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
289 features selected out of 579 total
F1 mean: 0.11 (+/- 0.07)

             precision    recall  f1-score   support

   GRATIANO       0.64      0.54      0.58        13
    JESSICA       0.47      0.92      0.62        24
  LAUNCELOT       0.67      0.40      0.50        15
     PORTIA       0.80      0.44      0.57         9
    SALERIO       0.50      0.11      0.18         9
   STEPHANO       1.00      0.20      0.33         5

avg / total       0.62      0.55      0.51        75

=========================================================================================================

=========================================================================================================
Speaker: SALERIO
Number of Total Listeners: 56

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
218 features selected out of 436 total
F1 mean: 0.09 (+/- 0.05)

             precision    recall  f1-score   support

    ANTONIO       0.00      0.00      0.00         6
   BASSANIO       0.50      0.38      0.43         8
   GRATIANO       1.00      0.12      0.22         8
    LORENZO       0.33      0.12      0.18         8
    SHYLOCK       0.00      0.00      0.00         6
    SOLANIO       0.43      1.00      0.61        20

avg / total       0.42      0.45      0.34        56

=========================================================================================================

=========================================================================================================
Speaker: BASSANIO
Number of Total Listeners: 142

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
481 features selected out of 963 total
F1 mean: 0.06 (+/- 0.01)

             precision    recall  f1-score   support

    ANTONIO       0.50      0.28      0.36        25
DUKE OF VENICE       0.00      0.00      0.00         5
      GOBBO       0.50      0.20      0.29         5
   GRATIANO       0.50      0.36      0.42        22
  LAUNCELOT       0.60      0.50      0.55         6
    LORENZO       0.00      0.00      0.00         5
    NERISSA       0.00      0.00      0.00        13
     PORTIA       0.50      0.84      0.63        37
    SALERIO       0.00      0.00      0.00         5
    SHYLOCK       0.42      0.95      0.58        19

avg / total       0.39      0.48      0.40       142

=========================================================================================================

Play: 20

=========================================================================================================
Speaker: SIMPLE
Number of Total Listeners: 29

Best model LogisticRegression(C=1.3000000000000003, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
80 features selected out of 160 total
F1 mean: 0.53 (+/- 0.37)

             precision    recall  f1-score   support

      EVANS       1.00      1.00      1.00         5
   FALSTAFF       0.60      0.75      0.67         8
       HOST       0.67      0.50      0.57         8
    QUICKLY       1.00      1.00      1.00         8

avg / total       0.80      0.79      0.79        29

=========================================================================================================

=========================================================================================================
Speaker: QUICKLY
Number of Total Listeners: 90

Best model LogisticRegression(C=0.40000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
286 features selected out of 572 total
F1 mean: 0.12 (+/- 0.07)

             precision    recall  f1-score   support

      CAIUS       0.60      0.69      0.64        13
      EVANS       0.46      0.86      0.60         7
   FALSTAFF       0.90      1.00      0.95        26
     FENTON       0.90      0.90      0.90        10
   MRS PAGE       0.67      0.57      0.62         7
      RUGBY       0.50      0.38      0.43         8
     SIMPLE       0.73      0.62      0.67        13
    WILLIAM       0.00      0.00      0.00         6

avg / total       0.68      0.72      0.69        90

=========================================================================================================

=========================================================================================================
Speaker: FALSTAFF
Number of Total Listeners: 204

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
564 features selected out of 1129 total
F1 mean: 0.08 (+/- 0.05)

             precision    recall  f1-score   support

   BARDOLPH       0.75      0.25      0.38        12
      EVANS       0.50      0.31      0.38        16
       FORD       0.56      0.94      0.70        35
       HOST       0.62      0.62      0.62        16
   MRS FORD       0.61      0.77      0.68        22
   MRS PAGE       0.50      0.06      0.11        17
        NYM       0.50      0.11      0.18         9
       PAGE       0.00      0.00      0.00         8
     PISTOL       0.54      0.62      0.58        21
    QUICKLY       0.47      0.96      0.63        27
    SHALLOW       0.00      0.00      0.00         5
     SIMPLE       0.50      0.11      0.18         9
    SLENDER       0.50      0.14      0.22         7

avg / total       0.52      0.54      0.47       204

=========================================================================================================

=========================================================================================================
Speaker: NYM
Number of Total Listeners: 20

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
46 features selected out of 93 total
F1 mean: 0.35 (+/- 0.11)

             precision    recall  f1-score   support

   FALSTAFF       0.00      0.00      0.00         8
     PISTOL       0.60      1.00      0.75        12

avg / total       0.36      0.60      0.45        20

=========================================================================================================

=========================================================================================================
Speaker: CAIUS
Number of Total Listeners: 87

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
147 features selected out of 295 total
F1 mean: 0.07 (+/- 0.06)

             precision    recall  f1-score   support

      EVANS       0.41      0.64      0.50        11
       FORD       0.50      0.29      0.36         7
       HOST       0.43      1.00      0.60        20
       PAGE       0.00      0.00      0.00         8
    QUICKLY       0.59      0.77      0.67        13
      RUGBY       1.00      0.22      0.36         9
    SHALLOW       0.00      0.00      0.00         8
     SIMPLE       0.00      0.00      0.00         5
    SLENDER       0.00      0.00      0.00         6

avg / total       0.38      0.47      0.37        87

=========================================================================================================

=========================================================================================================
Speaker: EVANS
Number of Total Listeners: 173

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
319 features selected out of 639 total
F1 mean: 0.10 (+/- 0.08)

             precision    recall  f1-score   support

      CAIUS       0.57      0.36      0.44        11
   FALSTAFF       1.00      0.14      0.25        14
       FORD       0.56      0.31      0.40        16
       HOST       1.00      0.20      0.33         5
   MRS PAGE       0.35      0.35      0.35        17
       PAGE       0.39      0.71      0.51        21
    QUICKLY       0.33      0.06      0.11        16
    SHALLOW       0.47      0.68      0.56        25
     SIMPLE       1.00      0.67      0.80         6
    SLENDER       0.48      0.43      0.45        23
    WILLIAM       0.43      0.79      0.56        19

avg / total       0.52      0.46      0.43       173

=========================================================================================================

=========================================================================================================
Speaker: SHALLOW
Number of Total Listeners: 127

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
186 features selected out of 373 total
F1 mean: 0.07 (+/- 0.05)

             precision    recall  f1-score   support

       ANNE       0.00      0.00      0.00         9
      CAIUS       0.00      0.00      0.00         7
      EVANS       0.60      0.32      0.42        28
       FORD       0.00      0.00      0.00         9
       HOST       0.50      0.08      0.13        13
       PAGE       0.39      0.48      0.43        25
    SLENDER       0.42      0.92      0.57        36

avg / total       0.38      0.43      0.35       127

=========================================================================================================

=========================================================================================================
Speaker: MRS FORD
Number of Total Listeners: 124

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
252 features selected out of 504 total
F1 mean: 0.15 (+/- 0.05)

             precision    recall  f1-score   support

   FALSTAFF       0.60      0.12      0.21        24
       FORD       0.50      0.06      0.11        16
   MRS PAGE       0.58      0.99      0.73        69
       PAGE       0.00      0.00      0.00        10
      ROBIN       0.00      0.00      0.00         5

avg / total       0.50      0.58      0.46       124

=========================================================================================================

=========================================================================================================
Speaker: FENTON
Number of Total Listeners: 22

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
115 features selected out of 230 total
F1 mean: 0.34 (+/- 0.39)

             precision    recall  f1-score   support

       ANNE       0.60      0.60      0.60         5
   MRS PAGE       1.00      0.17      0.29         6
    QUICKLY       0.62      0.91      0.74        11

avg / total       0.72      0.64      0.58        22

=========================================================================================================

=========================================================================================================
Speaker: ANNE
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
59 features selected out of 118 total
F1 mean: 0.23 (+/- 0.04)

             precision    recall  f1-score   support

     FENTON       1.00      0.40      0.57         5
    SHALLOW       0.00      0.00      0.00         6
    SLENDER       0.61      1.00      0.76        14

avg / total       0.54      0.64      0.54        25

=========================================================================================================

=========================================================================================================
Speaker: PISTOL
Number of Total Listeners: 33

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
103 features selected out of 206 total
F1 mean: 0.37 (+/- 0.13)

             precision    recall  f1-score   support

   FALSTAFF       0.62      0.89      0.73        18
        NYM       0.71      0.33      0.45        15

avg / total       0.66      0.64      0.60        33

=========================================================================================================

=========================================================================================================
Speaker: SLENDER
Number of Total Listeners: 116

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
216 features selected out of 432 total
F1 mean: 0.09 (+/- 0.08)

             precision    recall  f1-score   support

       ANNE       0.64      0.50      0.56        18
   BARDOLPH       0.00      0.00      0.00         5
      EVANS       0.36      0.43      0.39        23
   FALSTAFF       0.25      0.12      0.17         8
        NYM       0.00      0.00      0.00         5
       PAGE       0.65      0.50      0.56        22
     PISTOL       0.00      0.00      0.00         6
    SHALLOW       0.43      0.79      0.56        29

avg / total       0.42      0.47      0.42       116

=========================================================================================================

=========================================================================================================
Speaker: FORD
Number of Total Listeners: 165

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
349 features selected out of 699 total
F1 mean: 0.09 (+/- 0.09)

             precision    recall  f1-score   support

      CAIUS       0.50      0.25      0.33         8
      EVANS       0.43      0.12      0.18        26
   FALSTAFF       0.58      0.95      0.72        39
   MRS FORD       0.43      0.27      0.33        22
   MRS PAGE       0.50      0.29      0.36        28
       PAGE       0.41      0.69      0.51        35
    SHALLOW       1.00      0.14      0.25         7

avg / total       0.50      0.49      0.44       165

=========================================================================================================

=========================================================================================================
Speaker: HOST
Number of Total Listeners: 74

Best model LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=200, multi_class='ovr',
          penalty='l1', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=1)
187 features selected out of 375 total
F1 mean: 0.10 (+/- 0.10)

             precision    recall  f1-score   support

   BARDOLPH       1.00      0.67      0.80         6
      CAIUS       0.52      0.79      0.62        19
   FALSTAFF       0.73      0.80      0.76        10
       FORD       0.50      0.38      0.43         8
       PAGE       0.33      0.09      0.14        11
    SHALLOW       0.38      0.40      0.39        15
     SIMPLE       0.80      0.80      0.80         5

avg / total       0.55      0.55      0.53        74

=========================================================================================================

=========================================================================================================
Speaker: MRS PAGE
Number of Total Listeners: 157

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
389 features selected out of 778 total
F1 mean: 0.08 (+/- 0.02)

             precision    recall  f1-score   support

      EVANS       0.00      0.00      0.00        12
   FALSTAFF       0.50      0.07      0.12        15
       FORD       0.38      0.25      0.30        24
   MRS FORD       0.51      0.99      0.67        70
       PAGE       1.00      0.06      0.11        17
    QUICKLY       1.00      0.20      0.33        10
      ROBIN       1.00      0.11      0.20         9

avg / total       0.56      0.51      0.40       157

=========================================================================================================

=========================================================================================================
Speaker: PAGE
Number of Total Listeners: 163

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
233 features selected out of 466 total
F1 mean: 0.05 (+/- 0.07)

             precision    recall  f1-score   support

      CAIUS       0.00      0.00      0.00         8
      EVANS       0.35      0.41      0.38        27
   FALSTAFF       0.00      0.00      0.00         6
       FORD       0.45      0.84      0.59        32
       HOST       0.00      0.00      0.00         9
   MRS FORD       0.00      0.00      0.00        10
   MRS PAGE       0.56      0.25      0.34        20
    SHALLOW       0.40      0.63      0.49        27
    SLENDER       0.60      0.50      0.55        24

avg / total       0.37      0.44      0.38       163

=========================================================================================================

Play: 21

=========================================================================================================
Speaker: PUCK
Number of Total Listeners: 36

Best model LogisticRegression(C=0.40000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
219 features selected out of 438 total
F1 mean: 0.26 (+/- 0.14)

             precision    recall  f1-score   support

  DEMETRIUS       0.62      0.89      0.73         9
   LYSANDER       1.00      0.29      0.44         7
     OBERON       0.67      0.93      0.78        15
    TITANIA       0.00      0.00      0.00         5

avg / total       0.63      0.67      0.59        36

=========================================================================================================

=========================================================================================================
Speaker: HELENA
Number of Total Listeners: 65

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
320 features selected out of 641 total
F1 mean: 0.12 (+/- 0.20)

             precision    recall  f1-score   support

  DEMETRIUS       0.56      0.47      0.51        19
     HERMIA       0.57      0.57      0.57        21
   LYSANDER       0.52      0.65      0.58        20
     OBERON       0.67      0.40      0.50         5

avg / total       0.56      0.55      0.55        65

=========================================================================================================

=========================================================================================================
Speaker: BOTTOM
Number of Total Listeners: 92

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
269 features selected out of 539 total
F1 mean: 0.07 (+/- 0.04)

             precision    recall  f1-score   support

     COBWEB       0.00      0.00      0.00         7
      FLUTE       0.00      0.00      0.00         5
MUSTARDSEED       0.33      0.22      0.27         9
PEASEBLOSSOM       0.00      0.00      0.00         7
     QUINCE       0.48      1.00      0.65        32
      SNOUT       0.00      0.00      0.00        12
 STARVELING       0.00      0.00      0.00         6
    TITANIA       0.53      0.71      0.61        14

avg / total       0.28      0.48      0.34        92

=========================================================================================================

=========================================================================================================
Speaker: HERMIA
Number of Total Listeners: 77

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
279 features selected out of 558 total
F1 mean: 0.10 (+/- 0.08)

             precision    recall  f1-score   support

  DEMETRIUS       0.75      0.35      0.48        17
     HELENA       0.62      0.33      0.43        24
   LYSANDER       0.51      0.90      0.65        31
    THESEUS       1.00      0.20      0.33         5

avg / total       0.63      0.56      0.52        77

=========================================================================================================

=========================================================================================================
Speaker: QUINCE
Number of Total Listeners: 61

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
153 features selected out of 306 total
F1 mean: 0.16 (+/- 0.04)

             precision    recall  f1-score   support

     BOTTOM       0.56      1.00      0.72        32
      FLUTE       0.75      0.27      0.40        11
      SNOUT       0.00      0.00      0.00        11
 STARVELING       0.00      0.00      0.00         7

avg / total       0.43      0.57      0.45        61

=========================================================================================================

=========================================================================================================
Speaker: THESEUS
Number of Total Listeners: 95

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
380 features selected out of 761 total
F1 mean: 0.05 (+/- 0.02)

             precision    recall  f1-score   support

  DEMETRIUS       0.52      0.52      0.52        23
      EGEUS       0.56      0.50      0.53        10
     HERMIA       0.50      0.17      0.25         6
  HIPPOLYTA       0.43      0.86      0.57        28
   LYSANDER       0.50      0.12      0.19        17
PHILOSTRATE       1.00      0.09      0.17        11

avg / total       0.55      0.47      0.42        95

=========================================================================================================

=========================================================================================================
Speaker: TITANIA
Number of Total Listeners: 28

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
233 features selected out of 467 total
F1 mean: 0.32 (+/- 0.44)

             precision    recall  f1-score   support

     BOTTOM       0.89      0.80      0.84        10
     OBERON       0.63      0.92      0.75        13
       PUCK       0.00      0.00      0.00         5

avg / total       0.61      0.71      0.65        28

=========================================================================================================

=========================================================================================================
Speaker: LYSANDER
Number of Total Listeners: 96

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
285 features selected out of 571 total
F1 mean: 0.07 (+/- 0.07)

             precision    recall  f1-score   support

  DEMETRIUS       0.41      0.72      0.52        25
      EGEUS       0.40      0.33      0.36         6
     HELENA       0.62      0.26      0.37        19
     HERMIA       0.60      0.70      0.65        30
  HIPPOLYTA       0.00      0.00      0.00         5
    THESEUS       0.50      0.18      0.27        11

avg / total       0.50      0.50      0.46        96

=========================================================================================================

=========================================================================================================
Speaker: DEMETRIUS
Number of Total Listeners: 95

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
242 features selected out of 485 total
F1 mean: 0.05 (+/- 0.11)

             precision    recall  f1-score   support

     HELENA       0.73      0.47      0.57        17
     HERMIA       0.50      0.47      0.48        15
  HIPPOLYTA       0.00      0.00      0.00         7
   LYSANDER       0.39      0.74      0.51        23
     OBERON       0.00      0.00      0.00         5
       PUCK       0.67      0.57      0.62         7
    THESEUS       0.55      0.69      0.61        16
     THISBY       0.00      0.00      0.00         5

avg / total       0.44      0.49      0.45        95

=========================================================================================================

=========================================================================================================
Speaker: HIPPOLYTA
Number of Total Listeners: 21

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
89 features selected out of 179 total
F1 mean: 0.38 (+/- 0.06)

             precision    recall  f1-score   support

  DEMETRIUS       0.00      0.00      0.00         7
    THESEUS       0.67      1.00      0.80        14

avg / total       0.44      0.67      0.53        21

=========================================================================================================

=========================================================================================================
Speaker: OBERON
Number of Total Listeners: 34

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
331 features selected out of 663 total
F1 mean: 0.32 (+/- 0.23)

             precision    recall  f1-score   support

       PUCK       0.70      0.78      0.74        18
    TITANIA       0.71      0.62      0.67        16

avg / total       0.71      0.71      0.70        34

=========================================================================================================

Play: 22

=========================================================================================================
Speaker: BEATRICE
Number of Total Listeners: 177

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
415 features selected out of 830 total
F1 mean: 0.07 (+/- 0.02)

             precision    recall  f1-score   support

    ANTONIO       0.50      0.10      0.17        10
   BENEDICK       0.53      0.97      0.68        61
    CLAUDIO       0.40      0.17      0.24        12
      FRIAR       0.00      0.00      0.00         5
       HERO       0.00      0.00      0.00        15
    LEONATO       0.45      0.57      0.51        35
   MARGARET       0.50      0.08      0.14        12
  MESSENGER       1.00      0.09      0.17        11
      PEDRO       0.55      0.38      0.44        16

avg / total       0.47      0.51      0.42       177

=========================================================================================================

=========================================================================================================
Speaker: VERGES
Number of Total Listeners: 30

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
63 features selected out of 126 total
F1 mean: 0.24 (+/- 0.03)

             precision    recall  f1-score   support

   DOGBERRY       0.60      1.00      0.75        18
    LEONATO       0.00      0.00      0.00         6
SECOND WATCH       0.00      0.00      0.00         6

avg / total       0.36      0.60      0.45        30

=========================================================================================================

=========================================================================================================
Speaker: ANTONIO
Number of Total Listeners: 30

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
108 features selected out of 216 total
F1 mean: 0.25 (+/- 0.03)

             precision    recall  f1-score   support

    CLAUDIO       0.00      0.00      0.00         5
    LEONATO       0.63      1.00      0.78        19
      PEDRO       0.00      0.00      0.00         6

avg / total       0.40      0.63      0.49        30

=========================================================================================================

=========================================================================================================
Speaker: MARGARET
Number of Total Listeners: 30

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
140 features selected out of 280 total
F1 mean: 0.18 (+/- 0.13)

             precision    recall  f1-score   support

   BEATRICE       1.00      0.20      0.33        10
   BENEDICK       1.00      0.80      0.89         5
       HERO       0.62      1.00      0.77        15

avg / total       0.81      0.70      0.64        30

=========================================================================================================

=========================================================================================================
Speaker: LEONATO
Number of Total Listeners: 263

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
464 features selected out of 929 total
F1 mean: 0.04 (+/- 0.06)

             precision    recall  f1-score   support

    ANTONIO       0.65      0.41      0.50        27
   BEATRICE       0.47      0.54      0.50        26
   BENEDICK       0.42      0.19      0.26        27
   BORACHIO       0.00      0.00      0.00         6
    CLAUDIO       0.31      0.62      0.41        52
   DOGBERRY       0.62      0.47      0.53        17
      FRIAR       0.40      0.12      0.18        17
       HERO       0.00      0.00      0.00        14
       JOHN       0.00      0.00      0.00         5
  MESSENGER       1.00      0.25      0.40        12
      PEDRO       0.39      0.61      0.47        51
     VERGES       0.00      0.00      0.00         9

avg / total       0.40      0.40      0.36       263

=========================================================================================================

=========================================================================================================
Speaker: MESSENGER
Number of Total Listeners: 26

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
71 features selected out of 142 total
F1 mean: 0.27 (+/- 0.28)

             precision    recall  f1-score   support

   BEATRICE       1.00      0.18      0.31        11
    LEONATO       0.62      1.00      0.77        15

avg / total       0.78      0.65      0.57        26

=========================================================================================================

=========================================================================================================
Speaker: DOGBERRY
Number of Total Listeners: 102

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
263 features selected out of 526 total
F1 mean: 0.05 (+/- 0.04)

             precision    recall  f1-score   support

   BORACHIO       0.42      0.45      0.43        11
    CONRADE       1.00      0.14      0.25         7
FIRST WATCH       0.00      0.00      0.00         7
    LEONATO       0.62      0.29      0.40        17
SECOND WATCH       0.50      0.12      0.20        16
     SEXTON       0.40      0.36      0.38        11
     VERGES       0.46      0.94      0.62        33

avg / total       0.49      0.47      0.40       102

=========================================================================================================

=========================================================================================================
Speaker: CLAUDIO
Number of Total Listeners: 255

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
385 features selected out of 770 total
F1 mean: 0.07 (+/- 0.01)

             precision    recall  f1-score   support

    ANTONIO       0.00      0.00      0.00         8
   BEATRICE       0.00      0.00      0.00         7
   BENEDICK       0.60      0.11      0.18        55
   BORACHIO       0.50      0.12      0.20         8
       HERO       0.00      0.00      0.00        13
       JOHN       0.00      0.00      0.00        11
    LEONATO       0.44      0.20      0.28        55
      PEDRO       0.44      0.98      0.61        98

avg / total       0.41      0.45      0.34       255

=========================================================================================================

=========================================================================================================
Speaker: SECOND WATCH
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
53 features selected out of 107 total
F1 mean: 0.13 (+/- 0.16)

             precision    recall  f1-score   support

   BORACHIO       0.50      0.33      0.40         6
    CONRADE       0.50      0.50      0.50         6
   DOGBERRY       0.53      1.00      0.69         9
     VERGES       0.00      0.00      0.00         6

avg / total       0.40      0.52      0.43        27

=========================================================================================================

=========================================================================================================
Speaker: JOHN
Number of Total Listeners: 58

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
196 features selected out of 393 total
F1 mean: 0.19 (+/- 0.22)

             precision    recall  f1-score   support

   BORACHIO       0.62      0.95      0.75        19
    CLAUDIO       0.50      0.33      0.40        15
    CONRADE       1.00      0.22      0.36         9
      PEDRO       0.59      0.67      0.62        15

avg / total       0.64      0.60      0.57        58

=========================================================================================================

=========================================================================================================
Speaker: PEDRO
Number of Total Listeners: 275

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
408 features selected out of 816 total
F1 mean: 0.05 (+/- 0.02)

             precision    recall  f1-score   support

    ANTONIO       0.00      0.00      0.00         6
  BALTHASAR       0.50      0.12      0.20         8
   BEATRICE       0.00      0.00      0.00        14
   BENEDICK       0.50      0.16      0.24        57
   BORACHIO       0.00      0.00      0.00         6
    CLAUDIO       0.41      0.98      0.58       102
   DOGBERRY       0.00      0.00      0.00         6
       HERO       0.00      0.00      0.00        10
       JOHN       0.00      0.00      0.00        14
    LEONATO       0.42      0.10      0.16        52

avg / total       0.35      0.42      0.30       275

=========================================================================================================

=========================================================================================================
Speaker: HERO
Number of Total Listeners: 74

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
228 features selected out of 457 total
F1 mean: 0.19 (+/- 0.15)

             precision    recall  f1-score   support

   BEATRICE       0.00      0.00      0.00         9
    CLAUDIO       0.42      0.42      0.42        12
    LEONATO       0.45      0.71      0.56        14
   MARGARET       0.62      0.83      0.71        12
      PEDRO       0.80      0.36      0.50        11
     URSULA       0.74      0.88      0.80        16

avg / total       0.53      0.58      0.54        74

=========================================================================================================

=========================================================================================================
Speaker: BORACHIO
Number of Total Listeners: 52

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
217 features selected out of 434 total
F1 mean: 0.23 (+/- 0.11)

             precision    recall  f1-score   support

    CLAUDIO       1.00      0.40      0.57         5
    CONRADE       0.52      0.83      0.64        18
       JOHN       0.76      1.00      0.86        16
SECOND WATCH       0.00      0.00      0.00        13

avg / total       0.51      0.63      0.54        52

=========================================================================================================

=========================================================================================================
Speaker: FRIAR
Number of Total Listeners: 36

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
168 features selected out of 337 total
F1 mean: 0.10 (+/- 0.12)

             precision    recall  f1-score   support

   BEATRICE       0.00      0.00      0.00         5
   BENEDICK       0.50      0.11      0.18         9
       HERO       0.00      0.00      0.00         6
    LEONATO       0.44      0.94      0.60        16

avg / total       0.32      0.44      0.31        36

=========================================================================================================

=========================================================================================================
Speaker: CONRADE
Number of Total Listeners: 29

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
74 features selected out of 148 total
F1 mean: 0.21 (+/- 0.04)

             precision    recall  f1-score   support

   BORACHIO       0.54      1.00      0.70        14
       JOHN       1.00      0.50      0.67         6
SECOND WATCH       0.00      0.00      0.00         9

avg / total       0.47      0.59      0.48        29

=========================================================================================================

=========================================================================================================
Speaker: BENEDICK
Number of Total Listeners: 195

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
502 features selected out of 1004 total
F1 mean: 0.17 (+/- 0.08)

             precision    recall  f1-score   support

   BEATRICE       0.55      0.95      0.70        61
    CLAUDIO       0.56      0.40      0.47        47
      FRIAR       0.50      0.09      0.15        11
    LEONATO       0.50      0.17      0.26        23
   MARGARET       1.00      0.29      0.44         7
      PEDRO       0.57      0.54      0.56        46

avg / total       0.56      0.56      0.52       195

=========================================================================================================

Play: 23

=========================================================================================================
Speaker: RODERIGO
Number of Total Listeners: 74

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
198 features selected out of 397 total
F1 mean: 0.20 (+/- 0.02)

             precision    recall  f1-score   support

  BRABANTIO       0.00      0.00      0.00        12
     CASSIO       0.00      0.00      0.00         7
       IAGO       0.68      1.00      0.81        50
    OTHELLO       0.00      0.00      0.00         5

avg / total       0.46      0.68      0.54        74

=========================================================================================================

=========================================================================================================
Speaker: DUKE
Number of Total Listeners: 44

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
152 features selected out of 304 total
F1 mean: 0.15 (+/- 0.04)

             precision    recall  f1-score   support

  BRABANTIO       0.44      0.64      0.52        11
  DESDEMONA       0.00      0.00      0.00         6
FIRST SENATOR       0.58      0.79      0.67        14
    OTHELLO       0.44      0.31      0.36        13

avg / total       0.42      0.50      0.45        44

=========================================================================================================

=========================================================================================================
Speaker: CASSIO
Number of Total Listeners: 194

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
388 features selected out of 777 total
F1 mean: 0.05 (+/- 0.01)

             precision    recall  f1-score   support

     BIANCA       0.80      0.44      0.57         9
  DESDEMONA       0.50      0.08      0.13        13
     EMILIA       0.00      0.00      0.00         7
   GRATIANO       0.00      0.00      0.00         7
       IAGO       0.44      0.99      0.61        76
   LODOVICO       0.00      0.00      0.00         9
    MONTANO       0.56      0.24      0.33        21
    OTHELLO       0.50      0.12      0.20        32
   RODERIGO       0.00      0.00      0.00        12
SECOND GENTLEMAN       0.00      0.00      0.00         8

avg / total       0.39      0.46      0.34       194

=========================================================================================================

=========================================================================================================
Speaker: BRABANTIO
Number of Total Listeners: 49

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
251 features selected out of 503 total
F1 mean: 0.15 (+/- 0.13)

             precision    recall  f1-score   support

       DUKE       0.75      0.55      0.63        11
       IAGO       0.00      0.00      0.00        11
    OTHELLO       0.67      0.40      0.50        10
   RODERIGO       0.49      1.00      0.65        17

avg / total       0.47      0.55      0.47        49

=========================================================================================================

=========================================================================================================
Speaker: GRATIANO
Number of Total Listeners: 37

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
60 features selected out of 121 total
F1 mean: 0.13 (+/- 0.10)

             precision    recall  f1-score   support

     CASSIO       0.00      0.00      0.00         6
     EMILIA       0.00      0.00      0.00         6
       IAGO       0.37      1.00      0.54        11
   LODOVICO       0.00      0.00      0.00         6
    OTHELLO       0.57      0.50      0.53         8

avg / total       0.23      0.41      0.27        37

=========================================================================================================

=========================================================================================================
Speaker: DESDEMONA
Number of Total Listeners: 231

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
414 features selected out of 829 total
F1 mean: 0.14 (+/- 0.05)

             precision    recall  f1-score   support

     CASSIO       0.50      0.06      0.11        17
      CLOWN       0.00      0.00      0.00         6
     EMILIA       0.68      0.55      0.61        62
       IAGO       1.00      0.06      0.11        33
   LODOVICO       0.00      0.00      0.00        14
    OTHELLO       0.55      0.99      0.71        99

avg / total       0.60      0.58      0.49       231

=========================================================================================================

=========================================================================================================
Speaker: IAGO
Number of Total Listeners: 403

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
1039 features selected out of 2078 total
F1 mean: 0.08 (+/- 0.02)

             precision    recall  f1-score   support

     BIANCA       0.00      0.00      0.00         9
  BRABANTIO       0.00      0.00      0.00         6
     CASSIO       0.49      0.62      0.55        72
  DESDEMONA       0.60      0.21      0.32        42
     EMILIA       0.58      0.17      0.26        42
   GRATIANO       0.67      0.10      0.17        20
   LODOVICO       1.00      0.11      0.20        18
    MONTANO       1.00      0.08      0.14        13
    OTHELLO       0.50      0.95      0.66       126
   RODERIGO       0.82      0.58      0.68        55

avg / total       0.59      0.54      0.48       403

=========================================================================================================

=========================================================================================================
Speaker: LODOVICO
Number of Total Listeners: 66

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
153 features selected out of 307 total
F1 mean: 0.08 (+/- 0.04)

             precision    recall  f1-score   support

     CASSIO       0.00      0.00      0.00        10
  DESDEMONA       0.00      0.00      0.00         9
   GRATIANO       0.50      0.09      0.15        11
       IAGO       0.44      0.88      0.58        16
    OTHELLO       0.47      0.75      0.58        20

avg / total       0.33      0.45      0.34        66

=========================================================================================================

=========================================================================================================
Speaker: OTHELLO
Number of Total Listeners: 391

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
901 features selected out of 1802 total
F1 mean: 0.07 (+/- 0.03)

             precision    recall  f1-score   support

  BRABANTIO       0.00      0.00      0.00         9
     CASSIO       0.57      0.11      0.18        37
  DESDEMONA       0.57      0.83      0.68       112
       DUKE       0.64      0.78      0.70         9
     EMILIA       0.67      0.24      0.35        42
   GRATIANO       1.00      0.08      0.15        12
       IAGO       0.59      0.85      0.70       130
   LODOVICO       0.33      0.08      0.12        26
    MONTANO       0.00      0.00      0.00         8
   RODERIGO       0.00      0.00      0.00         6

avg / total       0.55      0.58      0.51       391

=========================================================================================================

=========================================================================================================
Speaker: EMILIA
Number of Total Listeners: 157

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
337 features selected out of 674 total
F1 mean: 0.12 (+/- 0.06)

             precision    recall  f1-score   support

     CASSIO       0.00      0.00      0.00         6
  DESDEMONA       0.62      0.88      0.73        50
   GRATIANO       0.00      0.00      0.00        13
       IAGO       0.58      0.41      0.48        37
    MONTANO       0.00      0.00      0.00         5
    OTHELLO       0.57      0.74      0.64        46

avg / total       0.50      0.59      0.53       157

=========================================================================================================

=========================================================================================================
Speaker: MONTANO
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
91 features selected out of 183 total
F1 mean: 0.20 (+/- 0.23)

             precision    recall  f1-score   support

     CASSIO       1.00      0.44      0.62         9
       IAGO       0.58      0.92      0.71        12
    OTHELLO       0.50      0.33      0.40         6

avg / total       0.70      0.63      0.61        27

=========================================================================================================

Play: 24

=========================================================================================================
Speaker: BOLINGBROKE
Number of Total Listeners: 138

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
521 features selected out of 1043 total
F1 mean: 0.06 (+/- 0.05)

             precision    recall  f1-score   support

    AUMERLE       0.80      0.33      0.47        12
   CARLISLE       1.00      0.20      0.33         5
DUCHESS OF YORK       1.00      0.12      0.22         8
      GAUNT       0.80      0.36      0.50        11
KING RICHARD       0.42      0.97      0.58        33
    MARSHAL       0.00      0.00      0.00         6
    MOWBRAY       0.50      0.10      0.17        10
NORTHUMBERLAND       0.61      0.50      0.55        22
      PERCY       0.71      0.50      0.59        10
       YORK       0.59      0.62      0.60        21

avg / total       0.60      0.52      0.48       138

=========================================================================================================

=========================================================================================================
Speaker: AUMERLE
Number of Total Listeners: 51

Best model LogisticRegression(C=1.8000000000000003, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
178 features selected out of 356 total
F1 mean: 0.21 (+/- 0.22)

             precision    recall  f1-score   support

BOLINGBROKE       0.75      0.82      0.78        11
   CARLISLE       1.00      0.20      0.33         5
DUCHESS OF YORK       0.00      0.00      0.00         6
  FITZWATER       1.00      0.60      0.75         5
KING RICHARD       0.78      1.00      0.88        14
       YORK       0.53      0.90      0.67        10

avg / total       0.68      0.71      0.65        51

=========================================================================================================

=========================================================================================================
Speaker: DUCHESS OF YORK
Number of Total Listeners: 46

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
178 features selected out of 356 total
F1 mean: 0.16 (+/- 0.14)

             precision    recall  f1-score   support

    AUMERLE       0.00      0.00      0.00        10
BOLINGBROKE       1.00      0.08      0.15        12
       YORK       0.53      1.00      0.70        24

avg / total       0.54      0.54      0.40        46

=========================================================================================================

=========================================================================================================
Speaker: MOWBRAY
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
269 features selected out of 539 total
F1 mean: 0.40 (+/- 0.11)

             precision    recall  f1-score   support

BOLINGBROKE       0.50      0.30      0.37        10
KING RICHARD       0.59      0.77      0.67        13

avg / total       0.55      0.57      0.54        23

=========================================================================================================

=========================================================================================================
Speaker: NORTHUMBERLAND
Number of Total Listeners: 71

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
278 features selected out of 556 total
F1 mean: 0.07 (+/- 0.07)

             precision    recall  f1-score   support

BOLINGBROKE       0.44      0.95      0.60        20
KING RICHARD       0.73      0.57      0.64        14
      PERCY       1.00      0.12      0.22         8
       ROSS       0.50      0.20      0.29        10
 WILLOUGHBY       0.50      0.45      0.48        11
       YORK       0.50      0.12      0.20         8

avg / total       0.58      0.51      0.46        71

=========================================================================================================

=========================================================================================================
Speaker: GAUNT
Number of Total Listeners: 25

Best model LogisticRegression(C=0.40000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
251 features selected out of 502 total
F1 mean: 0.55 (+/- 0.47)

             precision    recall  f1-score   support

BOLINGBROKE       1.00      0.60      0.75        10
KING RICHARD       0.79      1.00      0.88        15

avg / total       0.87      0.84      0.83        25

=========================================================================================================

=========================================================================================================
Speaker: YORK
Number of Total Listeners: 80

Best model LogisticRegression(C=1.2000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
389 features selected out of 778 total
F1 mean: 0.17 (+/- 0.13)

             precision    recall  f1-score   support

    AUMERLE       0.57      0.50      0.53        16
BOLINGBROKE       0.54      0.61      0.57        23
DUCHESS OF YORK       0.64      0.86      0.73        21
KING RICHARD       0.75      0.43      0.55         7
NORTHUMBERLAND       0.50      0.31      0.38        13

avg / total       0.58      0.59      0.57        80

=========================================================================================================

=========================================================================================================
Speaker: QUEEN
Number of Total Listeners: 26

Best model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=200, multi_class='multinomial',
          penalty='l1', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=1)
153 features selected out of 307 total
F1 mean: 0.37 (+/- 0.29)

             precision    recall  f1-score   support

      BUSHY       0.67      0.75      0.71         8
      GREEN       0.50      0.40      0.44         5
KING RICHARD       1.00      1.00      1.00         6
       LADY       1.00      1.00      1.00         7

avg / total       0.80      0.81      0.80        26

=========================================================================================================

=========================================================================================================
Speaker: KING RICHARD
Number of Total Listeners: 165

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
837 features selected out of 1675 total
F1 mean: 0.02 (+/- 0.02)

             precision    recall  f1-score   support

    AUMERLE       0.52      0.54      0.53        24
BOLINGBROKE       0.48      0.82      0.61        39
   CARLISLE       0.00      0.00      0.00         5
      GAUNT       0.46      0.73      0.56        22
    MARSHAL       0.50      0.14      0.22         7
    MOWBRAY       0.40      0.12      0.19        16
NORTHUMBERLAND       0.44      0.32      0.37        22
      QUEEN       0.75      0.60      0.67        10
     SCROOP       1.00      0.14      0.25         7
       YORK       0.43      0.23      0.30        13

avg / total       0.49      0.49      0.45       165

=========================================================================================================

=========================================================================================================
Speaker: ROSS
Number of Total Listeners: 21

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
63 features selected out of 126 total
F1 mean: 0.34 (+/- 0.03)

             precision    recall  f1-score   support

NORTHUMBERLAND       0.52      1.00      0.69        11
 WILLOUGHBY       0.00      0.00      0.00        10

avg / total       0.27      0.52      0.36        21

=========================================================================================================

Play: 25

=========================================================================================================
Speaker: SECOND MURDERER
Number of Total Listeners: 42

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
127 features selected out of 255 total
F1 mean: 0.39 (+/- 0.06)

             precision    recall  f1-score   support

   CLARENCE       0.00      0.00      0.00        14
FIRST MURDERER       0.67      1.00      0.80        28

avg / total       0.44      0.67      0.53        42

=========================================================================================================

=========================================================================================================
Speaker: BUCKINGHAM
Number of Total Listeners: 132

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
492 features selected out of 984 total
F1 mean: 0.09 (+/- 0.03)

             precision    recall  f1-score   support

BISHOP OF ELY       0.00      0.00      0.00         5
    CATESBY       0.50      0.14      0.22        14
 GLOUCESTER       0.57      0.97      0.72        68
   HASTINGS       0.71      0.45      0.56        11
      MAYOR       0.50      0.21      0.30        14
PRINCE OF WALES       0.00      0.00      0.00        10
QUEEN MARGARET       0.00      0.00      0.00         5
     RIVERS       0.00      0.00      0.00         5

avg / total       0.46      0.58      0.47       132

=========================================================================================================

=========================================================================================================
Speaker: MESSENGER
Number of Total Listeners: 21

Best model LogisticRegression(C=1.5000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
62 features selected out of 125 total
F1 mean: 0.19 (+/- 0.23)

             precision    recall  f1-score   support

ARCHBISHOP OF YORK       0.00      0.00      0.00         5
DUCHESS OF YORK       0.00      0.00      0.00         5
   HASTINGS       1.00      1.00      1.00         6
QUEEN ELIZABETH       0.33      1.00      0.50         5

avg / total       0.37      0.52      0.40        21

=========================================================================================================

=========================================================================================================
Speaker: DUCHESS OF YORK
Number of Total Listeners: 64

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
262 features selected out of 525 total
F1 mean: 0.09 (+/- 0.04)

             precision    recall  f1-score   support

ARCHBISHOP OF YORK       0.00      0.00      0.00         7
DAUGHTER OF CLARENCE       0.00      0.00      0.00         5
 GLOUCESTER       1.00      0.50      0.67        12
QUEEN ELIZABETH       0.54      1.00      0.70        26
SON OF CLARENCE       0.50      0.43      0.46         7
       YORK       0.50      0.29      0.36         7

avg / total       0.52      0.58      0.50        64

=========================================================================================================

=========================================================================================================
Speaker: CATESBY
Number of Total Listeners: 33

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
107 features selected out of 215 total
F1 mean: 0.22 (+/- 0.04)

             precision    recall  f1-score   support

 BUCKINGHAM       1.00      0.22      0.36         9
 GLOUCESTER       0.63      1.00      0.77        17
   HASTINGS       1.00      0.57      0.73         7

avg / total       0.81      0.70      0.65        33

=========================================================================================================

=========================================================================================================
Speaker: QUEEN ELIZABETH
Number of Total Listeners: 139

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
432 features selected out of 865 total
F1 mean: 0.06 (+/- 0.01)

             precision    recall  f1-score   support

       ANNE       0.00      0.00      0.00         9
ARCHBISHOP OF YORK       0.00      0.00      0.00         7
 BUCKINGHAM       0.00      0.00      0.00         6
     DORSET       0.00      0.00      0.00         5
DUCHESS OF YORK       0.54      0.54      0.54        24
 GLOUCESTER       0.51      1.00      0.68        58
       GREY       0.00      0.00      0.00         6
QUEEN MARGARET       0.00      0.00      0.00         9
     RIVERS       0.00      0.00      0.00         8
    STANLEY       0.50      0.14      0.22         7

avg / total       0.33      0.52      0.39       139

=========================================================================================================

=========================================================================================================
Speaker: STANLEY
Number of Total Listeners: 20

Best model LogisticRegression(C=0.30000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
103 features selected out of 207 total
F1 mean: 0.62 (+/- 0.62)

             precision    recall  f1-score   support

 GLOUCESTER       1.00      1.00      1.00        14
QUEEN ELIZABETH       1.00      1.00      1.00         6

avg / total       1.00      1.00      1.00        20

=========================================================================================================

=========================================================================================================
Speaker: ANNE
Number of Total Listeners: 55

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
244 features selected out of 488 total
F1 mean: 0.29 (+/- 0.02)

             precision    recall  f1-score   support

DUCHESS OF YORK       0.00      0.00      0.00         6
 GLOUCESTER       0.85      1.00      0.92        41
QUEEN ELIZABETH       0.57      0.50      0.53         8

avg / total       0.72      0.82      0.76        55

=========================================================================================================

=========================================================================================================
Speaker: HASTINGS
Number of Total Listeners: 48

Best model LogisticRegression(C=1.2000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
235 features selected out of 471 total
F1 mean: 0.29 (+/- 0.21)

             precision    recall  f1-score   support

 BUCKINGHAM       1.00      0.60      0.75        10
    CATESBY       1.00      1.00      1.00         8
 GLOUCESTER       0.64      0.93      0.76        15
  MESSENGER       1.00      1.00      1.00         5
   RATCLIFF       0.50      0.40      0.44         5
     RIVERS       1.00      0.60      0.75         5

avg / total       0.83      0.79      0.79        48

=========================================================================================================

=========================================================================================================
Speaker: CLARENCE
Number of Total Listeners: 51

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
296 features selected out of 592 total
F1 mean: 0.25 (+/- 0.20)

             precision    recall  f1-score   support

FIRST MURDERER       0.47      0.85      0.61        20
 GLOUCESTER       1.00      0.57      0.73         7
     KEEPER       1.00      1.00      1.00         5
SECOND MURDERER       0.50      0.16      0.24        19

avg / total       0.61      0.57      0.53        51

=========================================================================================================

=========================================================================================================
Speaker: RIVERS
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
100 features selected out of 200 total
F1 mean: 0.18 (+/- 0.20)

             precision    recall  f1-score   support

 GLOUCESTER       0.60      0.38      0.46         8
       GREY       1.00      0.80      0.89         5
   HASTINGS       1.00      0.20      0.33         5
QUEEN ELIZABETH       0.47      0.89      0.62         9

avg / total       0.71      0.59      0.57        27

=========================================================================================================

=========================================================================================================
Speaker: FIRST MURDERER
Number of Total Listeners: 46

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
102 features selected out of 204 total
F1 mean: 0.39 (+/- 0.02)

             precision    recall  f1-score   support

   CLARENCE       0.00      0.00      0.00        16
SECOND MURDERER       0.65      1.00      0.79        30

avg / total       0.43      0.65      0.51        46

=========================================================================================================

=========================================================================================================
Speaker: YORK
Number of Total Listeners: 32

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
97 features selected out of 194 total
F1 mean: 0.28 (+/- 0.25)

             precision    recall  f1-score   support

DUCHESS OF YORK       1.00      0.50      0.67         6
 GLOUCESTER       0.52      0.93      0.67        15
PRINCE OF WALES       0.50      0.09      0.15        11

avg / total       0.60      0.56      0.49        32

=========================================================================================================

=========================================================================================================
Speaker: QUEEN MARGARET
Number of Total Listeners: 67

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
363 features selected out of 726 total
F1 mean: 0.04 (+/- 0.05)

             precision    recall  f1-score   support

 BUCKINGHAM       0.33      0.14      0.20         7
     DORSET       0.00      0.00      0.00         5
DUCHESS OF YORK       0.50      0.11      0.18         9
 GLOUCESTER       0.44      0.95      0.60        22
QUEEN ELIZABETH       0.50      0.41      0.45        17
     RIVERS       0.00      0.00      0.00         7

avg / total       0.37      0.45      0.36        67

=========================================================================================================

=========================================================================================================
Speaker: PRINCE OF WALES
Number of Total Listeners: 39

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
102 features selected out of 205 total
F1 mean: 0.15 (+/- 0.16)

             precision    recall  f1-score   support

 BUCKINGHAM       0.50      0.40      0.44        15
 GLOUCESTER       0.44      0.71      0.55        17
       YORK       0.00      0.00      0.00         7

avg / total       0.39      0.46      0.41        39

=========================================================================================================

=========================================================================================================
Speaker: GLOUCESTER
Number of Total Listeners: 383

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
1110 features selected out of 2220 total
F1 mean: 0.05 (+/- 0.02)

             precision    recall  f1-score   support

       ANNE       0.87      0.75      0.80        44
 BRAKENBURY       0.00      0.00      0.00         6
 BUCKINGHAM       0.47      0.92      0.62        74
    CATESBY       0.80      0.33      0.47        24
   CLARENCE       0.60      0.60      0.60        10
DUCHESS OF YORK       1.00      0.08      0.15        12
   HASTINGS       1.00      0.33      0.50        15
KING EDWARD       1.00      0.20      0.33         5
      MAYOR       0.50      0.11      0.18         9
    NORFOLK       0.83      0.45      0.59        11
PRINCE OF WALES       0.00      0.00      0.00        17
QUEEN ELIZABETH       0.50      0.96      0.65        70
QUEEN MARGARET       0.80      0.24      0.36        17
   RATCLIFF       0.77      0.48      0.59        21
     RIVERS       0.67      0.13      0.22        15
    STANLEY       1.00      0.50      0.67        12
     TYRREL       1.00      0.38      0.55         8
       YORK       0.00      0.00      0.00        13

avg / total       0.62      0.57      0.52       383

=========================================================================================================

Play: 26

=========================================================================================================
Speaker: SAMPSON
Number of Total Listeners: 29

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
71 features selected out of 143 total
F1 mean: 0.37 (+/- 0.12)

             precision    recall  f1-score   support

      ABRAM       0.00      0.00      0.00         9
    GREGORY       0.69      1.00      0.82        20

avg / total       0.48      0.69      0.56        29

=========================================================================================================

=========================================================================================================
Speaker: TYBALT
Number of Total Listeners: 30

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
84 features selected out of 169 total
F1 mean: 0.13 (+/- 0.14)

             precision    recall  f1-score   support

   BENVOLIO       1.00      0.33      0.50         6
    CAPULET       1.00      0.17      0.29         6
   MERCUTIO       0.57      0.50      0.53         8
      ROMEO       0.45      0.90      0.60        10

avg / total       0.70      0.53      0.50        30

=========================================================================================================

=========================================================================================================
Speaker: LADY CAPULET
Number of Total Listeners: 62

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
199 features selected out of 399 total
F1 mean: 0.22 (+/- 0.15)

             precision    recall  f1-score   support

    CAPULET       1.00      0.31      0.47        13
     JULIET       0.64      1.00      0.78        28
      NURSE       0.71      0.48      0.57        21

avg / total       0.74      0.68      0.64        62

=========================================================================================================

=========================================================================================================
Speaker: NURSE
Number of Total Listeners: 137

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
370 features selected out of 741 total
F1 mean: 0.11 (+/- 0.05)

             precision    recall  f1-score   support

    CAPULET       0.00      0.00      0.00        10
FRIAR LAURENCE       1.00      0.22      0.36         9
     JULIET       0.52      1.00      0.68        46
LADY CAPULET       0.75      0.30      0.43        20
   MERCUTIO       0.00      0.00      0.00        10
      PETER       0.33      0.09      0.14        11
      ROMEO       0.57      0.65      0.61        31

avg / total       0.50      0.55      0.46       137

=========================================================================================================

=========================================================================================================
Speaker: CAPULET
Number of Total Listeners: 86

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
403 features selected out of 807 total
F1 mean: 0.06 (+/- 0.04)

             precision    recall  f1-score   support

     JULIET       0.33      0.08      0.12        13
LADY CAPULET       0.49      0.77      0.60        26
      NURSE       0.46      0.65      0.54        20
      PARIS       0.71      0.50      0.59        10
     PRINCE       1.00      0.67      0.80         6
 SERVINGMAN       0.00      0.00      0.00         6
     TYBALT       1.00      0.60      0.75         5

avg / total       0.52      0.53      0.49        86

=========================================================================================================

=========================================================================================================
Speaker: ROMEO
Number of Total Listeners: 208

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
710 features selected out of 1421 total
F1 mean: 0.13 (+/- 0.09)

             precision    recall  f1-score   support

  BALTHASAR       0.64      1.00      0.78         9
   BENVOLIO       0.60      0.73      0.66        44
    CAPULET       0.67      0.33      0.44         6
FRIAR LAURENCE       0.88      0.81      0.85        27
     JULIET       0.84      0.97      0.90        37
   MERCUTIO       0.57      0.82      0.67        39
      NURSE       0.85      0.50      0.63        22
      PARIS       0.00      0.00      0.00         6
 SERVINGMAN       1.00      0.10      0.18        10
     TYBALT       0.00      0.00      0.00         8

avg / total       0.68      0.70      0.66       208

=========================================================================================================

=========================================================================================================
Speaker: MERCUTIO
Number of Total Listeners: 101

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
449 features selected out of 899 total
F1 mean: 0.18 (+/- 0.08)

             precision    recall  f1-score   support

   BENVOLIO       0.61      0.71      0.66        42
      NURSE       1.00      0.12      0.22         8
      ROMEO       0.59      0.71      0.65        42
     TYBALT       0.00      0.00      0.00         9

avg / total       0.58      0.60      0.56       101

=========================================================================================================

=========================================================================================================
Speaker: FRIAR LAURENCE
Number of Total Listeners: 61

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
451 features selected out of 903 total
F1 mean: 0.12 (+/- 0.01)

             precision    recall  f1-score   support

  BALTHASAR       1.00      0.33      0.50         9
     JULIET       0.75      0.67      0.71         9
      NURSE       1.00      0.20      0.33         5
      PARIS       1.00      0.38      0.55         8
      ROMEO       0.65      1.00      0.79        30

avg / total       0.79      0.70      0.67        61

=========================================================================================================

=========================================================================================================
Speaker: PRINCE
Number of Total Listeners: 22

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
155 features selected out of 311 total
F1 mean: 0.16 (+/- 0.19)

             precision    recall  f1-score   support

    CAPULET       0.58      0.78      0.67         9
FRIAR LAURENCE       0.67      0.40      0.50         5
   MONTAGUE       0.57      0.50      0.53         8

avg / total       0.60      0.59      0.58        22

=========================================================================================================

=========================================================================================================
Speaker: JULIET
Number of Total Listeners: 144

Best model LogisticRegression(C=1.2000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
585 features selected out of 1171 total
F1 mean: 0.18 (+/- 0.11)

             precision    recall  f1-score   support

    CAPULET       0.50      0.14      0.22         7
FRIAR LAURENCE       0.89      0.53      0.67        15
LADY CAPULET       0.69      0.78      0.73        23
      NURSE       0.77      0.82      0.80        45
      PARIS       0.64      0.88      0.74         8
      ROMEO       0.85      0.89      0.87        46

avg / total       0.78      0.78      0.77       144

=========================================================================================================

=========================================================================================================
Speaker: PARIS
Number of Total Listeners: 26

Best model LogisticRegression(C=1.2000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
93 features selected out of 186 total
F1 mean: 0.25 (+/- 0.38)

             precision    recall  f1-score   support

    CAPULET       1.00      0.71      0.83         7
FRIAR LAURENCE       0.60      0.60      0.60        10
     JULIET       0.64      0.78      0.70         9

avg / total       0.72      0.69      0.70        26

=========================================================================================================

=========================================================================================================
Speaker: BENVOLIO
Number of Total Listeners: 89

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
284 features selected out of 569 total
F1 mean: 0.11 (+/- 0.05)

             precision    recall  f1-score   support

    CAPULET       1.00      0.17      0.29         6
   MERCUTIO       0.75      0.32      0.45        28
   MONTAGUE       1.00      0.29      0.44         7
      ROMEO       0.57      0.98      0.72        43
     TYBALT       0.00      0.00      0.00         5

avg / total       0.66      0.61      0.54        89

=========================================================================================================

Play: 27

=========================================================================================================
Speaker: TRANIO
Number of Total Listeners: 155

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
336 features selected out of 672 total
F1 mean: 0.03 (+/- 0.04)

             precision    recall  f1-score   support

   BAPTISTA       0.42      0.66      0.51        32
     BIANCA       0.67      0.20      0.31        10
  BIONDELLO       0.50      0.07      0.12        15
     GREMIO       0.37      0.74      0.49        31
  HORTENSIO       0.56      0.42      0.48        12
   LUCENTIO       0.50      0.17      0.25        18
     PEDANT       0.56      0.56      0.56        18
  PETRUCHIO       0.25      0.05      0.09        19

avg / total       0.45      0.43      0.38       155

=========================================================================================================

=========================================================================================================
Speaker: BAPTISTA
Number of Total Listeners: 145

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
219 features selected out of 439 total
F1 mean: 0.04 (+/- 0.03)

             precision    recall  f1-score   support

     BIANCA       0.00      0.00      0.00         5
  BIONDELLO       0.50      0.29      0.36        14
     GREMIO       0.50      0.13      0.21        23
  HORTENSIO       0.00      0.00      0.00        11
  KATHERINA       0.50      0.10      0.17        10
   LUCENTIO       0.00      0.00      0.00         9
  PETRUCHIO       0.34      0.69      0.45        32
     TRANIO       0.41      0.76      0.53        34
  VINCENTIO       0.00      0.00      0.00         7

avg / total       0.33      0.39      0.30       145

=========================================================================================================

=========================================================================================================
Speaker: TAILOR
Number of Total Listeners: 22

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
40 features selected out of 80 total
F1 mean: 0.37 (+/- 0.13)

             precision    recall  f1-score   support

     GRUMIO       0.53      0.73      0.62        11
  PETRUCHIO       0.57      0.36      0.44        11

avg / total       0.55      0.55      0.53        22

=========================================================================================================

=========================================================================================================
Speaker: HORTENSIO
Number of Total Listeners: 127

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
292 features selected out of 585 total
F1 mean: 0.05 (+/- 0.03)

             precision    recall  f1-score   support

   BAPTISTA       0.00      0.00      0.00         9
     BIANCA       0.43      0.25      0.32        12
     GREMIO       0.00      0.00      0.00         8
     GRUMIO       0.50      0.27      0.35        15
  KATHERINA       0.00      0.00      0.00        12
   LUCENTIO       0.44      0.37      0.40        19
  PETRUCHIO       0.42      0.93      0.58        41
     TRANIO       0.67      0.36      0.47        11

avg / total       0.36      0.44      0.36       127

=========================================================================================================

=========================================================================================================
Speaker: BIONDELLO
Number of Total Listeners: 54

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
180 features selected out of 360 total
F1 mean: 0.13 (+/- 0.11)

             precision    recall  f1-score   support

   BAPTISTA       0.48      0.79      0.59        14
   LUCENTIO       0.85      0.73      0.79        15
     PEDANT       0.50      0.14      0.22         7
     TRANIO       0.55      0.50      0.52        12
  VINCENTIO       0.60      0.50      0.55         6

avg / total       0.61      0.59      0.58        54

=========================================================================================================

=========================================================================================================
Speaker: LORD
Number of Total Listeners: 26

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
228 features selected out of 457 total
F1 mean: 0.10 (+/- 0.10)

             precision    recall  f1-score   support

FIRST HUNTSMAN       0.57      0.57      0.57         7
    PLAYERS       0.67      0.40      0.50         5
SECOND HUNTSMAN       0.50      0.20      0.29         5
        SLY       0.57      0.89      0.70         9

avg / total       0.58      0.58      0.55        26

=========================================================================================================

=========================================================================================================
Speaker: GRUMIO
Number of Total Listeners: 86

Best model LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=200, multi_class='ovr',
          penalty='l1', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=1)
269 features selected out of 538 total
F1 mean: 0.26 (+/- 0.16)

             precision    recall  f1-score   support

     CURTIS       1.00      1.00      1.00        20
     GREMIO       0.00      0.00      0.00         7
  HORTENSIO       0.50      0.33      0.40        15
  KATHERINA       1.00      0.78      0.88         9
  PETRUCHIO       0.51      0.92      0.66        25
     TAILOR       0.75      0.30      0.43        10

avg / total       0.66      0.67      0.63        86

=========================================================================================================

=========================================================================================================
Speaker: SLY
Number of Total Listeners: 39

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
123 features selected out of 247 total
F1 mean: 0.08 (+/- 0.09)

             precision    recall  f1-score   support

FIRST SERVANT       0.40      0.25      0.31         8
       LORD       0.33      0.14      0.20         7
       PAGE       0.46      0.92      0.62        13
SECOND SERVANT       0.40      0.40      0.40         5
THIRD SERVANT       0.00      0.00      0.00         6

avg / total       0.35      0.44      0.36        39

=========================================================================================================

=========================================================================================================
Speaker: BIANCA
Number of Total Listeners: 47

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
126 features selected out of 253 total
F1 mean: 0.12 (+/- 0.06)

             precision    recall  f1-score   support

  HORTENSIO       0.50      0.17      0.25        12
   LUCENTIO       0.41      0.89      0.57        19
  PETRUCHIO       0.00      0.00      0.00         5
     TRANIO       0.00      0.00      0.00         6
  VINCENTIO       0.50      0.20      0.29         5

avg / total       0.35      0.43      0.32        47

=========================================================================================================

=========================================================================================================
Speaker: VINCENTIO
Number of Total Listeners: 52

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
101 features selected out of 203 total
F1 mean: 0.02 (+/- 0.06)

             precision    recall  f1-score   support

   BAPTISTA       0.32      0.60      0.41        10
  BIONDELLO       0.67      0.67      0.67         6
     GREMIO       0.00      0.00      0.00         7
   LUCENTIO       0.33      0.17      0.22         6
     PEDANT       0.40      0.44      0.42         9
  PETRUCHIO       0.57      0.89      0.70         9
     TRANIO       0.00      0.00      0.00         5

avg / total       0.34      0.44      0.38        52

=========================================================================================================

=========================================================================================================
Speaker: PEDANT
Number of Total Listeners: 34

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
109 features selected out of 218 total
F1 mean: 0.13 (+/- 0.18)

             precision    recall  f1-score   support

  BIONDELLO       0.00      0.00      0.00         7
  PETRUCHIO       0.00      0.00      0.00         6
     TRANIO       0.54      1.00      0.70        13
  VINCENTIO       0.50      0.62      0.56         8

avg / total       0.32      0.53      0.40        34

=========================================================================================================

=========================================================================================================
Speaker: KATHERINA
Number of Total Listeners: 114

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
345 features selected out of 691 total
F1 mean: 0.10 (+/- 0.01)

             precision    recall  f1-score   support

   BAPTISTA       1.00      0.22      0.36         9
     BIANCA       0.00      0.00      0.00         5
     GREMIO       0.00      0.00      0.00         6
     GRUMIO       1.00      0.18      0.31        11
  HORTENSIO       0.50      0.07      0.12        15
  PETRUCHIO       0.57      0.98      0.73        63
      WIDOW       0.00      0.00      0.00         5

avg / total       0.56      0.59      0.47       114

=========================================================================================================

=========================================================================================================
Speaker: LUCENTIO
Number of Total Listeners: 93

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
189 features selected out of 378 total
F1 mean: 0.09 (+/- 0.11)

             precision    recall  f1-score   support

   BAPTISTA       0.38      0.43      0.40         7
     BIANCA       0.35      0.39      0.37        18
  BIONDELLO       0.83      0.83      0.83        12
     GREMIO       1.00      0.29      0.44         7
  HORTENSIO       0.40      0.86      0.55        21
  PETRUCHIO       0.50      0.25      0.33        12
     TRANIO       0.00      0.00      0.00        11
  VINCENTIO       0.00      0.00      0.00         5

avg / total       0.43      0.46      0.41        93

=========================================================================================================

=========================================================================================================
Speaker: GREMIO
Number of Total Listeners: 111

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
267 features selected out of 534 total
F1 mean: 0.06 (+/- 0.04)

             precision    recall  f1-score   support

   BAPTISTA       0.44      0.18      0.26        22
     GRUMIO       0.33      0.25      0.29         8
  HORTENSIO       0.50      0.08      0.14        12
   LUCENTIO       0.00      0.00      0.00         9
  PETRUCHIO       0.45      0.58      0.51        24
     TRANIO       0.41      0.87      0.56        30
  VINCENTIO       0.00      0.00      0.00         6

avg / total       0.38      0.42      0.35       111

=========================================================================================================

=========================================================================================================
Speaker: PETRUCHIO
Number of Total Listeners: 328

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
642 features selected out of 1284 total
F1 mean: 0.04 (+/- 0.04)

             precision    recall  f1-score   support

   BAPTISTA       0.43      0.32      0.36        38
     BIANCA       0.00      0.00      0.00         6
  BIONDELLO       0.00      0.00      0.00         9
     GREMIO       0.38      0.10      0.16        29
     GRUMIO       0.73      0.32      0.45        34
  HORTENSIO       0.36      0.44      0.40        61
  KATHERINA       0.39      0.93      0.55        82
   LUCENTIO       0.00      0.00      0.00        18
     TAILOR       0.00      0.00      0.00         7
     TRANIO       0.50      0.05      0.10        19
  VINCENTIO       0.60      0.25      0.35        12
      WIDOW       0.00      0.00      0.00        13

avg / total       0.37      0.41      0.33       328

=========================================================================================================

Play: 28

=========================================================================================================
Speaker: ALONSO
Number of Total Listeners: 85

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
211 features selected out of 422 total
F1 mean: 0.10 (+/- 0.07)

             precision    recall  f1-score   support

    ANTONIO       1.00      0.07      0.12        15
  FRANCISCO       0.00      0.00      0.00         5
    GONZALO       0.39      0.79      0.52        24
   PROSPERO       1.00      0.35      0.52        17
  SEBASTIAN       0.45      0.54      0.49        24

avg / total       0.61      0.46      0.41        85

=========================================================================================================

=========================================================================================================
Speaker: GONZALO
Number of Total Listeners: 104

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
278 features selected out of 557 total
F1 mean: 0.13 (+/- 0.05)

             precision    recall  f1-score   support

     ADRIAN       1.00      0.20      0.33         5
     ALONSO       0.83      0.21      0.33        24
    ANTONIO       0.43      0.51      0.47        37
  SEBASTIAN       0.45      0.63      0.53        38

avg / total       0.56      0.47      0.45       104

=========================================================================================================

=========================================================================================================
Speaker: ANTONIO
Number of Total Listeners: 116

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
266 features selected out of 533 total
F1 mean: 0.15 (+/- 0.03)

             precision    recall  f1-score   support

     ADRIAN       0.00      0.00      0.00         8
     ALONSO       0.00      0.00      0.00        15
    GONZALO       0.00      0.00      0.00        35
  SEBASTIAN       0.50      1.00      0.67        58

avg / total       0.25      0.50      0.33       116

=========================================================================================================

=========================================================================================================
Speaker: PROSPERO
Number of Total Listeners: 167

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
744 features selected out of 1488 total
F1 mean: 0.11 (+/- 0.07)

             precision    recall  f1-score   support

     ALONSO       0.48      0.57      0.52        21
      ARIEL       0.58      0.85      0.69        41
    CALIBAN       1.00      0.11      0.20         9
  FERDINAND       0.69      0.32      0.44        28
    GONZALO       0.50      0.20      0.29        10
    MIRANDA       0.67      0.88      0.76        49
  SEBASTIAN       0.00      0.00      0.00         9

avg / total       0.60      0.61      0.56       167

=========================================================================================================

=========================================================================================================
Speaker: CALIBAN
Number of Total Listeners: 97

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
277 features selected out of 554 total
F1 mean: 0.11 (+/- 0.03)

             precision    recall  f1-score   support

      ARIEL       0.00      0.00      0.00         9
    MIRANDA       0.50      0.20      0.29         5
   PROSPERO       0.75      0.33      0.46         9
   STEPHANO       0.45      1.00      0.62        41
   TRINCULO       0.00      0.00      0.00        33

avg / total       0.29      0.46      0.32        97

=========================================================================================================

=========================================================================================================
Speaker: STEPHANO
Number of Total Listeners: 124

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
269 features selected out of 538 total
F1 mean: 0.22 (+/- 0.05)

             precision    recall  f1-score   support

      ARIEL       0.00      0.00      0.00         9
    CALIBAN       0.47      0.52      0.50        54
   PROSPERO       0.00      0.00      0.00         6
   TRINCULO       0.48      0.56      0.52        55

avg / total       0.42      0.48      0.44       124

=========================================================================================================

=========================================================================================================
Speaker: MIRANDA
Number of Total Listeners: 72

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
244 features selected out of 489 total
F1 mean: 0.35 (+/- 0.09)

             precision    recall  f1-score   support

  FERDINAND       1.00      0.08      0.15        25
   PROSPERO       0.67      1.00      0.80        47

avg / total       0.79      0.68      0.58        72

=========================================================================================================

=========================================================================================================
Speaker: FERDINAND
Number of Total Listeners: 49

Best model LogisticRegression(C=0.70000000000000007, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
219 features selected out of 438 total
F1 mean: 0.39 (+/- 0.17)

             precision    recall  f1-score   support

    MIRANDA       0.62      0.43      0.51        23
   PROSPERO       0.61      0.77      0.68        26

avg / total       0.61      0.61      0.60        49

=========================================================================================================

=========================================================================================================
Speaker: SEBASTIAN
Number of Total Listeners: 127

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
222 features selected out of 444 total
F1 mean: 0.11 (+/- 0.04)

             precision    recall  f1-score   support

     ADRIAN       0.00      0.00      0.00         9
     ALONSO       1.00      0.05      0.10        19
    ANTONIO       0.47      0.98      0.64        57
    GONZALO       0.43      0.08      0.14        37
   PROSPERO       0.00      0.00      0.00         5

avg / total       0.49      0.47      0.34       127

=========================================================================================================

=========================================================================================================
Speaker: TRINCULO
Number of Total Listeners: 70

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
195 features selected out of 390 total
F1 mean: 0.35 (+/- 0.04)

             precision    recall  f1-score   support

    CALIBAN       0.60      0.09      0.16        32
   STEPHANO       0.55      0.95      0.70        38

avg / total       0.57      0.56      0.45        70

=========================================================================================================

=========================================================================================================
Speaker: ARIEL
Number of Total Listeners: 51

Best model LogisticRegression(C=1.1000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
330 features selected out of 661 total
F1 mean: 0.54 (+/- 0.47)

             precision    recall  f1-score   support

    GONZALO       0.75      0.60      0.67         5
   PROSPERO       0.93      0.98      0.95        41
   STEPHANO       1.00      0.80      0.89         5

avg / total       0.92      0.92      0.92        51

=========================================================================================================

Play: 29

=========================================================================================================
Speaker: ALCIBIADES
Number of Total Listeners: 48

Best model LogisticRegression(C=0.59999999999999998, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
237 features selected out of 475 total
F1 mean: 0.49 (+/- 0.20)

             precision    recall  f1-score   support

FIRST SENATOR       0.57      0.62      0.59        13
SECOND SENATOR       0.56      0.42      0.48        12
      TIMON       0.92      1.00      0.96        23

avg / total       0.73      0.75      0.74        48

=========================================================================================================

=========================================================================================================
Speaker: THIRD LORD
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
28 features selected out of 57 total
F1 mean: 0.22 (+/- 0.02)

             precision    recall  f1-score   support

 FIRST LORD       0.00      0.00      0.00         6
SECOND LORD       0.48      1.00      0.65        11
      TIMON       0.00      0.00      0.00         6

avg / total       0.23      0.48      0.31        23

=========================================================================================================

=========================================================================================================
Speaker: SECOND SENATOR
Number of Total Listeners: 26

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
136 features selected out of 272 total
F1 mean: 0.17 (+/- 0.04)

             precision    recall  f1-score   support

 ALCIBIADES       0.00      0.00      0.00         7
FIRST SENATOR       0.54      1.00      0.70        14
      TIMON       0.00      0.00      0.00         5

avg / total       0.29      0.54      0.38        26

=========================================================================================================

=========================================================================================================
Speaker: APEMANTUS
Number of Total Listeners: 135

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
358 features selected out of 717 total
F1 mean: 0.08 (+/- 0.02)

             precision    recall  f1-score   support

ALL SERVANTS       0.00      0.00      0.00         6
     CAPHIS       0.00      0.00      0.00         5
 FIRST LORD       0.00      0.00      0.00        13
       FOOL       1.00      0.09      0.17        11
   MERCHANT       0.00      0.00      0.00         5
       POET       0.00      0.00      0.00         8
SECOND LORD       0.00      0.00      0.00        12
      TIMON       0.56      1.00      0.72        75

avg / total       0.39      0.56      0.41       135

=========================================================================================================

=========================================================================================================
Speaker: FLAVIUS
Number of Total Listeners: 37

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
271 features selected out of 543 total
F1 mean: 0.46 (+/- 0.01)

             precision    recall  f1-score   support

  FLAMINIUS       0.00      0.00      0.00         5
      TIMON       0.86      1.00      0.93        32

avg / total       0.75      0.86      0.80        37

=========================================================================================================

=========================================================================================================
Speaker: SECOND LORD
Number of Total Listeners: 48

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
89 features selected out of 179 total
F1 mean: 0.14 (+/- 0.09)

             precision    recall  f1-score   support

  APEMANTUS       0.00      0.00      0.00         6
 FIRST LORD       0.51      1.00      0.68        21
 THIRD LORD       0.00      0.00      0.00         8
      TIMON       0.57      0.31      0.40        13

avg / total       0.38      0.52      0.40        48

=========================================================================================================

=========================================================================================================
Speaker: TIMON
Number of Total Listeners: 339

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
883 features selected out of 1766 total
F1 mean: 0.03 (+/- 0.01)

             precision    recall  f1-score   support

 ALCIBIADES       0.75      0.39      0.51        31
  APEMANTUS       0.34      0.99      0.51        74
       BOTH       0.00      0.00      0.00         9
     CAPHIS       0.00      0.00      0.00         7
 FIRST LORD       0.47      0.33      0.39        21
FIRST SENATOR       0.50      0.09      0.15        11
  FLAMINIUS       0.00      0.00      0.00         5
    FLAVIUS       0.60      0.67      0.63        42
 HORTENSIUS       0.33      0.17      0.22         6
   JEWELLER       0.00      0.00      0.00         8
   LUCILIUS       0.50      0.11      0.18         9
   MERCHANT       0.00      0.00      0.00         7
  MESSENGER       1.00      0.20      0.33         5
OLD ATHENIAN       0.00      0.00      0.00        10
    PAINTER       0.43      0.59      0.50        22
   PHILOTUS       0.00      0.00      0.00         5
       POET       0.50      0.13      0.21        15
SECOND LORD       0.50      0.15      0.23        20
SECOND SENATOR       0.00      0.00      0.00         8
    SERVANT       0.00      0.00      0.00         6
 THIRD LORD       0.00      0.00      0.00         7
   TIMANDRA       0.00      0.00      0.00         5
      TITUS       0.00      0.00      0.00         6

avg / total       0.38      0.42      0.33       339

=========================================================================================================

=========================================================================================================
Speaker: FIRST LORD
Number of Total Listeners: 50

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
92 features selected out of 184 total
F1 mean: 0.15 (+/- 0.03)

             precision    recall  f1-score   support

  APEMANTUS       0.00      0.00      0.00         7
SECOND LORD       0.52      1.00      0.69        23
 THIRD LORD       0.00      0.00      0.00         6
      TIMON       0.50      0.21      0.30        14

avg / total       0.38      0.52      0.40        50

=========================================================================================================

=========================================================================================================
Speaker: FIRST SENATOR
Number of Total Listeners: 52

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
176 features selected out of 352 total
F1 mean: 0.10 (+/- 0.04)

             precision    recall  f1-score   support

 ALCIBIADES       1.00      0.25      0.40        12
    FLAVIUS       0.00      0.00      0.00         6
SECOND SENATOR       0.45      1.00      0.62        22
      TIMON       0.00      0.00      0.00        12

avg / total       0.42      0.48      0.35        52

=========================================================================================================

=========================================================================================================
Speaker: PAINTER
Number of Total Listeners: 35

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
136 features selected out of 273 total
F1 mean: 0.42 (+/- 0.00)

             precision    recall  f1-score   support

       POET       0.71      1.00      0.83        25
      TIMON       0.00      0.00      0.00        10

avg / total       0.51      0.71      0.60        35

=========================================================================================================

=========================================================================================================
Speaker: POET
Number of Total Listeners: 37

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
207 features selected out of 415 total
F1 mean: 0.24 (+/- 0.08)

             precision    recall  f1-score   support

  APEMANTUS       0.50      0.20      0.29         5
    PAINTER       0.69      1.00      0.81        24
      TIMON       0.00      0.00      0.00         8

avg / total       0.51      0.68      0.57        37

=========================================================================================================

Play: 30

=========================================================================================================
Speaker: TAMORA
Number of Total Listeners: 85

Best model LogisticRegression(C=0.90000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
416 features selected out of 832 total
F1 mean: 0.14 (+/- 0.07)

             precision    recall  f1-score   support

      AARON       1.00      0.40      0.57         5
     CHIRON       0.33      0.09      0.14        11
  DEMETRIUS       0.44      0.31      0.36        13
    LAVINIA       0.54      0.64      0.58        11
 SATURNINUS       0.67      0.70      0.68        20
      TITUS       0.54      0.80      0.65        25

avg / total       0.56      0.56      0.53        85

=========================================================================================================

=========================================================================================================
Speaker: LUCIUS
Number of Total Listeners: 88

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
303 features selected out of 606 total
F1 mean: 0.07 (+/- 0.08)

             precision    recall  f1-score   support

      AARON       0.61      0.90      0.73        21
 FIRST GOTH       1.00      0.29      0.44         7
       GOTH       0.00      0.00      0.00         5
     MARCUS       0.57      0.19      0.29        21
 SATURNINUS       0.00      0.00      0.00         8
      TITUS       0.50      0.92      0.65        26

avg / total       0.51      0.56      0.47        88

=========================================================================================================

=========================================================================================================
Speaker: CHIRON
Number of Total Listeners: 58

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
119 features selected out of 239 total
F1 mean: 0.15 (+/- 0.02)

             precision    recall  f1-score   support

      AARON       1.00      0.06      0.11        18
  DEMETRIUS       0.47      1.00      0.64        27
    LAVINIA       0.00      0.00      0.00         5
     TAMORA       0.00      0.00      0.00         8

avg / total       0.53      0.48      0.33        58

=========================================================================================================

=========================================================================================================
Speaker: TITUS
Number of Total Listeners: 222

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
771 features selected out of 1542 total
F1 mean: 0.05 (+/- 0.05)

             precision    recall  f1-score   support

  BASSIANUS       0.00      0.00      0.00         8
        BOY       0.00      0.00      0.00        14
     CHIRON       0.50      0.14      0.22         7
      CLOWN       1.00      0.25      0.40         8
  DEMETRIUS       0.00      0.00      0.00         7
     LUCIUS       0.48      0.29      0.36        35
     MARCUS       0.43      0.91      0.58        68
    PUBLIUS       0.50      0.29      0.36         7
    QUINTUS       0.00      0.00      0.00         5
 SATURNINUS       0.56      0.29      0.38        31
     TAMORA       0.56      0.56      0.56        32

avg / total       0.43      0.47      0.40       222

=========================================================================================================

=========================================================================================================
Speaker: SATURNINUS
Number of Total Listeners: 92

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
337 features selected out of 675 total
F1 mean: 0.05 (+/- 0.08)

             precision    recall  f1-score   support

      AARON       0.00      0.00      0.00         5
  BASSIANUS       1.00      0.10      0.18        10
     LUCIUS       0.00      0.00      0.00         8
     MARCUS       0.50      0.20      0.29        15
     TAMORA       0.63      0.50      0.56        24
      TITUS       0.42      0.93      0.58        30

avg / total       0.49      0.48      0.40        92

=========================================================================================================

=========================================================================================================
Speaker: AARON
Number of Total Listeners: 90

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
453 features selected out of 906 total
F1 mean: 0.13 (+/- 0.10)

             precision    recall  f1-score   support

     CHIRON       0.67      0.08      0.14        25
  DEMETRIUS       0.42      0.97      0.58        29
       GOTH       0.00      0.00      0.00         5
     LUCIUS       0.78      0.88      0.82        16
      NURSE       1.00      0.13      0.24        15

avg / total       0.62      0.51      0.41        90

=========================================================================================================

=========================================================================================================
Speaker: LAVINIA
Number of Total Listeners: 31

Best model LogisticRegression(C=0.40000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
125 features selected out of 251 total
F1 mean: 0.09 (+/- 0.05)

             precision    recall  f1-score   support

  BASSIANUS       1.00      0.40      0.57         5
     CHIRON       0.00      0.00      0.00         7
  DEMETRIUS       0.00      0.00      0.00         7
     TAMORA       0.41      1.00      0.59        12

avg / total       0.32      0.45      0.32        31

=========================================================================================================

=========================================================================================================
Speaker: MARCUS
Number of Total Listeners: 109

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
462 features selected out of 924 total
F1 mean: 0.17 (+/- 0.11)

             precision    recall  f1-score   support

        BOY       0.50      0.06      0.11        16
     LUCIUS       0.60      0.22      0.32        27
 SATURNINUS       0.67      0.20      0.31        10
      TITUS       0.57      0.96      0.72        56

avg / total       0.58      0.58      0.49       109

=========================================================================================================

=========================================================================================================
Speaker: DEMETRIUS
Number of Total Listeners: 80

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
213 features selected out of 427 total
F1 mean: 0.11 (+/- 0.08)

             precision    recall  f1-score   support

      AARON       0.50      0.19      0.28        26
     CHIRON       0.46      0.94      0.62        34
    LAVINIA       0.00      0.00      0.00         5
      NURSE       0.00      0.00      0.00         6
     TAMORA       0.00      0.00      0.00         9

avg / total       0.36      0.46      0.35        80

=========================================================================================================

Play: 31

=========================================================================================================
Speaker: ULYSSES
Number of Total Listeners: 174

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
717 features selected out of 1434 total
F1 mean: 0.07 (+/- 0.05)

             precision    recall  f1-score   support

   ACHILLES       0.67      0.43      0.52        14
  AGAMEMNON       0.45      0.19      0.26        27
       AJAX       0.00      0.00      0.00        15
   CRESSIDA       1.00      0.08      0.15        12
   DIOMEDES       0.43      0.14      0.21        22
     HECTOR       0.00      0.00      0.00         5
     NESTOR       0.39      0.90      0.54        40
  PATROCLUS       0.00      0.00      0.00         5
  THERSITES       0.00      0.00      0.00        10
    TROILUS       0.42      0.92      0.57        24

avg / total       0.39      0.42      0.32       174

=========================================================================================================

=========================================================================================================
Speaker: HECTOR
Number of Total Listeners: 121

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
343 features selected out of 686 total
F1 mean: 0.05 (+/- 0.05)

             precision    recall  f1-score   support

   ACHILLES       0.41      0.60      0.49        15
     AENEAS       0.38      0.38      0.38         8
  AGAMEMNON       0.00      0.00      0.00         9
       AJAX       0.38      0.27      0.32        11
 ANDROMACHE       0.50      0.44      0.47         9
  CASSANDRA       0.50      0.10      0.17        10
   DIOMEDES       0.40      0.22      0.29         9
   MENELAUS       0.00      0.00      0.00         6
     NESTOR       0.50      0.20      0.29         5
      PRIAM       0.50      0.20      0.29         5
  THERSITES       0.00      0.00      0.00         5
    TROILUS       0.32      0.91      0.48        22
    ULYSSES       0.50      0.14      0.22         7

avg / total       0.35      0.37      0.31       121

=========================================================================================================

=========================================================================================================
Speaker: DIOMEDES
Number of Total Listeners: 121

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
175 features selected out of 350 total
F1 mean: 0.09 (+/- 0.11)

             precision    recall  f1-score   support

     AENEAS       0.00      0.00      0.00         6
  AGAMEMNON       0.00      0.00      0.00         7
       AJAX       0.50      0.18      0.27        11
   CRESSIDA       0.31      0.88      0.46        26
     NESTOR       0.00      0.00      0.00         8
      PARIS       0.62      0.71      0.67         7
  THERSITES       0.00      0.00      0.00        16
    TROILUS       0.40      0.27      0.32        22
    ULYSSES       0.40      0.44      0.42        18

avg / total       0.28      0.36      0.28       121

=========================================================================================================

=========================================================================================================
Speaker: TROILUS
Number of Total Listeners: 219

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
724 features selected out of 1449 total
F1 mean: 0.04 (+/- 0.03)

             precision    recall  f1-score   support

     AENEAS       0.71      0.25      0.37        20
  CASSANDRA       0.00      0.00      0.00         5
   CRESSIDA       0.36      0.93      0.51        56
   DIOMEDES       1.00      0.05      0.10        20
     HECTOR       0.60      0.47      0.53        19
   PANDARUS       0.73      0.39      0.51        41
      PARIS       0.50      0.29      0.36         7
      PRIAM       0.00      0.00      0.00         6
  THERSITES       0.50      0.14      0.22        14
    ULYSSES       0.50      0.32      0.39        31

avg / total       0.55      0.44      0.40       219

=========================================================================================================

=========================================================================================================
Speaker: NESTOR
Number of Total Listeners: 68

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
278 features selected out of 557 total
F1 mean: 0.15 (+/- 0.04)

             precision    recall  f1-score   support

  AGAMEMNON       1.00      0.07      0.12        15
       AJAX       0.00      0.00      0.00        11
   DIOMEDES       0.00      0.00      0.00        10
    ULYSSES       0.48      1.00      0.65        32

avg / total       0.45      0.49      0.33        68

=========================================================================================================

=========================================================================================================
Speaker: AENEAS
Number of Total Listeners: 55

Best model LogisticRegression(C=0.30000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
244 features selected out of 488 total
F1 mean: 0.17 (+/- 0.05)

             precision    recall  f1-score   support

  AGAMEMNON       0.94      1.00      0.97        15
   DIOMEDES       0.00      0.00      0.00         7
   PANDARUS       1.00      0.22      0.36         9
      PARIS       0.58      0.78      0.67         9
    TROILUS       0.60      1.00      0.75        15

avg / total       0.68      0.71      0.64        55

=========================================================================================================

=========================================================================================================
Speaker: PARIS
Number of Total Listeners: 46

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
196 features selected out of 393 total
F1 mean: 0.13 (+/- 0.15)

             precision    recall  f1-score   support

     AENEAS       0.56      0.62      0.59         8
   DIOMEDES       0.75      0.38      0.50         8
      HELEN       0.40      1.00      0.57        12
   PANDARUS       0.00      0.00      0.00        12
    TROILUS       1.00      0.50      0.67         6

avg / total       0.46      0.50      0.43        46

=========================================================================================================

=========================================================================================================
Speaker: AJAX
Number of Total Listeners: 100

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
141 features selected out of 282 total
F1 mean: 0.08 (+/- 0.04)

             precision    recall  f1-score   support

   ACHILLES       0.50      0.23      0.32        13
  AGAMEMNON       0.50      0.47      0.48        17
   DIOMEDES       0.45      0.42      0.43        12
     HECTOR       0.00      0.00      0.00         7
     NESTOR       0.29      0.14      0.19        14
  THERSITES       0.41      1.00      0.58        21
    ULYSSES       0.44      0.25      0.32        16

avg / total       0.40      0.43      0.38       100

=========================================================================================================

=========================================================================================================
Speaker: ACHILLES
Number of Total Listeners: 121

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
280 features selected out of 560 total
F1 mean: 0.11 (+/- 0.10)

             precision    recall  f1-score   support

  AGAMEMNON       1.00      0.08      0.15        12
       AJAX       0.00      0.00      0.00        18
     HECTOR       0.75      0.56      0.64        16
  PATROCLUS       0.60      0.35      0.44        26
  THERSITES       0.37      0.94      0.53        35
    ULYSSES       0.75      0.21      0.33        14

avg / total       0.52      0.45      0.39       121

=========================================================================================================

=========================================================================================================
Speaker: THERSITES
Number of Total Listeners: 142

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
384 features selected out of 768 total
F1 mean: 0.08 (+/- 0.07)

             precision    recall  f1-score   support

   ACHILLES       0.45      0.76      0.56        45
       AJAX       0.78      0.60      0.68        30
   CRESSIDA       0.00      0.00      0.00         7
   DIOMEDES       0.40      0.20      0.27        10
  PATROCLUS       0.55      0.47      0.51        34
    TROILUS       0.43      0.38      0.40         8
    ULYSSES       0.50      0.12      0.20         8

avg / total       0.52      0.52      0.50       142

=========================================================================================================

=========================================================================================================
Speaker: AGAMEMNON
Number of Total Listeners: 108

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
332 features selected out of 664 total
F1 mean: 0.05 (+/- 0.04)

             precision    recall  f1-score   support

   ACHILLES       0.33      0.09      0.14        11
     AENEAS       0.69      0.60      0.64        15
       AJAX       0.60      0.25      0.35        12
   DIOMEDES       0.67      0.17      0.27        12
     HECTOR       0.00      0.00      0.00         5
   MENELAUS       0.00      0.00      0.00         7
     NESTOR       0.43      0.45      0.44        20
    ULYSSES       0.37      0.88      0.52        26

avg / total       0.44      0.44      0.38       108

=========================================================================================================

=========================================================================================================
Speaker: PATROCLUS
Number of Total Listeners: 56

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
124 features selected out of 249 total
F1 mean: 0.14 (+/- 0.11)

             precision    recall  f1-score   support

   ACHILLES       0.71      0.26      0.38        19
       AJAX       0.00      0.00      0.00         5
  THERSITES       0.50      1.00      0.67        24
    ULYSSES       1.00      0.12      0.22         8

avg / total       0.60      0.54      0.45        56

=========================================================================================================

=========================================================================================================
Speaker: PANDARUS
Number of Total Listeners: 204

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
475 features selected out of 951 total
F1 mean: 0.14 (+/- 0.09)

             precision    recall  f1-score   support

     AENEAS       0.00      0.00      0.00         7
        BOY       0.00      0.00      0.00         6
   CRESSIDA       0.58      0.94      0.72        89
      HELEN       0.48      0.54      0.51        24
      PARIS       0.50      0.10      0.17        20
    SERVANT       1.00      0.06      0.12        16
    TROILUS       0.75      0.50      0.60        42

avg / total       0.59      0.59      0.52       204

=========================================================================================================

=========================================================================================================
Speaker: HELEN
Number of Total Listeners: 32

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
60 features selected out of 121 total
F1 mean: 0.36 (+/- 0.03)

             precision    recall  f1-score   support

   PANDARUS       0.59      1.00      0.75        19
      PARIS       0.00      0.00      0.00        13

avg / total       0.35      0.59      0.44        32

=========================================================================================================

=========================================================================================================
Speaker: CRESSIDA
Number of Total Listeners: 226

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
392 features selected out of 785 total
F1 mean: 0.09 (+/- 0.04)

             precision    recall  f1-score   support

  ALEXANDER       0.00      0.00      0.00         9
   DIOMEDES       0.50      0.15      0.24        26
   MENELAUS       0.75      0.50      0.60         6
   PANDARUS       0.60      0.91      0.72        91
  THERSITES       0.50      0.07      0.12        14
    TROILUS       0.48      0.56      0.52        62
    ULYSSES       0.00      0.00      0.00        18

avg / total       0.48      0.56      0.48       226

=========================================================================================================

=========================================================================================================
Speaker: CASSANDRA
Number of Total Listeners: 31

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
86 features selected out of 173 total
F1 mean: 0.08 (+/- 0.10)

             precision    recall  f1-score   support

 ANDROMACHE       0.00      0.00      0.00         6
     HECTOR       0.41      0.85      0.55        13
      PRIAM       0.00      0.00      0.00         5
    TROILUS       0.50      0.29      0.36         7

avg / total       0.28      0.42      0.31        31

=========================================================================================================

Play: 32

=========================================================================================================
Speaker: ANTONIO
Number of Total Listeners: 39

Best model LogisticRegression(C=0.30000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
206 features selected out of 412 total
F1 mean: 0.20 (+/- 0.14)

             precision    recall  f1-score   support

FIRST OFFICER       0.33      0.14      0.20         7
  SEBASTIAN       0.67      1.00      0.80        14
SECOND OFFICER       0.00      0.00      0.00         7
      VIOLA       0.53      0.73      0.62        11

avg / total       0.45      0.59      0.50        39

=========================================================================================================

=========================================================================================================
Speaker: DUKE
Number of Total Listeners: 82

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
347 features selected out of 694 total
F1 mean: 0.18 (+/- 0.14)

             precision    recall  f1-score   support

      CLOWN       0.75      0.16      0.26        19
      CURIO       1.00      0.50      0.67         8
     OLIVIA       1.00      0.07      0.12        15
      VIOLA       0.55      1.00      0.71        40

avg / total       0.72      0.59      0.49        82

=========================================================================================================

=========================================================================================================
Speaker: MALVOLIO
Number of Total Listeners: 150

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
376 features selected out of 752 total
F1 mean: 0.14 (+/- 0.06)

             precision    recall  f1-score   support

  AGUECHEEK       0.00      0.00      0.00        10
      CLOWN       0.79      0.68      0.73        28
     FABIAN       0.50      0.07      0.12        28
      MARIA       0.00      0.00      0.00        16
     OLIVIA       0.73      0.81      0.77        27
   SIR TOBY       0.38      0.85      0.53        41

avg / total       0.48      0.52      0.44       150

=========================================================================================================

=========================================================================================================
Speaker: SIR TOBY
Number of Total Listeners: 285

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
481 features selected out of 963 total
F1 mean: 0.09 (+/- 0.07)

             precision    recall  f1-score   support

  AGUECHEEK       0.42      0.84      0.56        83
      CLOWN       0.45      0.20      0.28        25
     FABIAN       0.46      0.55      0.50        60
   MALVOLIO       0.50      0.11      0.19        35
      MARIA       0.57      0.27      0.36        49
     OLIVIA       0.00      0.00      0.00        11
  SEBASTIAN       0.00      0.00      0.00         5
      VIOLA       0.71      0.29      0.42        17

avg / total       0.46      0.46      0.40       285

=========================================================================================================

=========================================================================================================
Speaker: AGUECHEEK
Number of Total Listeners: 156

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
243 features selected out of 487 total
F1 mean: 0.09 (+/- 0.01)

             precision    recall  f1-score   support

      CLOWN       0.00      0.00      0.00        11
     FABIAN       0.00      0.00      0.00        22
   MALVOLIO       0.00      0.00      0.00         6
      MARIA       0.00      0.00      0.00        22
     OLIVIA       0.00      0.00      0.00         7
   SIR TOBY       0.50      1.00      0.67        78
      VIOLA       0.00      0.00      0.00        10

avg / total       0.25      0.50      0.33       156

=========================================================================================================

=========================================================================================================
Speaker: FABIAN
Number of Total Listeners: 98

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
206 features selected out of 412 total
F1 mean: 0.10 (+/- 0.01)

             precision    recall  f1-score   support

  AGUECHEEK       0.00      0.00      0.00        20
      CLOWN       1.00      0.20      0.33         5
   MALVOLIO       0.50      0.11      0.17        19
      MARIA       0.00      0.00      0.00         6
   SIR TOBY       0.46      0.98      0.62        43
      VIOLA       1.00      0.20      0.33         5

avg / total       0.40      0.47      0.34        98

=========================================================================================================

=========================================================================================================
Speaker: OLIVIA
Number of Total Listeners: 178

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
383 features selected out of 767 total
F1 mean: 0.09 (+/- 0.06)

             precision    recall  f1-score   support

  AGUECHEEK       0.00      0.00      0.00         5
      CLOWN       0.55      0.75      0.63        32
       DUKE       0.50      0.20      0.29        20
     FABIAN       0.00      0.00      0.00         7
   MALVOLIO       0.71      0.74      0.73        27
      MARIA       1.00      0.08      0.14        13
  SEBASTIAN       0.75      0.43      0.55         7
   SIR TOBY       0.00      0.00      0.00        13
      VIOLA       0.56      0.96      0.71        54

avg / total       0.53      0.58      0.50       178

=========================================================================================================

=========================================================================================================
Speaker: CLOWN
Number of Total Listeners: 137

Best model LogisticRegression(C=1.8000000000000003, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
364 features selected out of 729 total
F1 mean: 0.18 (+/- 0.15)

             precision    recall  f1-score   support

  AGUECHEEK       0.60      0.43      0.50         7
       DUKE       0.79      0.73      0.76        15
     FABIAN       1.00      0.33      0.50         6
   MALVOLIO       0.68      0.87      0.76        31
      MARIA       0.75      0.75      0.75        12
     OLIVIA       0.70      0.86      0.78        22
  SEBASTIAN       1.00      0.80      0.89         5
   SIR TOBY       0.50      0.38      0.43        21
      VIOLA       0.88      0.83      0.86        18

avg / total       0.72      0.72      0.70       137

=========================================================================================================

=========================================================================================================
Speaker: VIOLA
Number of Total Listeners: 154

Best model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=200, multi_class='ovr',
          penalty='l1', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=1)
418 features selected out of 836 total
F1 mean: 0.15 (+/- 0.19)

             precision    recall  f1-score   support

  AGUECHEEK       0.50      0.11      0.18         9
    ANTONIO       0.57      0.57      0.57         7
    CAPTAIN       1.00      1.00      1.00        11
      CLOWN       0.94      0.88      0.91        17
       DUKE       0.80      0.75      0.77        32
     FABIAN       0.71      0.71      0.71         7
     OLIVIA       0.79      0.92      0.85        48
  SEBASTIAN       0.75      0.60      0.67         5
   SIR TOBY       0.52      0.61      0.56        18

avg / total       0.76      0.77      0.75       154

=========================================================================================================

=========================================================================================================
Speaker: SEBASTIAN
Number of Total Listeners: 40

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
193 features selected out of 386 total
F1 mean: 0.11 (+/- 0.04)

             precision    recall  f1-score   support

    ANTONIO       0.59      1.00      0.74        17
       DUKE       0.50      0.20      0.29         5
     OLIVIA       0.60      0.38      0.46         8
   SIR TOBY       1.00      0.40      0.57         5
      VIOLA       0.50      0.20      0.29         5

avg / total       0.62      0.60      0.55        40

=========================================================================================================

=========================================================================================================
Speaker: MARIA
Number of Total Listeners: 100

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
254 features selected out of 508 total
F1 mean: 0.09 (+/- 0.03)

             precision    recall  f1-score   support

  AGUECHEEK       0.67      0.09      0.16        22
      CLOWN       1.00      0.43      0.60        14
     FABIAN       0.00      0.00      0.00         7
   MALVOLIO       0.00      0.00      0.00         9
     OLIVIA       1.00      0.25      0.40         8
   SIR TOBY       0.44      0.97      0.60        40

avg / total       0.54      0.49      0.39       100

=========================================================================================================

Play: 33

=========================================================================================================
Speaker: DUKE
Number of Total Listeners: 61

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
303 features selected out of 606 total
F1 mean: 0.23 (+/- 0.07)

             precision    recall  f1-score   support

    PROTEUS       0.70      0.74      0.72        19
     THURIO       0.75      0.20      0.32        15
  VALENTINE       0.73      1.00      0.84        27

avg / total       0.73      0.72      0.67        61

=========================================================================================================

=========================================================================================================
Speaker: LAUNCE
Number of Total Listeners: 74

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
219 features selected out of 439 total
F1 mean: 0.20 (+/- 0.02)

             precision    recall  f1-score   support

   PANTHINO       1.00      0.29      0.44         7
    PROTEUS       0.80      0.36      0.50        11
      SPEED       0.72      1.00      0.83        48
  VALENTINE       0.00      0.00      0.00         8

avg / total       0.68      0.73      0.66        74

=========================================================================================================

=========================================================================================================
Speaker: JULIA
Number of Total Listeners: 126

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
348 features selected out of 696 total
F1 mean: 0.18 (+/- 0.10)

             precision    recall  f1-score   support

       HOST       0.83      0.31      0.45        16
    LUCETTA       0.64      1.00      0.78        48
    PROTEUS       0.61      0.77      0.68        30
     SILVIA       1.00      0.39      0.56        18
     THURIO       0.00      0.00      0.00         9
  VALENTINE       0.00      0.00      0.00         5

avg / total       0.64      0.66      0.60       126

=========================================================================================================

=========================================================================================================
Speaker: PROTEUS
Number of Total Listeners: 211

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
473 features selected out of 946 total
F1 mean: 0.09 (+/- 0.06)

             precision    recall  f1-score   support

    ANTONIO       0.67      0.57      0.62         7
       DUKE       0.77      0.59      0.67        17
      JULIA       0.51      0.65      0.57        40
     LAUNCE       0.50      0.05      0.10        19
   PANTHINO       0.00      0.00      0.00         5
     SILVIA       0.67      0.18      0.29        22
      SPEED       1.00      0.50      0.67        22
     THURIO       0.67      0.17      0.28        23
  VALENTINE       0.46      0.95      0.62        56

avg / total       0.59      0.54      0.48       211

=========================================================================================================

=========================================================================================================
Speaker: FIRST OUTLAW
Number of Total Listeners: 23

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
54 features selected out of 109 total
F1 mean: 0.17 (+/- 0.06)

             precision    recall  f1-score   support

SECOND OUTLAW       0.48      1.00      0.65        10
THIRD OUTLAW       0.50      0.14      0.22         7
  VALENTINE       0.00      0.00      0.00         6

avg / total       0.36      0.48      0.35        23

=========================================================================================================

=========================================================================================================
Speaker: SECOND OUTLAW
Number of Total Listeners: 25

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
50 features selected out of 101 total
F1 mean: 0.12 (+/- 0.12)

             precision    recall  f1-score   support

FIRST OUTLAW       0.43      1.00      0.61        10
THIRD OUTLAW       0.50      0.12      0.20         8
  VALENTINE       0.00      0.00      0.00         7

avg / total       0.33      0.44      0.31        25

=========================================================================================================

=========================================================================================================
Speaker: VALENTINE
Number of Total Listeners: 218

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
468 features selected out of 936 total
F1 mean: 0.10 (+/- 0.06)

             precision    recall  f1-score   support

       DUKE       0.78      0.69      0.73        26
FIRST OUTLAW       0.40      0.18      0.25        11
      JULIA       0.00      0.00      0.00         5
     LAUNCE       0.00      0.00      0.00         9
    PROTEUS       0.71      0.77      0.74        44
SECOND OUTLAW       0.33      0.17      0.22        12
     SILVIA       0.55      0.56      0.55        32
      SPEED       0.44      0.92      0.59        49
THIRD OUTLAW       0.00      0.00      0.00        11
     THURIO       0.00      0.00      0.00        19

avg / total       0.45      0.55      0.48       218

=========================================================================================================

=========================================================================================================
Speaker: SPEED
Number of Total Listeners: 126

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
274 features selected out of 549 total
F1 mean: 0.40 (+/- 0.14)

             precision    recall  f1-score   support

     LAUNCE       0.89      0.98      0.93        49
    PROTEUS       1.00      0.64      0.78        22
     SILVIA       0.00      0.00      0.00         7
  VALENTINE       0.78      0.94      0.85        48

avg / total       0.82      0.85      0.82       126

=========================================================================================================

=========================================================================================================
Speaker: SILVIA
Number of Total Listeners: 87

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
245 features selected out of 490 total
F1 mean: 0.10 (+/- 0.09)

             precision    recall  f1-score   support

   EGLAMOUR       1.00      0.29      0.44         7
      JULIA       0.63      0.86      0.73        22
    PROTEUS       0.67      0.32      0.43        19
      SPEED       0.00      0.00      0.00         8
     THURIO       0.00      0.00      0.00         7
  VALENTINE       0.46      0.88      0.60        24

avg / total       0.51      0.55      0.48        87

=========================================================================================================

=========================================================================================================
Speaker: THURIO
Number of Total Listeners: 62

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
105 features selected out of 211 total
F1 mean: 0.10 (+/- 0.03)

             precision    recall  f1-score   support

       DUKE       0.50      0.17      0.25         6
      JULIA       0.00      0.00      0.00        13
    PROTEUS       0.45      1.00      0.62        22
     SILVIA       0.00      0.00      0.00         8
  VALENTINE       0.55      0.46      0.50        13

avg / total       0.32      0.47      0.35        62

=========================================================================================================

Play: 34

=========================================================================================================
Speaker: AUTOLYCUS
Number of Total Listeners: 111

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
350 features selected out of 700 total
F1 mean: 0.06 (+/- 0.07)

             precision    recall  f1-score   support

    CAMILLO       0.00      0.00      0.00         8
      CLOWN       0.47      0.98      0.64        49
     DORCAS       0.00      0.00      0.00        11
   FLORIZEL       0.50      0.17      0.25         6
      MOPSA       0.43      0.21      0.29        14
   SHEPHERD       0.00      0.00      0.00        23

avg / total       0.29      0.47      0.33       111

=========================================================================================================

=========================================================================================================
Speaker: SHEPHERD
Number of Total Listeners: 76

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
210 features selected out of 420 total
F1 mean: 0.08 (+/- 0.11)

             precision    recall  f1-score   support

  AUTOLYCUS       0.50      0.09      0.15        22
      CLOWN       0.47      0.97      0.63        31
   FLORIZEL       0.00      0.00      0.00         7
    PERDITA       0.50      0.17      0.25         6
  POLIXENES       0.33      0.20      0.25        10

avg / total       0.42      0.46      0.35        76

=========================================================================================================

=========================================================================================================
Speaker: PERDITA
Number of Total Listeners: 38

Best model LogisticRegression(C=0.90000000000000002, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
230 features selected out of 460 total
F1 mean: 0.15 (+/- 0.25)

             precision    recall  f1-score   support

    CAMILLO       0.55      0.46      0.50        13
   FLORIZEL       0.56      0.82      0.67        17
  POLIXENES       1.00      0.25      0.40         8

avg / total       0.65      0.58      0.55        38

=========================================================================================================

=========================================================================================================
Speaker: MOPSA
Number of Total Listeners: 30

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
50 features selected out of 100 total
F1 mean: 0.17 (+/- 0.04)

             precision    recall  f1-score   support

  AUTOLYCUS       0.50      0.11      0.18         9
      CLOWN       0.00      0.00      0.00         9
     DORCAS       0.43      1.00      0.60        12

avg / total       0.32      0.43      0.29        30

=========================================================================================================

=========================================================================================================
Speaker: POLIXENES
Number of Total Listeners: 88

Best model LogisticRegression(C=2.5000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
414 features selected out of 829 total
F1 mean: 0.12 (+/- 0.14)

             precision    recall  f1-score   support

    CAMILLO       0.60      1.00      0.75        26
   FLORIZEL       0.60      0.69      0.64        13
   HERMIONE       0.83      0.45      0.59        11
    LEONTES       0.59      0.67      0.62        15
    PAULINA       0.00      0.00      0.00         5
    PERDITA       1.00      0.29      0.44         7
   SHEPHERD       0.80      0.36      0.50        11

avg / total       0.65      0.64      0.60        88

=========================================================================================================

=========================================================================================================
Speaker: HERMIONE
Number of Total Listeners: 58

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
336 features selected out of 672 total
F1 mean: 0.09 (+/- 0.03)

             precision    recall  f1-score   support

 FIRST LORD       0.00      0.00      0.00         5
    LEONTES       0.49      1.00      0.66        26
  MAMILLIUS       1.00      0.33      0.50         9
    OFFICER       0.00      0.00      0.00         5
  POLIXENES       1.00      0.15      0.27        13

avg / total       0.60      0.53      0.43        58

=========================================================================================================

=========================================================================================================
Speaker: FLORIZEL
Number of Total Listeners: 69

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
319 features selected out of 638 total
F1 mean: 0.10 (+/- 0.07)

             precision    recall  f1-score   support

  AUTOLYCUS       0.00      0.00      0.00         6
    CAMILLO       0.56      0.90      0.69        21
    LEONTES       1.00      0.57      0.73         7
    PERDITA       0.61      0.61      0.61        18
  POLIXENES       0.69      0.75      0.72        12
   SHEPHERD       0.00      0.00      0.00         5

avg / total       0.55      0.62      0.57        69

=========================================================================================================

=========================================================================================================
Speaker: FIRST LORD
Number of Total Listeners: 21

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
83 features selected out of 167 total
F1 mean: 0.36 (+/- 0.10)

             precision    recall  f1-score   support

  ANTIGONUS       0.00      0.00      0.00         7
    LEONTES       0.67      1.00      0.80        14

avg / total       0.44      0.67      0.53        21

=========================================================================================================

=========================================================================================================
Speaker: LEONTES
Number of Total Listeners: 214

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
785 features selected out of 1571 total
F1 mean: 0.04 (+/- 0.03)

             precision    recall  f1-score   support

  ANTIGONUS       0.57      0.17      0.26        24
    CAMILLO       0.92      0.55      0.69        20
 FIRST LORD       0.44      0.40      0.42        20
   FLORIZEL       1.00      0.10      0.18        10
   HERMIONE       0.51      0.64      0.57        33
      LORDS       0.00      0.00      0.00         5
  MAMILLIUS       0.25      0.09      0.13        11
    OFFICER       1.00      0.14      0.25         7
    PAULINA       0.39      0.96      0.55        49
    PERDITA       0.00      0.00      0.00         7
  POLIXENES       0.50      0.17      0.26        23
    SERVANT       0.00      0.00      0.00         5

avg / total       0.50      0.46      0.40       214

=========================================================================================================

=========================================================================================================
Speaker: CAMILLO
Number of Total Listeners: 106

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
431 features selected out of 862 total
F1 mean: 0.14 (+/- 0.12)

             precision    recall  f1-score   support

 ARCHIDAMUS       1.00      0.50      0.67         6
  AUTOLYCUS       1.00      0.08      0.15        12
   FLORIZEL       0.46      0.81      0.59        27
    LEONTES       0.80      0.89      0.84        18
    PERDITA       0.50      0.20      0.29        20
  POLIXENES       0.73      0.83      0.78        23

avg / total       0.68      0.61      0.57       106

=========================================================================================================

=========================================================================================================
Speaker: PAULINA
Number of Total Listeners: 95

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
449 features selected out of 899 total
F1 mean: 0.07 (+/- 0.02)

             precision    recall  f1-score   support

  ANTIGONUS       0.00      0.00      0.00        11
  CLEOMENES       0.00      0.00      0.00         8
 FIRST LORD       0.00      0.00      0.00         6
     GAOLER       1.00      0.25      0.40         8
    LEONTES       0.52      1.00      0.68        48
    PERDITA       0.00      0.00      0.00         5
  POLIXENES       0.00      0.00      0.00         9

avg / total       0.34      0.53      0.38        95

=========================================================================================================

=========================================================================================================
Speaker: CLOWN
Number of Total Listeners: 112

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
293 features selected out of 587 total
F1 mean: 0.11 (+/- 0.04)

             precision    recall  f1-score   support

  AUTOLYCUS       0.49      0.90      0.64        49
     DORCAS       0.25      0.14      0.18         7
      MOPSA       0.00      0.00      0.00        11
  POLIXENES       0.00      0.00      0.00         5
    SERVANT       0.00      0.00      0.00         5
   SHEPHERD       0.63      0.34      0.44        35

avg / total       0.43      0.51      0.43       112

=========================================================================================================

=========================================================================================================
Speaker: ANTIGONUS
Number of Total Listeners: 27

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
131 features selected out of 263 total
F1 mean: 0.17 (+/- 0.07)

             precision    recall  f1-score   support

 FIRST LORD       0.00      0.00      0.00         8
    LEONTES       0.52      1.00      0.68        14
    PAULINA       0.00      0.00      0.00         5

avg / total       0.27      0.52      0.35        27

=========================================================================================================

Play: 35

=========================================================================================================
Speaker: DUKE
Number of Total Listeners: 290

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
809 features selected out of 1618 total
F1 mean: 0.07 (+/- 0.04)

             precision    recall  f1-score   support

     ANGELO       0.54      0.24      0.33        29
    CLAUDIO       1.00      0.11      0.20         9
      ELBOW       0.00      0.00      0.00         7
    ESCALUS       0.64      0.25      0.36        28
   ISABELLA       0.59      0.72      0.65        58
     JULIET       1.00      0.12      0.22         8
      LUCIO       0.43      0.98      0.60        59
    MARIANA       0.75      0.11      0.19        27
      PETER       0.00      0.00      0.00         7
     POMPEY       0.67      0.22      0.33         9
    PROVOST       0.65      0.69      0.67        49

avg / total       0.58      0.53      0.47       290

=========================================================================================================

=========================================================================================================
Speaker: FIRST GENTLEMAN
Number of Total Listeners: 28

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
56 features selected out of 113 total
F1 mean: 0.22 (+/- 0.00)

             precision    recall  f1-score   support

      LUCIO       0.50      1.00      0.67        14
MRS OVERDONE       0.00      0.00      0.00         5
SECOND GENTLEMAN       0.00      0.00      0.00         9

avg / total       0.25      0.50      0.33        28

=========================================================================================================

=========================================================================================================
Speaker: POMPEY
Number of Total Listeners: 93

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
210 features selected out of 420 total
F1 mean: 0.05 (+/- 0.02)

             precision    recall  f1-score   support

   ABHORSON       0.73      0.73      0.73        11
 BARNARDINE       0.50      0.20      0.29         5
       DUKE       0.00      0.00      0.00         5
      ELBOW       0.42      0.31      0.36        16
    ESCALUS       0.50      0.93      0.65        30
      FROTH       0.50      0.18      0.27        11
      LUCIO       0.00      0.00      0.00         6
MRS OVERDONE       0.88      0.78      0.82         9

avg / total       0.49      0.55      0.48        93

=========================================================================================================

=========================================================================================================
Speaker: ELBOW
Number of Total Listeners: 46

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
121 features selected out of 242 total
F1 mean: 0.10 (+/- 0.08)

             precision    recall  f1-score   support

     ANGELO       0.00      0.00      0.00         5
       DUKE       0.00      0.00      0.00         6
    ESCALUS       0.61      0.95      0.75        20
     POMPEY       0.53      0.53      0.53        15

avg / total       0.44      0.59      0.50        46

=========================================================================================================

=========================================================================================================
Speaker: ISABELLA
Number of Total Listeners: 175

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
506 features selected out of 1013 total
F1 mean: 0.10 (+/- 0.02)

             precision    recall  f1-score   support

     ANGELO       0.57      0.76      0.66        51
    CLAUDIO       0.91      0.48      0.62        21
       DUKE       0.71      0.80      0.76        46
      LUCIO       0.60      0.65      0.63        40
    MARIANA       1.00      0.12      0.22         8
    PROVOST       0.00      0.00      0.00         9

avg / total       0.65      0.65      0.62       175

=========================================================================================================

=========================================================================================================
Speaker: ESCALUS
Number of Total Listeners: 124

Best model LogisticRegression(C=0.30000000000000004, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
265 features selected out of 530 total
F1 mean: 0.09 (+/- 0.04)

             precision    recall  f1-score   support

     ANGELO       0.90      0.50      0.64        18
       DUKE       0.56      0.91      0.69        22
      ELBOW       0.61      0.61      0.61        23
      FROTH       0.50      0.09      0.15        11
    JUSTICE       1.00      0.40      0.57         5
      LUCIO       0.75      0.50      0.60        12
     POMPEY       0.53      0.85      0.66        27
    PROVOST       0.00      0.00      0.00         6

avg / total       0.62      0.60      0.57       124

=========================================================================================================

=========================================================================================================
Speaker: CLAUDIO
Number of Total Listeners: 52

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
213 features selected out of 427 total
F1 mean: 0.22 (+/- 0.21)

             precision    recall  f1-score   support

       DUKE       1.00      0.12      0.22         8
   ISABELLA       0.56      1.00      0.71        20
      LUCIO       0.62      0.42      0.50        12
    PROVOST       0.57      0.33      0.42        12

avg / total       0.64      0.58      0.52        52

=========================================================================================================

=========================================================================================================
Speaker: PROVOST
Number of Total Listeners: 78

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
219 features selected out of 439 total
F1 mean: 0.14 (+/- 0.07)

             precision    recall  f1-score   support

   ABHORSON       0.00      0.00      0.00         5
     ANGELO       1.00      0.12      0.22         8
    CLAUDIO       0.00      0.00      0.00         6
       DUKE       0.60      1.00      0.75        42
   ISABELLA       0.00      0.00      0.00         7
     POMPEY       0.57      0.40      0.47        10

avg / total       0.50      0.60      0.49        78

=========================================================================================================

=========================================================================================================
Speaker: ANGELO
Number of Total Listeners: 116

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
360 features selected out of 721 total
F1 mean: 0.14 (+/- 0.08)

             precision    recall  f1-score   support

       DUKE       0.55      0.35      0.43        17
    ESCALUS       0.61      0.64      0.62        22
   ISABELLA       0.54      1.00      0.70        44
      LUCIO       0.00      0.00      0.00        20
    PROVOST       0.00      0.00      0.00        13

avg / total       0.40      0.55      0.45       116

=========================================================================================================

=========================================================================================================
Speaker: LUCIO
Number of Total Listeners: 174

Best model LogisticRegression(C=0.10000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='ovr', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
428 features selected out of 857 total
F1 mean: 0.05 (+/- 0.02)

             precision    recall  f1-score   support

     ANGELO       0.00      0.00      0.00        16
    CLAUDIO       1.00      0.22      0.36         9
       DUKE       0.43      0.95      0.59        56
      ELBOW       0.33      0.11      0.17         9
    ESCALUS       0.00      0.00      0.00         9
FIRST GENTLEMAN       0.62      0.38      0.48        13
   ISABELLA       0.61      0.67      0.63        30
    MARIANA       0.00      0.00      0.00         6
     POMPEY       0.40      0.22      0.29         9
    PROVOST       0.00      0.00      0.00         7
SECOND GENTLEMAN       0.00      0.00      0.00        10

avg / total       0.38      0.48      0.38       174

=========================================================================================================

=========================================================================================================
Speaker: MARIANA
Number of Total Listeners: 41

Best model LogisticRegression(C=0.20000000000000001, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=200,
          multi_class='multinomial', penalty='l1', random_state=None,
          solver='lbfgs', tol=0.0001, verbose=1)
120 features selected out of 241 total
F1 mean: 0.16 (+/- 0.09)

             precision    recall  f1-score   support

     ANGELO       0.00      0.00      0.00         5
       DUKE       0.56      1.00      0.72        22
   ISABELLA       1.00      0.22      0.36         9
      LUCIO       0.00      0.00      0.00         5

avg / total       0.52      0.59      0.47        41

=========================================================================================================

